{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***INDICE***\n",
    "1. Caricamento file\n",
    "2. Caricamento dei testi dei file in oggetti Document()\n",
    "3. Set up del LLM\n",
    "4. Creazione degli indici\n",
    "5. Caricamento indici da disco\n",
    "6. Query semplice sull'indice"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INSTALLAZIONE DELLE LIBRERIE NECESSARIE**\n",
    "- *Le librerie sono state installate in un env di anaconda* \n",
    "\n",
    "*Librerie necessarie*\n",
    "- !pip install llama-index \n",
    "- !pip install openai \n",
    "- !pip install langchain \n",
    "- !pip install gpt-index \n",
    "- !pip install PyPDF2 \n",
    "- !pip install pypdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. CARICAMENTO FILE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chain of thoughts.pdf', 'Prompt patterns.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir(\"./documenti\")          # otteniamo la lista di tutti i file contenuti nella directory di lavoro di colab\n",
    "\n",
    "lista_pdf = []\n",
    "for file in files:              # col ciclo otterremo la lista \"lista_pdf\" che conterrà i file in formato \n",
    "    if \".pdf\" in file:          # pdf presenti nella direrctory\n",
    "        lista_pdf.append(file)\n",
    "\n",
    "print(lista_pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. CARICAMENTO DEI TESTI DEI FILE IN OGGETTI DOCUMENT()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers import SimpleDirectoryReader     # importiamo SimpleDirectoryReader che ci permetterà di leggere il file ceh desideriamo\n",
    "\n",
    "\n",
    "doc_set = {}              # insieme che conterrà coppie \"chiavi:valori\" -> nome del file : [Document(text = '')]     perr ogni file caricato\n",
    "all_docs = []             # Lista che conterrà i testi di ciascun file come elementi\n",
    "\n",
    "i = 0\n",
    "\n",
    "for nomefile in lista_pdf:\n",
    "  # \"input_dir\" -> innseriamo la directory \"documenti\" che contiene i nostri pdf\n",
    "  # \"required_exts\": imposta l'estensione di cui abbiamo bisogno (anche qui deve essere una lista \n",
    "  \n",
    "  pdf_docs = SimpleDirectoryReader(input_dir = \"./documenti/\", required_exts = [\".pdf\"]).load_data()        # pdf_docs -> variabile che contiene il testo del documento letto da SimpleDirectoryReader\n",
    "  \n",
    "  i+=1\n",
    "  # insert nomefile metadata into each file\n",
    "  for d in pdf_docs:\n",
    "    d.extra_info = {\"nome file\": nomefile}            # inserendo il nome del file come metadato permetterà un'interrogazione più semplice\n",
    "    \n",
    "  doc_set[nomefile] = pdf_docs\n",
    "  all_docs.extend(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Chain of thoughts.pdf': [Document(text='Chain-of-Thought Prompting Elicits Reasoning\\nin Large Language Models\\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\\nGoogle Research, Brain Team\\n{jasonwei,dennyzhou}@google.com\\nAbstract\\nWe explore how generating a chain of thought —a series of intermediate reasoning\\nsteps—signiﬁcantly improves the ability of large language models to perform\\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\\nnaturally in sufﬁciently large language models via a simple method called chain-of-\\nthought prompting , where a few chain of thought demonstrations are provided as\\nexemplars in prompting.\\nExperiments on three large language models show that chain-of-thought prompting\\nimproves performance on a range of arithmetic, commonsense, and symbolic\\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\\naccuracy on the GSM8K benchmark of math word problems, surpassing even\\nﬁnetuned GPT-3 with a veriﬁer.\\nA: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.Chain-of-Thought PromptingQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: The answer is 27.Standard Prompting\\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?Model Input\\nModel OutputModel OutputModel Input\\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2201.11903v6  [cs.CL]  10 Jan 2023', doc_id='92a93c01-5904-4665-8dc7-20b4ffd69f9d', embedding=None, doc_hash='0ed04fe8d36c5656429f0633f36af2051e1b1ac69a911e7bcb636f3defa1dc91', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='1 Introduction\\nMath Word Problems (GSM8K)020406080100\\n3355\\n1857Solve rate (%)Finetuned GPT-3 175B\\nPrior best\\nPaLM 540B: standard prompting\\nPaLM 540B: chain-of-thought prompting\\nFigure 2: PaLM 540B uses chain-of-\\nthought prompting to achieve new state-\\nof-the-art performance on the GSM8K\\nbenchmark of math word problems.\\nFinetuned GPT-3 and prior best are from\\nCobbe et al. (2021).The NLP landscape has recently been revolutionized by\\nlanguage models (Peters et al., 2018; Devlin et al., 2019;\\nBrown et al., 2020, inter alia ). Scaling up the size of lan-\\nguage models has been shown to confer a range of beneﬁts,\\nsuch as improved performance and sample efﬁciency (Ka-\\nplan et al., 2020; Brown et al., 2020, inter alia ). However,\\nscaling up model size alone has not proved sufﬁcient for\\nachieving high performance on challenging tasks such as\\narithmetic, commonsense, and symbolic reasoning (Rae\\net al., 2021).\\nThis work explores how the reasoning ability of large\\nlanguage models can be unlocked by a simple method\\nmotivated by two ideas. First, techniques for arithmetic\\nreasoning can beneﬁt from generating natural language\\nrationales that lead to the ﬁnal answer. Prior work has\\ngiven models the ability to generate natural language inter-\\nmediate steps by training from scratch (Ling et al., 2017)\\nor ﬁnetuning a pretrained model (Cobbe et al., 2021), in\\naddition to neuro-symbolic methods that use formal lan-\\nguages instead of natural language (Roy and Roth, 2015;\\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\\n2019). Second, large language models offer the exciting\\nprospect of in-context few-shot learning via prompting . That is, instead of ﬁnetuning a separate\\nlanguage model checkpoint for each new task, one can simply “prompt” the model with a few\\ninput–output exemplars demonstrating the task. Remarkably, this has been successful for a range of\\nsimple question-answering tasks (Brown et al., 2020).\\nBoth of the above ideas, however, have key limitations. For rationale-augmented training and\\nﬁnetuning methods, it is costly to create a large set of high quality rationales, which is much more\\ncomplicated than simple input–output pairs used in normal machine learning. For the traditional few-\\nshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning\\nabilities, and often does not improve substantially with increasing language model scale (Rae et al.,\\n2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.\\nSpeciﬁcally, we explore the ability of language models to perform few-shot prompting for reasoning\\ntasks, given a prompt that consists of triples: hinput, chain of thought , outputi. Achain of thought is\\na series of intermediate natural language reasoning steps that lead to the ﬁnal output, and we refer to\\nthis approach as chain-of-thought prompting . An example prompt is shown in Figure 1.\\nWe present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks,\\nshowing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking\\ndegree. Figure 2 illustrates one such result—on the GSM8K benchmark of math word problems\\n(Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting\\nby a large margin and achieves new state-of-the-art performance. A prompting only approach is\\nimportant because it does not require a large training dataset and because a single model checkpoint\\ncan perform many tasks without loss of generality. This work underscores how large language models\\ncan learn via a few examples with natural language data about the task (c.f. automatically learning\\nthe patterns underlying inputs and outputs via a large training dataset).\\n2 Chain-of-Thought Prompting\\nConsider one’s own thought process when solving a complicated reasoning task such as a multi-step\\nmath word problem. It is typical to decompose the problem into intermediate steps and solve each\\nbefore giving the ﬁnal answer: “After Jane gives 2 ﬂowers to her mom she has 10 :::then after she\\ngives 3 to her dad she will have 7 :::so the answer is 7. ” The goal of this paper is to endow language\\nmodels with the ability to generate a similar chain of thought —a coherent series of intermediate\\nreasoning steps that lead to the ﬁnal answer for a problem. We will show that sufﬁciently large\\n2', doc_id='7cc1a710-2ed2-4691-947c-bbac8bf0630e', embedding=None, doc_hash='7778451b1e7bb41b6be219026f9529d567f51a4f81704de4bd5b45b840d5cf5c', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\\nmimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\\ntypically come after the ﬁnal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\\n2022, inter alia )).\\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\\nin language models.\\n1.First, chain of thought, in principle, allows models to decompose multi-step problems into\\nintermediate steps, which means that additional computation can be allocated to problems\\nthat require more reasoning steps.\\n2.Second, a chain of thought provides an interpretable window into the behavior of the model,\\nsuggesting how it might have arrived at a particular answer and providing opportunities\\nto debug where the reasoning path went wrong (although fully characterizing a model’s\\ncomputations that support an answer remains an open question).\\n3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,\\ncommonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\\nin principle) to any task that humans can solve via language.\\n4.Finally, chain-of-thought reasoning can be readily elicited in sufﬁciently large off-the-shelf\\nlanguage models simply by including examples of chain of thought sequences into the\\nexemplars of few-shot prompting.\\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\\nreasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\\n3 Arithmetic Reasoning\\nWe begin by considering math word problems of the form in Figure 1, which measure the arithmetic\\nreasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\\nlanguage models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-\\nof-thought prompting when used with the 540B parameter language model performs comparably with\\ntask-speciﬁc ﬁnetuned models on several tasks, even achieving new state of the art on the challenging\\nGSM8K benchmark (Cobbe et al., 2021).\\n3.1 Experimental Setup\\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\\nBenchmarks. We consider the following ﬁve math word problem benchmarks: (1)theGSM8K\\nbenchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word\\nproblems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word\\nproblems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS\\nbenchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\\nStandard prompting. For the baseline, we consider standard few-shot prompting, popularized by\\nBrown et al. (2020), in which a language model is given in-context exemplars of input–output pairs\\nbefore outputting a prediction for a test-time example. Exemplars are formatted as questions and\\nanswers. The model gives the answer directly, as shown in Figure 1 (left).\\nChain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\\nprompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\\nof the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\\nwith chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the\\nfull set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\\nprompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\\nchain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\\n3', doc_id='89514f0c-96c7-4203-b50a-5c2e8a0ffa73', embedding=None, doc_hash='2740a57bf6c7cc64a5c17109e1c5f2ec144acd9a756a617467755e8cf61e254b', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.Q: Sammy wanted to go to where the people were. Where might he go? Options: (a) race track (b) populated areas (c) desert (d) apartment (e) roadblock A: The answer must be a place with a lot of people. Race tracks, desert, apartments, and roadblocks don\\'t have a lot of people, but populated areas do. So the answer is (b). Q: Yes or no: Would a pear sink in water? A: The density of a pear is about 0.6 g/cm^3, which is less than water. Thus, a pear would float. So the answer is no.Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?  A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the answer is 05/23/1943. Q: Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\"  A: Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. So the answer is no.Q: Take the last letters of the words in “Lady Gaga” and concatenate them. A: The last letter of “Lady” is “y”. The last letter of “Gaga” is “a”. Concatenating them is “ya”. So the answer is ya.Q: A coin is heads up. Maybelle flips the coin. Shalonda does not flip the coin. Is the coin still heads up? A: The coin was flipped by Maybelle. So the coin was flipped 1 time, which is an odd number. The coin started heads up, so after an odd number of flips, it will be tails up. So the answer is no.Math Word Problems (free response)Math Word Problems (multiple choice)CSQA (commonsense)\\nStrategyQADate UnderstandingSports Understanding\\nLast Letter ConcatenationCoin Flip (state tracking)Q: How many keystrokes are needed to type the numbers from 1 to 500?Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788 A: There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401 three-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. The answer is (b).\\nSayCan (Instructing a robot)Human: How would you bring me something that isn’t a fruit? Explanation: the user wants something to eat that isn’t a fruit. An energy bar is not a fruit, so I will bring the user an energy bar.  Plan: 1. find(energy bar) 2. pick(energy bar) 3. find(user) 4. put(energy bar) 5. done().Figure 3: Examples of hinput, chain of thought, output itriples for arithmetic, commonsense, and\\nsymbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.\\nmath word problems, we used this single set of eight chain of thought exemplars for all benchmarks\\nexcept AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars\\nand solutions from the training set, as given in Appendix Table 21.\\nLanguage models. We evaluate ﬁve large language models. The ﬁrst is GPT-3 (Brown et al.,\\n2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which\\npresumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang\\net al., 2022).The second is LaMDA (Thoppilan et al., 2022), which has models of 422M, 2B, 8B,\\n68B, and 137B parameters. The third is PaLM , which has models of 8B, 62B, and 540B parameters.\\nThe fourth is UL2 20B (Tay et al., 2022), and the ﬁfth is Codex (Chen et al., 2021, code-davinci-002\\nin the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows\\nchain-of-thought prompting can be improved by taking the majority ﬁnal answer over many sampled\\ngenerations (Wang et al., 2022a)). For LaMDA, we report averaged results over ﬁve random seeds,\\nwhere each seed had a different randomly shufﬂed order of exemplars. As LaMDA experiments\\ndid not show large variance among different seeds, to save compute we report results for a single\\nexemplar order for all other models.\\n3.2 Results\\nThe strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental\\noutputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix.\\nThere are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent\\nability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively\\nimpact performance for small models, and only yields performance gains when used with models of\\n\\x18100B parameters. We qualitatively found that models of smaller scale produced ﬂuent but illogical\\nchains of thought, leading to lower performance than standard prompting.\\n4', doc_id='73272290-3dd6-4e8d-b3e1-68106e4ad9b1', embedding=None, doc_hash='3fd24529a0e6dbf3f3ac5569ca2c9409ca08a46fcca48aed614ef43d9fc8c904', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='0204060GSM8K\\nsolve rate (%)LaMDA GPT PaLMStandard prompting\\nChain-of-thought prompting\\nPrior supervised best\\n020406080SV AMP\\nsolve rate (%)\\n0.4 81370255075100MAWPS\\nsolve rate (%)\\n0.4 7175 862540\\nModel scale (# parameters in billions)\\nFigure 4: Chain-of-thought prompting enables\\nlarge language models to solve challenging math\\nproblems. Notably, chain-of-thought reasoning\\nis an emergent ability of increasing model scale.\\nPrior best numbers are from Cobbe et al. (2021)\\nfor GSM8K, Jie et al. (2022) for SV AMP, and Lan\\net al. (2021) for MAWPS.Second, chain-of-thought prompting has larger\\nperformance gains for more-complicated prob-\\nlems. For instance, for GSM8K (the dataset\\nwith the lowest baseline performance), perfor-\\nmance more than doubled for the largest GPT\\nand PaLM models. On the other hand, for Sin-\\ngleOp, the easiest subset of MAWPS which only\\nrequires a single step to solve, performance im-\\nprovements were either negative or very small\\n(see Appendix Table 3).\\nThird, chain-of-thought prompting via GPT-3\\n175B and PaLM 540B compares favorably to\\nprior state of the art, which typically ﬁnetunes a\\ntask-speciﬁc model on a labeled training dataset.\\nFigure 4 shows how PaLM 540B uses chain-of-\\nthought prompting to achieve new state of the art\\non GSM8K, SV AMP, and MAWPS (though note\\nthat standard prompting already passed the prior\\nbest for SV AMP). On the other two datasets,\\nAQuA and ASDiv, PaLM with chain-of-thought\\nprompting reaches within 2% of the state of the\\nart (Appendix Table 2).\\nTo better understand why chain-of-thought\\nprompting works, we manually examined model-\\ngenerated chains of thought by LaMDA 137B\\nfor GSM8K. Of 50 random examples where the\\nmodel returned the correct ﬁnal answer, all of\\nthe generated chains of thought were also log-\\nically and mathematically correct except two\\nthat coincidentally arrived at the correct answer\\n(see Appendix D.1, and Table 8 for examples\\nof correct model-generated chains of thought).\\nWe also randomly examined 50 random sam-\\nples for which the model gave the wrong answer.\\nThe summary of this analysis is that 46% of the\\nchains of thought were almost correct, barring\\nminor mistakes (calculator error, symbol map-\\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\\nmade by PaLM 62B and whether those errors were ﬁxed by scaling to PaLM 540B. The summary\\nis that scaling PaLM to 540B ﬁxes a large portion of one-step missing and semantic understanding\\nerrors in the 62B model (see Appendix A.1).\\n3.3 Ablation Study\\nThe observed beneﬁts of using chain-of-thought prompting raises the natural question of whether the\\nsame performance improvements can be conferred via other types of prompting. Figure 5 shows an\\nablation study with three variations of chain of thought described below.\\nEquation only. One reason for why chain-of-thought prompting might help is that it produces the\\nmathematical equation to be evaluated, and so we test a variation where the model is prompted\\nto output only a mathematical equation before giving the answer. Figure 5 shows that equation\\nonly prompting does not help much for GSM8K, which implies that the semantics of the questions\\nin GSM8K are too challenging to directly translate into an equation without the natural language\\nreasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we ﬁnd\\nthat equation only prompting does improve performance, since the equation can be easily derived\\nfrom the question (see Appendix Table 6).\\n5', doc_id='fc7f5212-628d-44b6-9b29-ea77a9d42d49', embedding=None, doc_hash='a2549f9f47aaab9eced4a87a3096cfc0a738c72e45d0073994579daef6b19883', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='LaMDA PaLM0204060GSM8K solve rate (%)Standard prompting\\nEquation only\\nVariable compute only\\nReasoning after answer\\nChain-of-thought prompting\\nFigure 5: Ablation study for dif-\\nferent variations of prompting us-\\ning LaMDA 137B and PaLM 540B.\\nResults for other datasets are given\\nin Appendix Table 6 and Table 7.Variable compute only. Another intuition is that chain of\\nthought allows the model to spend more computation (i.e.,\\nintermediate tokens) on harder problems. To isolate the effect\\nof variable computation from chain-of-thought reasoning, we\\ntest a conﬁguration where the model is prompted to output a\\nonly sequence of dots ( :::) equal to the number of characters in\\nthe equation needed to solve the problem. This variant performs\\nabout the same as the baseline, which suggests that variable\\ncomputation by itself is not the reason for the success of chain-\\nof-thought prompting, and that there appears to be utility from\\nexpressing intermediate steps via natural language.\\nChain of thought after answer. Another potential beneﬁt of\\nchain-of-thought prompting could simply be that such prompts\\nallow the model to better access relevant knowledge acquired\\nduring pretraining. Therefore, we test an alternative conﬁgura-\\ntion where the chain of thought prompt is only given after the\\nanswer, isolating whether the model actually depends on the\\nproduced chain of thought to give the ﬁnal answer. This variant\\nperforms about the same as the baseline, which suggests that\\nthe sequential reasoning embodied in the chain of thought is\\nuseful for reasons beyond just activating knowledge.\\n3.4 Robustness of Chain of Thought\\nGSM8K05101520Solve rate (%)Standard prompting\\nChain-of-thought prompting\\n\\x01different annotator (B)\\n\\x01different annotator (C)\\n\\x01intentionally concise style\\n\\x01exemplars from GSM8K ( \\x0b)\\n\\x01exemplars from GSM8K ( \\x0c)\\n\\x01exemplars from GSM8K ( \\r)\\nMAWPS0204060\\nFigure 6: Chain-of-thought prompting\\nhas variance for different prompt exam-\\nples (as expected) but outperforms stan-\\ndard prompting for various annotators as\\nwell as for different exemplars.Sensitivity to exemplars is a key consideration of prompt-\\ning approaches—for instance, varying the permutation of\\nfew-shot exemplars can cause the accuracy of GPT-3 on\\nSST-2 to range from near chance (54.3%) to near state of\\nthe art (93.4%) (Zhao et al., 2021). In this ﬁnal subsec-\\ntion, we evaluate robustness to chains of thought written\\nby different annotators. In addition to the results above,\\nwhich used chains of thought written by an Annotator\\nA, two other co-authors of this paper (Annotators B and\\nC) independently wrote chains of thought for the same\\nfew-shot exemplars (shown in Appendix H). Annotator A\\nalso wrote another chain of thought that was more concise\\nthan the original, following the style of solutions given in\\nCobbe et al. (2021).1\\nFigure 6 shows these results for LaMDA 137B on GSM8K\\nand MAWPS (ablation results for other datasets are given\\nin Appendix Table 6 / Table 7). Although there is variance\\namong different chain of thought annotations, as would be\\nexpected when using exemplar-based prompting (Le Scao\\nand Rush, 2021; Reynolds and McDonell, 2021; Zhao\\net al., 2021), all sets of chain of thought prompts outper-\\nform the standard baseline by a large margin. This result\\nimplies that successful use of chain of thought does not\\ndepend on a particular linguistic style.\\nTo conﬁrm that successful chain-of-thought prompting\\nworks for other sets of exemplars, we also run experiments\\nwith three sets of eight exemplars randomly sampled from the GSM8K training set, an independent\\n1For instance, whereas original chain of thought uses several short sentences ( “’There were originally 9\\ncomputers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is\\n29. ”), the concise chain of thought would read “5 * 4 = 20 new computers were added. So there are 9 + 20 = 29\\nnew computers in the server room now” .\\n6', doc_id='562d27df-8214-48be-9775-1e0d2ce4eef1', embedding=None, doc_hash='05d0f93532d723720f5e935824226a826cb881239ae1f87572295282b155ed2a', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='source (examples in this dataset already included reasoning steps like a chain of thought).2Fig-\\nure 6 shows that these prompts performed comparably with our manually written exemplars, also\\nsubstantially outperforming standard prompting.\\nIn addition to robustness to annotators, independently-written chains of thought, different exemplars,\\nand various language models, we also ﬁnd that chain-of-thought prompting for arithmetic reasoning\\nis robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2).\\n4 Commonsense Reasoning\\nAlthough chain of thought is particularly suitable for math word problems, the language-based nature\\nof chain of thought actually makes it applicable to a broad class of commonsense reasoning problems,\\nwhich involve reasoning about physical and human interactions under the presumption of general\\nbackground knowledge. Commonsense reasoning is key for interacting with the world and is still\\nbeyond the reach of current natural language understanding systems (Talmor et al., 2021).\\nBenchmarks. We consider ﬁve datasets covering a diverse range of commonsense reasoning types.\\nThe popular CSQA (Talmor et al., 2019) asks commonsense questions about the world involving\\ncomplex semantics that often require prior knowledge. StrategyQA (Geva et al., 2021) requires\\nmodels to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets\\nfrom the BIG-bench effort (BIG-bench collaboration, 2021): Date Understanding, which involves\\ninferring a date from a given context, and Sports Understanding, which involves determining whether\\na sentence relating to sports is plausible or implausible. Finally, the SayCan dataset (Ahn et al.,\\n2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete\\nset. Figure 3 shows examples with chain of thought annotations for all datasets.\\nPrompts. We follow the same experimental setup as the prior section. For CSQA and StrategyQA,\\nwe randomly selected examples from the training set and manually composed chains of thought for\\nthem to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected\\nthe ﬁrst ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on\\nthe rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al.\\n(2022) and also manually composed chains of thought.\\nResults. Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different\\nmodel scales are shown in Table 4). For all tasks, scaling up model size improved the performance\\nof standard prompting; chain-of-thought prompting led to further gains, with improvements appear-\\ning to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong\\nperformance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs\\n69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%).\\nThese results demonstrate that chain-of-thought prompting can also improve performance on tasks\\nrequiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA).\\n86254020406080100 Solve rate (%)CSQA\\n8625405060708090StrategyQA\\nStandard prompting\\nChain of thought\\nPrior supervised best\\nHuman\\n862540020406080\\nModel scale (# parameters in billions)Date\\n862540406080100Sports\\n86254020406080100SayCan\\nFigure 7: Chain-of-thought prompting also improves the commonsense reasoning abilities of\\nlanguage models. The language model shown here is PaLM. Prior best numbers are from the\\nleaderboards of CSQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021) (single-model only,\\nas of May 5, 2022). Additional results using various sizes of LaMDA, GPT-3, and PaLM are shown\\nin Table 4.\\n2We sample examples \\x1460tokens to ﬁt into our input context window, and also limit the examples to \\x142\\nsteps to solve for a fair comparison with the eight exemplars that we composed.\\n7', doc_id='4a0435e3-bab0-4aca-99b9-cc0119aa9fc2', embedding=None, doc_hash='bb993ac50beb8e755ace86bcc4d6569aa451eed54212211b2239d378ff5c5d68', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='5 Symbolic Reasoning\\n0255075100 Solve rate (%)Letter Concat: 2\\n(in domain)Letter Concat: 4\\n(OOD)Standard prompting\\nChain-of-thought prompting\\n8 62 540406080100 Solve rate (%)Coin Flip: 2\\n(in domain)\\n8 62 540\\nModel scale (# parameters in billions)Coin Flip: 4\\n(OOD)\\nFigure 8: Using chain-of-thought\\nprompting facilitates generalization to\\nlonger sequences in two symbolic rea-\\nsoning tasks.Our ﬁnal experimental evaluation considers symbolic rea-\\nsoning, which is simple for humans but potentially chal-\\nlenging for language models. We show that chain-of-\\nthought prompting not only enables language models to\\nperform symbolic reasoning tasks that are challenging in\\nthe standard prompting setting, but also facilitates length\\ngeneralization to inference-time inputs longer than those\\nseen in the few-shot exemplars.\\nTasks. We use the following two toy tasks.\\n•Last letter concatenation. This task asks the model\\nto concatenate the last letters of words in a name (e.g.,\\n“Amy Brown”!“yn” ). It is a more challenging version\\nof ﬁrst letter concatenation, which language models can\\nalready perform without chain of thought.3We generate\\nfull names by randomly concatenating names from the\\ntop one-thousand ﬁrst and last names from name census\\ndata ( https://namecensus.com/ ).\\n•Coin ﬂip. This task asks the model to answer whether a\\ncoin is still heads up after people either ﬂip or don’t ﬂip\\nthe coin (e.g., “A coin is heads up. Phoebe ﬂips the coin.\\nOsvaldo does not ﬂip the coin. Is the coin still heads up?”\\n!“no” ).\\nAs the construction of these symbolic reasoning tasks is\\nwell-deﬁned, for each task we consider an in-domain test\\nset for which examples had the same number of steps as\\nthe training/few-shot exemplars, as well as an out-of-domain (OOD) test set, for which evaluation\\nexamples had more steps than those in the exemplars. For last letter concatenation, the model only\\nsees exemplars of names with two words, and then performs last letter concatenation on names with 3\\nand 4 words.4We do the same for the number of potential ﬂips in the coin ﬂip task. Our experimental\\nsetup uses the same methods and models as in the prior two sections. We again manually compose\\nchains of thought for the few-shot exemplars for each task, which are given in Figure 3.\\nResults. The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM,\\nwith results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting\\nleads to almost 100% solve rates (note that standard prompting already solves coin ﬂip with PaLM\\n540, though not for LaMDA 137B). Note that these in-domain evaluations are “toy tasks” in the\\nsense that perfect solution structures are already provided by the chains of thought in the few-shot\\nexemplars; all the model has to do is repeat the same steps with the new symbols in the test-time\\nexample. And yet, small models still fail—the ability to perform abstract manipulations on unseen\\nsymbols for these three tasks only arises at the scale of 100B model parameters.\\nAs for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,\\nlanguage models achieve upward scaling curves (though performance is lower than in the in-domain\\nsetting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of\\nthought for language models of sufﬁcient scale.\\n6 Discussion\\nWe have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step rea-\\nsoning behavior in large language models. We ﬁrst saw that chain-of-thought prompting improves\\nperformance by a large margin on arithmetic reasoning, yielding improvements that are much stronger\\nthan ablations and robust to different annotators, exemplars, and language models (Section 3). Next,\\n3We tested 10 common names using GPT-3 davinci and it got all but one correct.\\n4For names of length longer than 2 words, we concatenate multiple ﬁrst and last names together.\\n8', doc_id='3454bba2-ab95-4e96-a2c9-766ee41a6ea1', embedding=None, doc_hash='dc8a4aa833ddaed440953b79461749a465fbbaf92f817e84cfdc2b9b4f6b9c09', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\\nmodel. No language models were ﬁnetuned in the process of writing this paper.\\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\\nappears to expand the set of tasks that large language models can perform successfully—in other\\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\\nof large language models. This observation likely raises more questions than it answers—for instance,\\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\\nWhat other prompting methods might expand the range of tasks that language models can solve?\\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\\nand incorrect answers; improving factual generations of language models is an open direction for\\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\\nreal-world applications; further research could explore how to induce reasoning in smaller models.\\n7 Related Work\\nThis work is inspired by many research areas, which we detail in an extended related work section\\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\\npioneer the idea of using natural language rationales to solve math word problems through a series\\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\\nprediction method performs better than directly predicting the ﬁnal outputs.\\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\\noutputs of language models with a chain of thought.\\n8 Conclusions\\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\\nfurther work on language-based approaches to reasoning.\\n9', doc_id='0f02bd04-41e6-4f5c-a6b8-773902ff019d', embedding=None, doc_hash='5a433da1c6dd9442d80a1ee4698045745c82439180addbd7cbca9b5a21fed62b', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Acknowledgements\\nWe thank Jacob Devlin, Claire Cui, Andrew Dai, and Ellie Pavlick for providing feedback on the\\npaper. We thank Jacob Austin, Yuhuai Wu, Henryk Michalewski, Aitor Lewkowycz, Charles Sutton,\\nand Aakanksha Chowdhery for helpful discussions. We thank Sid Maxwell for notifying us about a\\nmistake in the manual error analysis in the original manuscript.\\nReferences\\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. 2022. Do as I can, not as I\\nsay: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691 .\\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh\\nHajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-\\nbased formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\\nShort Papers) , Minneapolis, Minnesota. Association for Computational Linguistics.\\nDaniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding\\noperations and arguments with reading comprehension. EMNLP .\\nJacob Andreas, Dan Klein, and Sergey Levine. 2018. Learning with latent language. NAACL .\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language\\nmodels. arXiv preprint arXiv:2108.07732 .\\nBIG-bench collaboration. 2021. Beyond the imitation game: Measuring and extrapolating the\\ncapabilities of language models. In preparation .\\nKaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural\\nlanguage deductions. EMNLP .\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\\nand Dario Amodei. 2020. Language models are few-shot learners. NeurIPS .\\nJonathon Cai, Richard Shin, and Dawn Song. 2017. Making neural programming architectures\\ngeneralize via recursion. ICLR .\\nOana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI:\\nNatural language inference with natural language explanations. NeurIPS .\\nHoward Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022. Can rationalization\\nimprove robustness? NAACL .\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating\\nlarge language models trained on code. arXiv preprint arXiv:2107.03374 .\\nXinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V . Le. 2019. Neural\\nsymbolic reader: Scalable integration of distributed and symbolic representations for reading\\ncomprehension. ICLR .\\nTing-Rui Chiang and Yun-Nung Chen. 2019. Semantically-aligned equation generation for solving\\nand reasoning math word problems. In Proceedings of the 2019 Conference of the North Ameri-\\ncan Chapter of the Association for Computational Linguistics: Human Language Technologies,\\nVolume 1 (Long and Short Papers) , pages 2656–2668, Minneapolis, Minnesota. Association for\\nComputational Linguistics.\\n10', doc_id='5aee8c29-3eba-4808-8a7d-625ded3cd67f', embedding=None, doc_hash='d97984b4a00b46f8f8b0bc5f907372374b50d8014e4bf47768eb648192f4d1ff', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over\\nlanguage. IJCAI .\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher\\nHesse, and John Schulman. 2021. Training veriﬁers to solve math word problems. arXiv preprint\\narXiv:2110.14168 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language understanding. NAACL .\\nHonghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural\\nlogic machines. ICLR .\\nDheeru Dua, Sameer Singh, and Matt Gardner. 2020. Beneﬁts of intermediate annotations in reading\\ncomprehension. ACL.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did\\naristotle use a laptop? A question answering benchmark with implicit reasoning strategies. TACL .\\nYuling Gu, Bhavana Dalvi Mishra, and Peter Clark. 2022. DREAM: Uncovering mental models\\nbehind language models. NAACL .\\nBraden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher\\nRé. 2018. Training classiﬁers with natural language explanations. ACL.\\nPeter Hase and Mohit Bansal. 2022. When can models learn from explanations? a formal framework\\nfor understanding the roles of explanation data. ACL.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\\nand Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv\\npreprint arXiv:2103.03874 .\\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning\\nto solve arithmetic word problems with verb categorization. EMNLP .\\nZhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to reason deductively: Math word problem\\nsolving as complex relation extraction. arXiv preprint arXiv:2203.10316 .\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language\\nmodels. arXiv preprint arXiv:2001.08361 .\\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016.\\nMAWPS: A math word problem repository. NAACL .\\nAndrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y . Chan, Kory Matthewson, Michael Henry\\nTessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. 2022. Can language\\nmodels learn from explanations in context? arXiv preprint arXiv:2204.02329 .\\nYihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang,\\nand Ee-Peng Lim. 2021. MWPToolkit: An open-source framework for deep learning-based math\\nword problem solvers. arXiv preprint arXiv:2109.00799 .\\nTeven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? NAACL .\\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efﬁcient\\nprompt tuning. EMNLP .\\nIddo Lev, Bill MacCartney, Christopher Manning, and Roger Levy. 2004. Solving logic puzzles:\\nFrom robust processing to precise semantics. Proceedings of the 2nd Workshop on Text Meaning\\nand Interpretation .\\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning: Optimizing continuous prompts for generation.\\nACL.\\n11', doc_id='7e61ab25-bbe8-4809-bd5c-8cf3773c8f9f', embedding=None, doc_hash='68b1e7b5e72fdb3dc9144f3233f3d9b1f21eeb74c03b62376818d508b6ab6682', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Zhengzhong Liang, Steven Bethard, and Mihai Surdeanu. 2021. Explainable multi-hop verbal\\nreasoning through internal monologue. NAACL .\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale\\ngeneration: Learning to solve and explain algebraic word problems. ACL.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021.\\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language\\nprocessing. arXiv preprint arXiv:2107.13586 .\\nBodhisattwa Prasad Majumder, Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley.\\n2021. Rationale-inspired natural language explanations with commonsense. arXiv preprint\\narXiv:2106.13876 .\\nAna Marasovi ´c, Iz Beltagy, Doug Downey, and Matthew E Peters. 2022. Few-shot self-rationalization\\nwith natural language prompts. NAACL Findings .\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and\\nfactuality in abstractive summarization. In ACL.\\nShen Yun Miao, Chao Chun Liang, and Keh Yih Su. 2020. A diverse corpus for evaluating and\\ndeveloping English math word problem solvers. ACL.\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke\\nZettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work?\\narXiv preprint arXiv:2202.12837 .\\nSharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan.\\n2020. WT5?! Training text-to-text models to explain their predictions. arXiv preprint\\narXiv:2004.14546 .\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work:\\nScratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114 .\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to\\nfollow instructions with human feedback. arXiv preprint arXiv:2203.02155 .\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\\nsimple math word problems? NAACL .\\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\\nLuke Zettlemoyer. 2018. Deep contextualized word representations. NAACL .\\nXinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\\nWeizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473 .\\nPiotr Pi˛ ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\\nBERT’s mathematical abilities by predicting the order of reasoning. ACL.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models:\\nMethods, analysis & insights from training Gopher. arXiv preprint arXiv:2112.11446 .\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. Journal of Machine Learning Research , 21:1–67.\\nDheeraj Rajagopal, Vidhisha Balachandran, Eduard H. Hovy, and Yulia Tsvetkov. 2021. SelfExplain:\\nA self-explaining architecture for neural text classiﬁers. EMNLP .\\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain\\nyourself! Leveraging language models for commonsense reasoning. ACL.\\n12', doc_id='b68dff1b-ea83-44c5-bd42-90c3ea56ea7f', embedding=None, doc_hash='c088a1a7ac9006935e6d61f7c98589e7fe2d183add118467fb72c6670abaf207', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading\\ncomprehension with numerical reasoning. EMNLP .\\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Michael Collins, Dipanjan Das, Slav Petrov,\\nGaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language\\ngeneration models. arXiv preprint arXiv:2112.12870 .\\nGabriel Recchia. 2021. Teaching autoregressive language models complex tasks by demonstration.\\narXiv preprint arXiv:2109.02102 .\\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022.\\nA recipe for arbitrary text style transfer with large language models. ACL.\\nLaria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond\\nthe few-shot paradigm. Extended Abstracts of the 2021 CHI Conference on Human Factors in\\nComputing Systems .\\nSubhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. EMNLP .\\nSubhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about Quantities in Natural Language.\\nTACL .\\nMohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching\\nsoft rules to pre-trained language models. EMNLP .\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,\\nAntoine Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted\\ntraining enables zero-shot task generalization. ICLR .\\nJianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021.\\nGenerate & rank: A multi-task framework for math word problems. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2021 .\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A\\nquestion answering challenge targeting commonsense knowledge. NAACL .\\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-\\nthought: Teaching pre-trained models to systematically reason over implicit knowledge. NeurIPS .\\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and\\nJonathan Berant. 2021. CommonsenseQA 2.0: Exposing the limits of ai through gamiﬁcation.\\nNeurIPS Track on Datasets and Benchmarks .\\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven\\nZheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv\\npreprint arXiv:2205.05131 .\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for\\ndialog applications. arXiv preprint arXiv:2201.08239 .\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a.\\nSelf-consistency improves chain of thought reasoning in language models. arXiv preprint\\narXiv:2203.11171 .\\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana\\nArunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022b.\\nBenchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint\\narXiv:2204.07705 .\\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\\nAndrew M. Dai, and Quoc V . Le. 2022a. Finetuned language models are zero-shot learners. ICLR .\\n13', doc_id='b3d9a918-311a-4e3b-95f0-f6e7a186d72b', embedding=None, doc_hash='59a8abe8782e1a940eaf18667abe6f244923bff3e26055f80a66fea82ec3a4fc', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, et al. 2022b. Emergent abilities of large language\\nmodels. Transactions on Machine Learning Research .\\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing\\nhuman-AI collaboration for generating free-text explanations. NAACL .\\nSarah Wiegreffe and Ana Marasovi ´c. 2021. Teach me to explain: A review of datasets for explainable\\nNLP. NeurIPS .\\nSarah Wiegreffe, Ana Marasovi ´c, and Noah A. Smith. 2021. Measuring association between labels\\nand free-text rationales. EMNLP .\\nTongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and\\nCarrie J Cai. 2022a. PromptChainer: Chaining large language model prompts through visual\\nprogramming. CHI Extended Abstracts .\\nTongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022b. AI chains: Transparent and controllable\\nhuman-AI interaction by chaining large language model prompts. CHI.\\nYujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. 2020.\\nNeural execution engines: Learning to execute subroutines. NeurIPS .\\nHuihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, and Xiang Ren. 2021. Reﬁning language models\\nwith compositional explanations. NeurIPS .\\nXi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot in-context learning.\\narXiv preprint arXiv:2205.03401 .\\nYordan Yordanov, Vid Kocijan, Thomas Lukasiewicz, and Oana-Maria Camburu. 2021. Few-shot\\nout-of-domain transfer learning of natural language explanations. arXiv preprint arXiv:2112.06204 .\\nOmar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve\\nmachine learning for text categorization. NAACL .\\nWojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv:1410.4615 .\\nEric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. STaR: Bootstrapping reasoning with\\nreasoning. arXiv preprint arXiv:2203.14465 .\\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use:\\nImproving few-shot performance of language models. ICML .\\nWangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and\\nJian Tang. 2020. Towards interpretable natural language understanding with explanations as latent\\nvariables. NeurIPS .\\n14', doc_id='5bea457d-65f2-4956-a8e2-7ca02af80cbb', embedding=None, doc_hash='90a6184af20f266773699982ddd622e42150d0c0c3b4901916e7ee12de076440', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Checklist\\n1. For all authors...\\n(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\\ncontributions and scope? [Yes]\\n(b)Did you describe the limitations of your work? [Yes] See Section 6 and Appendix A.2.\\n(c)Did you discuss any potential negative societal impacts of your work? [Yes] We don’t\\nexpect negative societal impacts as a direct result of the contributions in our paper. One\\nconsideration, however, is that generated chain of thought is not always factual, which\\nis noted as a limitation in Appendix D.1 (and note that we do not suggest using such\\nchains of thought in a factual manner or in any real-world scenario).\\n(d)Have you read the ethics review guidelines and ensured that your paper conforms to\\nthem? [Yes]\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n3. If you ran experiments...\\n(a)Did you include the code, data, and instructions needed to reproduce the main experi-\\nmental results (either in the supplemental material or as a URL)? [Yes] We included\\ninputs, outputs, and targets for LaMDA and GPT-3 in the supplementary material.\\nAlthough we use proprietary models, we GPT-3 results are fully reproducible. Repro-\\nducibility is further discussed in Appendix E.1.\\n(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they\\nwere chosen)? [Yes] Data splits were speciﬁed, N/A for hyperparams.\\n(c)Did you report error bars (e.g., with respect to the random seed after running exper-\\niments multiple times)? [Yes] Standard deviation for multiple seeds using LaMDA\\n137B, where each seed is a different random order of exemplars, is given in Table 6\\nand Table 7.\\n(d)Did you include the total amount of compute and the type of resources used (e.g., type\\nof GPUs, internal cluster, or cloud provider)? [Yes] Type of resources are described in\\nAppendix E.2, though we did not estimate the total amount of compute.\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n(a)If your work uses existing assets, did you cite the creators? [Yes] We used two models\\nthat we anonymized based on the recommendation of the NeurIPS chairs. These models\\nwill be cited in the camera-ready version of the paper.\\n(b) Did you mention the license of the assets? [Yes] See Appendix E.3.\\n(c)Did you include any new assets either in the supplemental material or as a URL? [Yes]\\nThe coinﬂip and last letter concatenation datasets are the only new assets, and they are\\ngiven in the Supplementary Materials.\\n(d)Did you discuss whether and how consent was obtained from people whose data you’re\\nusing/curating? [N/A] No human data collected.\\n(e)Did you discuss whether the data you are using/curating contains personally identiﬁable\\ninformation or offensive content? [N/A] No human data collected.\\n5. If you used crowdsourcing or conducted research with human subjects...\\n(a)Did you include the full text of instructions given to participants and screenshots, if\\napplicable? [N/A]\\n(b)Did you describe any potential participant risks, with links to Institutional Review\\nBoard (IRB) approvals, if applicable? [N/A]\\n(c)Did you include the estimated hourly wage paid to participants and the total amount\\nspent on participant compensation? [N/A]\\n15', doc_id='8a277325-faf8-4894-95ea-12dbecd6334a', embedding=None, doc_hash='c4bf6631f95ec39d00f1571d1e293225c1ecd81a3cdf216a02d25fe4ea6af37a', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='A Frequently Asked Questions\\nA.1 Why does increasing model scale improve chain-of-thought prompting?\\nThe ﬁnding that successful chain-of-thought reasoning predictably emerges only at certain model\\nscales is intriguing. Scaling up language models has been shown to confer beneﬁts such as improved\\nperformance and sample efﬁciency (Kaplan et al., 2020), but chain-of-thought reasoning is emergent\\nin the sense that its success cannot be predicted only by extrapolating the performance of small scale\\nmodels, as chain of thought actually hurts performance for most models smaller than 10B parameters.\\nThe question of why model scale improves chain-of-thought prompting is certainly multi-faceted, and\\nwe made a preliminary attempt to shed insight into it via error analysis. This small analysis involved\\nmanually reading 45 errors made by PaLM 62B and categorizing them into semantic understanding\\n(20 errors), one step missing (18 errors), and other errors (7 errors). The “other category” included\\nhallucinations, repetitive outputs, and symbol mapping errors. This categorization is a coarse one\\nborrowed from the initial error analysis done on LaMDA in Appendix D.2, for which categories were\\nconceived based on what improvements were needed to make the chain of thought correct.\\nAs shown in Figure 9, scaling PaLM to 540B parameters ﬁxed a substantial portion of errors in all\\nthree categories. Examples of semantic understanding and one-step missing errors that were ﬁxed by\\nscaling PaLM to 540B are given in Figure 10. This result appears consistent with a hypothesis that\\nlanguage models acquire a range of semantic understanding and logical reasoning skills as a function\\nof model scale (though note that model scale is often conﬂated with other factors, such as amount of\\ntraining compute).\\nSemantic understanding\\n(62B made 20 errors of this type, 540B ﬁxes 6 of them)One step missing\\n(62B made 18 errors of this type, 540B ﬁxes 12 of them)Other\\n(62B made 7 errors of this type, 540B ﬁxes 4 of them)Types of errors made by a 62B language model:Errors ﬁxed by scaling from 62B to 540B\\nFigure 9: Error analysis of 45 problems that PaLM 62B got incorrect. These errors were categorized\\nthat semantic understanding, one step missing, and other. The other category includes hallucinations,\\nrepetitive outputs, and symbol mapping errors. Scaling PaLM to 540B ﬁxed a substantial portion of\\nerrors in all categories.\\nThere are also three notable points regarding why small language models fail. The ﬁrst observation\\nis that small language models fail at even relatively easy symbol mapping tasks. As demonstrated\\nin Section 5, for even symbolic reasoning tasks that only require generalization to new examples\\nusing the same chain of thought logical structure that was given in the few-shot exemplars, small\\nlanguage models still failed. The second observation is that small language models seem to have\\ninherently weaker arithmetic abilities, as shown by Brown et al. (2020), the ability to do simple\\narithmetic operations (without semantic understanding) requires sufﬁcient model scale. Finally, we\\nnoticed qualitatively that small language models often did not generate a ﬁnal answer that could be\\nparsed, due to either repetitions or logic that never arrived at a ﬁnal answer.\\nIn summary, the success of chain-of-thought reasoning as a result of model scale is a complicated\\nphenomena that likely involves a variety of emergent abilities (semantic understanding, symbol\\nmapping, staying on topic, arithmetic ability, faithfulness, etc). Future work could more thoroughly\\ninvestigate what properties of pretraining data, model architecture, and optimization objective causally\\nenable such reasoning capabilities.\\n16', doc_id='3324d53e-1705-41fc-bde8-9f4a37625bef', embedding=None, doc_hash='1725b9443ebfc0152f6d8cafee43e66b0a3f9a4f308c19e80ed061f104fa7acb', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text=\"Tracy used a piece of wire 4 feet long to support tomato plants in the garden. The wire was cut into pieces 6 inches long. How many pieces did she obtain?She cut the wire into 6 inch pieces. This means she got 4 * 6 = 24 pieces. The answer is 24. (Incorrect: semantic understanding error)62B Model OutputQuestion540B Model OutputThe wire was 4 feet long. This means it was 4 * 12 = 48 inches long. It was cut into pieces 6 inches long. This means she obtained 48 / 6 = 8 pieces. The answer is 8. (Correct)Tom's ship can travel at 10 miles per hour. He is sailing from 1 to 4 PM. He then travels back at a rate of 6 mph. How long does it take him to get back?He travels from 1 to 4 PM. That means he travels for 3 hours. He travels back at 6 mph. That means he travels for 3 * 6 = 18 miles. The answer is 18. (Incorrect: semantic understanding error)He travels at 10 mph from 1 to 4 PM. This means he travels 3 hours. 3 hours at 10 mph means he travels 3 * 10 = 30 miles. He then travels back at 6 mph. This means he travels 6 miles per hour. He has to travel 30 miles, so it takes him 30 / 6 = 5 hours. The answer is 5. (Correct)Stephen placed an online order for groceries. His final bill came to $40.00. Because this was through a delivery vendor, they tacked on a 25% fee to his final total and charged him $3.00 in delivery fees. Stephen also added a $4.00 tip. After the extra fees, what was the final price of Stephen's groceries?The final price of Stephen's groceries was $40.00. The delivery vendor added 25% to the final total, so the final total was 125% of $40.00. This means the final total was 125% * $40.00 = $50.00. The delivery vendor also charged $3.00 in delivery fees. So the final price of Stephen's groceries was $50.00 + $3.00 = $53.00. The answer is $53.00. (Incorrect: one step missing error)Stephen's bill was $40.00. The delivery vendor added 25% to the bill. This means the bill was increased by 25% of $40.00. This is 0.25 * $40.00 = $10.00. The delivery fee was $3.00. The tip was $4.00. So the total bill was $40.00 + $10.00 + $3.00 + 540b model $4.00 = $57.00. The answer is 57. (Correct)There are four schools competing at a basketball tournament. Each school has sent a girls’ basketball team and a boys’ basketball team and each team has 5 players each. Each school has also sent a coach for each team. In total, how many people have all of the schools sent?There are 4 schools. Each school has sent a girls’ basketball team and a boys’ basketball team. Each team has 5 players each. That means there are 4 * 2 * 5 = 40 players. Each team has a coach. That means there are 40 + 4 = 44 coaches. The answer is 44. (Incorrect: one step missing error)Each school has sent 2 teams, each with 5 players. This means each school has sent 2 * 5 = 10 players. Each school has also sent 2 coaches. This means each school has sent 10 + 2 = 12 people. There are 4 schools, so in total all of the schools have sent 4 * 12 = 48 people. The answer is 48. (Correct)62B Model Output\\n62B Model Output\\n62B Model Output540B Model Output\\n540B Model Output\\n540B Model OutputQuestion\\nQuestion\\nQuestionFigure 10: Examples of semantic understanding and one-step missing errors that were ﬁxed by\\nscaling PaLM from 62B to 540B.\\nA.2 What is the role of prompt engineering?\\nOne of the key considerations of prompting is sensitivity to the exact prompt. There is no shortage\\nof work showing that prompts affect language models in unexpected ways (Min et al., 2022). The\\ngeneral way that we created chain of thought annotations was by taking eight exemplars from the\\ntraining set and decomposing the reasoning process into multiple steps leading to the ﬁnal answer.\\nExamples of chain of thought annotations are provided in Figure 3, with full prompts given in\\nAppendix G. To analyze how sensitive chain of thought is to prompt engineering, we performed\\nrobustness experiments with respect to various factors.\\n•Different annotators. We ﬁrst analyze robustness to three different annotators (Section 3.4 and\\nFigure 6). Although there is notable variance in performance (which we will discuss later), chain\\nof thought performed better than the baseline by a large margin for all three annotators on eight\\ndatasets in arithmetic, commonsense, and symbolic reasoning (Table 6 and Table 7). Similar to the\\nannotation process in Cobbe et al. (2021), annotators were not given speciﬁc instructions about\\n17\", doc_id='b4032ef6-02fe-41d0-bc0c-77715b8b76d6', embedding=None, doc_hash='f2dcdab9d77f0af134f1c74d6e02c3a824ed8164012a081940bde5e55766c18e', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='how to write the chain of thought annotations other than to simply write the step-by-step reasoning\\nprocess that led to the ﬁnal answer. Thus, the annotations were written in each annotator’s own\\nlinguistic “chain of thought” writing style.\\n•Annotators without machine learning background. The GSM8K dataset (Cobbe et al., 2021)\\nconveniently provides a training set with reasoning chains written by crowd compute workers,\\nwhich enables us to investigate whether chain of thought still works with reasoning chains from an\\nindependent source without a background in machine learning. So we randomly sampled three sets\\nof eight exemplars with chains of thought from GSM8K. These chain of thought annotations also\\noutperformed the baseline by a large margin for all four arithmetic datasets (Table 6), indicating\\nthat chain of thought is not dependent on a particular set of annotators.\\n•Different exemplars. The different GSM8K exemplars experiment above (Table 6) also shows\\nthat chain-of-thought prompting works for different sets of exemplars. Notably, we test every set of\\nexemplars on all four arithmetic datasets (instead of picking exemplars from the training set for\\neach dataset), which suggests that the exemplars do not necessarily have to come from the same\\ndataset distribution as the test examples.\\n•Different order of exemplars. Prior work has shown that in some cases (e.g., classiﬁcation) even\\nthe order of prompts matter—varying the permutation of few-shot exemplars can cause the accuracy\\nof GPT-3 on SST-2 to range from near chance (54.3%) to near SOTA (93.4%) (Zhao et al., 2021).\\nWe show the standard deviation of performance from different exemplars in Table 6 and Table 7.\\nStandard deviations with respect to prompt order are relatively minimal in almost all cases. The\\none exception is the coin ﬂip task, for which exemplar orders have high standard deviation, likely\\nfor the reason cited in Zhao et al. (2021)—for classiﬁcation, many exemplars of the same category\\nin a row biases the model outputs).\\n•Different number of exemplars. We also found that gains from chain-of-thought prompting\\ngenerally still held when there was a varying number of few-shot exemplars. This is shown for ﬁve\\ndatasets in Figure 11 (we did not have the compute to run this for all datasets). We also found in\\npreliminary experiments that further increasing the number of exemplars in standard prompting\\ndid not lead to signiﬁcant gains (e.g., increasing from 8 to 16 exemplars did not improve the\\nperformance of standard prompting enough to catch up with chain-of-thought prompting).\\n•Different language models. Another interesting question is whether certain prompts that work\\nbetter for one model work better for other large language models. We ﬁnd that with the same\\nprompts, chain-of-thought prompting improves performance across all three models (LaMDA,\\nGPT-3, and PaLM) for all datasets except CSQA and StrategyQA for GPT-3 (Table 1, Table 4,\\nTable 5). The fact that gains from chain of thought did not transfer perfectly among models is\\na limitation; further work could investigate why how different pre-training datasets and model\\narchitectures affect the performance gain from chain-of-thought prompting.\\nPrompt engineering still matters, though. Although the results are relatively robust to the prompt\\nfor arithmetic reasoning, we want to be clear that prompt engineering still does matter, and can\\nimprove performance signiﬁcantly in many cases. Though most chain of thought annotations\\noutperform standard prompting, there is large variation in many cases. For instance, for the coin\\nﬂip task, the performance varied from 99.6% for Annotator A to 71.4% for Annotator C, though\\nboth were above standard prompting = 50.0% (see Table 7). There are even tasks where prompt\\nengineering is a requirement for good performance. In preliminary experiments, we tried using chain\\nof thought to enable language models to reverse the order of a list of 5 items. While two co-authors\\nwere not able to write chain of thought prompts that solved the task despite their best attempts, a third\\nco-author was able to write a chain of thought that perfectly solved the task.\\nHow to generate chain of thought annotations in a robust fashion could be an interesting direction\\nfor future work. For instance, an idea here could be to use a large language model to automatically\\ngenerate chains of thought via prompting (and potentially optimize this over a validation set).\\nA.3 Will chain-of-thought prompting improve performance for my task of interest?\\nWhile chain-of-thought prompting is in principle applicable for any text-to-text task, it is more\\nhelpful for some tasks than others. Based on the experiments in this paper, our intuition is that chain\\nof thought helps the most when three conditions are met: (1) the task is challenging and requires\\n18', doc_id='82f49a7b-1b00-47b4-9fa4-76d54af1a4a3', embedding=None, doc_hash='7d53a9f051c845097dba0324ba4d0854a9ccc26927c87397eff36932ce6a12f9', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='multi-step reasoning, (2) a large language model is used, and (3) the scaling curve is relatively ﬂat.\\nConversely, the beneﬁts are smaller when one or more of these conditions are not met.\\nThese intuitions are perhaps supported by the arithmetic reasoning results. The performance gain\\nfrom chain-of-thought prompting is largest for PaLM 540B on GSM8K (challenging multi-step\\nproblems, ﬂat scaling curve), which meets these conditions. The performance gain is small for the\\nsubsets of MAWPS that only require one or two steps (SingleOP, SingleEq, and AddSub), for which\\nPaLM 540B already achieves performance of 90% or higher (and it is also generally true that there is\\nless headroom for improvement when performance is already strong).\\nAlthough in this paper we focused on multi-step reasoning tasks (arithmetic, commonsense, and\\nsymbolic), chain-of-thought prompting can potentially be applied to any task for which humans use a\\n“chain of thought” to solve (at least in principle). We leave the empirical evaluation of chain-of-thought\\nprompting on such diverse tasks (e.g., machine translation, etc.) to future work.\\nA.4 Why is prompting with the equation only not enough for some arithmetic reasoning\\ndatasets?\\nPrompting with the equation only as an intermediate step does help on many datasets, especially when\\nthe datasets only require a few reasoning steps (SV AMP, ASDiv, MAWPS). For GSM8K, however,\\nusing the equation only did not improve performance substantially. Based on qualitative analysis, we\\nbelieve that these questions are too semantically challenging for the model to directly translate them\\ninto a math equation. Consider this example from LaMDA 137B:\\nQUESTION :Mike plays ping pong for 40 minutes. In the ﬁrst 20 minutes, he scores 4\\npoints. In the second 20 minutes, he scores 25% more points. How many total points did he\\nscore?\\nEQUATION ONLY (WRONG ANSWER ):(4 + 20 * 0.25) = 6. The answer is 6.\\nCHAIN OF THOUGHT (CORRECT ):Mike played ping pong for 40 minutes. In the ﬁrst\\n20 minutes, he scored 4 points. In the second 20 minutes, he scored 25% more points. So\\nhe scored 25% more in the second 20 minutes. 4 x 1.25 = 5. So he scored 5 points in the\\nsecond 20 minutes. So he scored 9 points in total. The answer is 9.\\nIt is hard for the model to directly translate all of the semantics into a single equation, but chain of\\nthought allows it to better reason about each part of the question via intermediate steps in natural\\nlanguage.\\n19', doc_id='ef11b412-f282-4191-be02-d253cd64f6b2', embedding=None, doc_hash='09b3f1d9c7b9186cb0b2b1e6da1446bb5ddc6b35b62404ad173f3be1db5a4156', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='B All Experimental Results\\nThis section contains tables for experimental results for varying models and model sizes, on all\\nbenchmarks, for standard prompting vs. chain-of-thought prompting.\\nFor the arithmetic reasoning benchmarks, some chains of thought (along with the equations produced)\\nwere correct, except the model performed an arithmetic operation incorrectly. A similar observation\\nwas made in Cobbe et al. (2021). Hence, we can further add a Python program as an external\\ncalculator (using the Python eval function) to all the equations in the generated chain of thought.\\nWhen there are multiple equations in a chain of thought, we propagate the external calculator results\\nfrom one equation to the following equations via string matching. As shown in Table 1, we see that\\nadding a calculator signiﬁcantly boosts performance of chain-of-thought prompting on most tasks.\\nTable 1: Chain of thought prompting outperforms standard prompting for various large language\\nmodels on ﬁve arithmetic reasoning benchmarks. All metrics are accuracy (%). Ext. calc.: post-hoc\\nexternal calculator for arithmetic computations only. Prior best numbers are from the following. a:\\nCobbe et al. (2021). b&e: Pi et al. (2022), c: Lan et al. (2021), d: Pi˛ ekos et al. (2021).\\nPrompting GSM8K SV AMP ASDiv AQuA MAWPS\\nPrior best N/A (ﬁnetuning) 55a57.4b75.3c37.9d88.4e\\nUL2 20B Standard 4.1 10.1 16.0 20.5 16.6\\nChain of thought 4.4 (+0.3) 12.5 (+2.4) 16.9 (+0.9) 23.6 (+3.1) 19.1 (+2.5)\\n+ ext. calc 6.9 28.3 34.3 23.6 42.7\\nLaMDA 137B Standard 6.5 29.5 40.1 25.5 43.2\\nChain of thought 14.3 (+7.8) 37.5 (+8.0) 46.6 (+6.5) 20.6 (-4.9) 57.9 (+14.7)\\n+ ext. calc 17.8 42.1 53.4 20.6 69.3\\nGPT-3 175B Standard 15.6 65.7 70.3 24.8 72.7\\n(text-davinci-002) Chain of thought 46.9 (+31.3) 68.9 (+3.2) 71.3 (+1.0) 35.8 (+11.0) 87.1 (+14.4)\\n+ ext. calc 49.6 70.3 71.1 35.8 87.5\\nCodex Standard 19.7 69.9 74.0 29.5 78.7\\n(code-davinci-002) Chain of thought 63.1 (+43.4) 76.4 (+6.5) 80.4 (+6.4) 45.3 (+15.8) 92.6 (+13.9)\\n+ ext. calc 65.4 77.0 80.0 45.3 93.3\\nPaLM 540B Standard 17.9 69.4 72.1 25.2 79.2\\nChain of thought 56.9 (+39.0) 79.0 (+9.6) 73.9 (+1.8) 35.8 (+10.6) 93.3 (+14.2)\\n+ ext. calc 58.6 79.8 72.6 35.8 93.5\\n20', doc_id='1012679a-f2f3-4971-ae29-dff562ba0f0f', embedding=None, doc_hash='8cff8e3140da32304bda2b4bd3f70d45febe8b4160d3e902b366d44a0231c1d2', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 2: Standard prompting versus chain of thought prompting on ﬁve arithmetic reasoning bench-\\nmarks. Note that chain of thought prompting is an emergent ability of model scale—it does not\\npositively impact performance until used with a model of sufﬁcient scale.\\nGSM8K SV AMP ASDiv AQuA MAWPS\\nModel standard CoT standard CoT standard CoT standard CoT standard CoT\\nUL2 20B 4.1 4.4 10.1 12.5 16.0 16.9 20.5 23.6 16.6 19.1\\nLaMDA 420M 2.6 0.4 2.5 1.6 3.2 0.8 23.5 8.3 3.2 0.9\\n2B 3.6 1.9 3.3 2.4 4.1 3.8 22.9 17.7 3.9 3.1\\n8B 3.2 1.6 4.3 3.4 5.9 5.0 22.8 18.6 5.3 4.8\\n68B 5.7 8.2 13.6 18.8 21.8 23.1 22.3 20.2 21.6 30.6\\n137B 6.5 14.3 29.5 37.5 40.1 46.6 25.5 20.6 43.2 57.9\\nGPT 350M 2.2 0.5 1.4 0.8 2.1 0.8 18.1 8.7 2.4 1.1\\n1.3B 2.4 0.5 1.5 1.7 2.6 1.4 12.6 4.3 3.1 1.7\\n6.7B 4.0 2.4 6.1 3.1 8.6 3.6 15.4 13.4 8.8 3.5\\n175B 15.6 46.9 65.7 68.9 70.3 71.3 24.8 35.8 72.7 87.1\\nCodex - 19.7 63.1 69.9 76.4 74.0 80.4 29.5 45.3 78.7 92.6\\nPaLM 8B 4.9 4.1 15.1 16.8 23.7 25.2 19.3 21.7 26.2 30.5\\n62B 9.6 29.9 48.2 46.7 58.7 61.9 25.6 22.4 61.8 80.3\\n540B 17.9 56.9 69.4 79.0 72.1 73.9 25.2 35.8 79.2 93.3\\nTable 3: Standard prompting versus chain of thought prompting on the four subsets of the MAWPS\\nbenchmark. The point of stratifying the MAWPS benchmark is to show that performance gains are\\nminimal on easy one-step or two-step problems where large language models already achieve high\\nperformance (e.g., SingleOp, SingleEq, and AddSub).\\nSingleOp SingleEq AddSub MultiArith\\nModel standard CoT standard CoT standard CoT standard CoT\\nUL2 20B 24.9 27.2 18.0 20.2 18.5 18.2 5.0 10.7\\nLaMDA 420M 2.8 1.0 2.4 0.4 1.9 0.7 5.8 1.5\\n2B 4.6 4.1 2.4 3.3 2.7 3.2 5.8 1.8\\n8B 8.0 7.0 4.5 4.4 3.4 5.2 5.2 2.4\\n68B 36.5 40.8 23.9 26.0 17.3 23.2 8.732.4\\n137B 73.2 76.2 48.8 58.7 43.0 51.9 7.644.9\\nGPT 350M 3.2 1.8 2.0 0.2 2.0 1.5 2.3 0.8\\n1.3B 5.3 3.0 2.4 1.6 2.3 1.5 2.2 0.5\\n6.7B 13.5 3.9 8.7 4.9 8.6 2.5 4.5 2.8\\n175B 90.9 88.8 82.7 86.6 83.3 81.3 33.8 91.7\\nCodex - 93.1 91.8 86.8 93.1 90.9 89.1 44.0 96.2\\nPaLM 8B 41.8 46.6 29.5 28.2 29.4 31.4 4.215.8\\n62B 87.9 85.6 77.2 83.5 74.7 78.2 7.373.7\\n540B 94.1 94.1 86.5 92.3 93.9 91.9 42.2 94.7\\n21', doc_id='9d928f50-4140-4cd8-911d-42ad4b224974', embedding=None, doc_hash='1aa0e45c0c48d31205bccdec467d6e8d47c53734ccd8e0f17a86b3bf99bd1844', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 4: Standard prompting versus chain of thought prompting on ﬁve commonsense reasoning\\nbenchmarks. Chain of thought prompting is an emergent ability of model scale—it does not positively\\nimpact performance until used with a model of sufﬁcient scale.\\nCSQA StrategyQA Date Sports SayCan\\nModel standard CoT standard CoT standard CoT standard CoT standard CoT\\nUL2 20B 34.2 51.4 59.0 53.3 13.5 14.0 57.9 65.3 20.0 41.7\\nLaMDA 420M 20.1 19.2 46.4 24.9 1.9 1.6 50.0 49.7 7.5 7.5\\n2B 20.2 19.6 52.6 45.2 8.0 6.8 49.3 57.5 8.3 8.3\\n8B 19.0 20.3 54.1 46.8 9.5 5.4 50.0 52.1 28.3 33.3\\n68B 37.0 44.1 59.6 62.2 15.5 18.6 55.2 77.5 35.0 42.5\\n137B 53.6 57.9 62.4 65.4 21.5 26.8 59.5 85.8 43.3 46.6\\nGPT 350M 14.7 15.2 20.6 0.9 4.3 0.9 33.8 41.6 12.5 0.8\\n1.3B 12.0 19.2 45.8 35.7 4.0 1.4 0.0 26.9 20.8 9.2\\n6.7B 19.0 24.0 53.6 50.0 8.9 4.9 0.0 4.4 17.5 35.0\\n175B 79.5 73.5 65.9 65.4 43.8 52.1 69.6 82.4 81.7 87.5\\nCodex - 82.3 77.9 67.1 73.2 49.0 64.8 71.7 98.5 85.8 88.3\\nPaLM 8B 19.8 24.9 55.6 53.5 12.9 13.1 55.1 75.2 34.2 40.0\\n62B 65.4 68.1 58.4 63.4 29.8 44.7 72.1 93.6 65.8 70.0\\n540B 78.1 79.9 68.6 77.8 49.0 65.3 80.5 95.4 80.8 91.7\\nTable 5: Standard prompting versus chain of thought prompting enables length generalization to\\nlonger inference examples on two symbolic manipulation tasks.\\nLast Letter Concatenation Coin Flip (state tracking)\\n2 OOD: 3 OOD: 4 2 OOD: 3 OOD: 4\\nModel standard CoT standard CoT standard CoT standard CoT standard CoT standard CoT\\nUL2 20B 0.6 18.8 0.0 0.2 0.0 0.0 70.4 67.1 51.6 52.2 48.7 50.4\\nLaMDA 420M 0.3 1.6 0.0 0.0 0.0 0.0 52.9 49.6 50.0 50.5 49.5 49.1\\n2B 2.3 6.0 0.0 0.0 0.0 0.0 54.9 55.3 47.4 48.7 49.8 50.2\\n8B 1.5 11.5 0.0 0.0 0.0 0.0 52.9 55.5 48.2 49.6 51.2 50.6\\n68B 4.4 52.0 0.0 0.8 0.0 2.5 56.2 83.2 50.4 69.1 50.9 59.6\\n137B 5.8 77.5 0.034.4 0.013.5 49.0 99.6 50.7 91.0 49.1 74.5\\nPaLM 8B 2.6 18.8 0.0 0.0 0.0 0.2 60.0 74.4 47.3 57.1 50.9 51.8\\n62B 6.8 85.0 0.059.6 0.013.4 91.4 96.8 43.9 91.0 38.3 72.4\\n540B 7.6 99.4 0.294.8 0.063.0 98.1 100.0 49.3 98.6 54.8 90.2\\n22', doc_id='34e8e52a-43ad-4dd9-882e-8f1936a13ca0', embedding=None, doc_hash='f905eed02704fcde7952323189c5e39435e30848a864b7b5f5f6a930f187709b', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 6: Ablation and robustness results for arithmetic reasoning datasets. Chain of thought generally\\noutperforms ablations by a large amount. “Equation only” performs in between standard prompting\\nand chain of thought prompting, as it allows for intermediate reasoning steps via equations but does\\nnot leverage natural language. Chain of thought prompting has variance (as expected) when used\\nwith prompts written by different annotators or when using other exemplars, but still outperforms\\nstandard prompting by a large margin. Standard deviation shown is for different order of few-shot\\nprompting exemplars, with ﬁve different random seeds. Results here are shown for LaMDA 137B, as\\nadditional queries for GPT-3 and PaLM are both limited and expensive.\\nGSM8K SV AMP ASDiv MAWPS\\nStandard prompting 6.5 \\x060.4 29.5\\x060.6 40.1\\x060.6 43.2\\x060.9\\nChain of thought prompting 14.3 \\x060.4 36.7\\x060.4 46.6\\x060.7 57.9\\x061.5\\nAblations\\n\\x01equation only 5.4 \\x060.2 35.1\\x060.4 45.9\\x060.6 50.1\\x061.0\\n\\x01variable compute only 6.4 \\x060.3 28.0\\x060.6 39.4\\x060.4 41.3\\x061.1\\n\\x01reasoning after answer 6.1 \\x060.4 30.7\\x060.9 38.6\\x060.6 43.6\\x061.0\\nRobustness\\n\\x01different annotator (B) 15.5 \\x060.6 35.2\\x060.4 46.5\\x060.4 58.2\\x061.0\\n\\x01different annotator (C) 17.6 \\x061.0 37.5\\x062.0 48.7\\x060.7 60.1\\x062.0\\n\\x01intentionally concise style 11.1 \\x060.3 38.7\\x060.8 48.0\\x060.3 59.6\\x060.7\\n\\x01exemplars from GSM8K ( \\x0b) 12.6 \\x060.6 32.8\\x061.1 44.1\\x060.9 53.9\\x061.1\\n\\x01exemplars from GSM8K ( \\x0c) 12.7 \\x060.5 34.8\\x061.1 46.9\\x060.6 60.9\\x060.8\\n\\x01exemplars from GSM8K ( \\r) 12.6 \\x060.7 35.6\\x060.5 44.4\\x062.6 54.2\\x064.7\\nTable 7: Ablation and robustness results for four datasets in commonsense and symbolic reasoning.\\nChain of thought generally outperforms ablations by a large amount. Chain of thought prompting has\\nvariance (as expected) when used with prompts written by different annotators or when using other\\nexemplars, but still outperforms standard prompting by a large margin. Standard deviation shown\\nis for different order of few-shot prompting exemplars, with ﬁve different random seeds. Results\\nhere are shown for LaMDA 137B, as additional queries for GPT-3 and PaLM are both limited and\\nexpensive. The exception is that we run SayCan using PaLM here, as the SayCan evaluation set is\\nonly 120 examples and therefore less expensive to run multiple times.\\nCommonsense Symbolic\\nDate Sports SayCan Concat Coin\\nStandard prompting 21.5 \\x060.6 59.5\\x063.0 80.8\\x061.8 5.8\\x060.6 49.0\\x062.1\\nChain of thought prompting 26.8 \\x062.1 85.8\\x061.8 91.7\\x061.4 77.5\\x063.8 99.6\\x060.3\\nAblations\\n\\x01variable compute only 21.3 \\x060.7 61.6\\x062.2 74.2\\x062.3 7.2\\x061.6 50.7\\x060.7\\n\\x01reasoning after answer 20.9 \\x061.0 63.0\\x062.0 83.3\\x060.6 0.0\\x060.0 50.2\\x060.5\\nRobustness\\n\\x01different annotator (B) 27.4 \\x061.7 75.4\\x062.7 88.3\\x061.4 76.0\\x061.9 77.5\\x067.9\\n\\x01different annotator (C) 25.5 \\x062.5 81.1\\x063.6 85.0\\x061.8 68.1\\x062.2 71.4\\x0611.1\\n23', doc_id='4f95b555-9359-411a-8cf1-5f02a61689cc', embedding=None, doc_hash='fd957369ce4da44417ecdbad08856cfc170fa009e6e7980ab39573a76c35901b', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='C Extended Related Work\\nChain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\\ning, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\\nintermediate language steps.\\nC.1 Prompting\\nThe recent success of large-scale language models has led to growing interest in improving their\\ncapability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\\nsurvey). This paper falls in the category of general prompting approaches, whereby input prompts are\\noptimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\\n2021; Lester et al., 2021; Reif et al., 2022, inter alia ).\\nOne recent line of work aims to improve the ability of language models to perform a task by providing\\ninstructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\\net al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\\npairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\\nprepended to the inputs), chain-of-thought prompting augments the outputs of language models.\\nAnother related direction is sequentially combining the outputs of language models; human–computer\\ninteraction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\\nlanguage models improves task outcomes in a 20-person user study.\\nC.2 Natural language explanations\\nAnother closely related direction uses natural language explanations (NLEs), often with the goal of\\nimproving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi ´c, 2021, inter alia ). That\\nline of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\\n2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\\nprediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\\nthe chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\\nNLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\\nchain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\\nsteps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\\nwith NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\\nexplanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\\nand symbolic tasks that require multi-hop reasoning.\\nC.3 Program synthesis and execution\\nUsing intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\\nand Sutskever, 2014, inter alia ). Recent work along in this direction has included a number of\\narchitectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\\nlarge language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\\nours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\\naddition, evaluate polynomials, and execute python programs. Whereas generating a program and\\nthen executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\\nprimitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\\nprinciple.\\nC.4 Numeric and logical reasoning\\nNumeric and logical reasoning has been a long-studied task in machine learning and natural language\\nprocessing (Lev et al., 2004, inter alia ). Recent work has also aimed to inject numeric reasoning\\nabilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\\nexecutable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\\nusing specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\\nlanguage models to perform logical or formal reasoning, often by verablizing the rules in natural\\nlanguage formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\\n24', doc_id='d0e40b8a-1fe5-4da8-848c-c9b6624e9c48', embedding=None, doc_hash='18ccbe33c75c84aa7d47da63d52c204a84a045fcd87ac260a8ef3e5acbe1bc2b', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Perhaps the most-related work here is Recchia (2021), which shows that ﬁnetuning enables longhand\\nmodule operations, which has previously been difﬁcult for performers. Whereas work in this direction\\nis often task-speciﬁc and uses ﬁnetuning, we show that chain-of-thought prompting works for a broad\\nrange of tasks without any ﬁnetuning.\\nC.5 Intermediate language steps\\nExtensive prior work has shown the beneﬁts of endowing neural networks with the ability to produce\\nintermediate steps via training or ﬁnetuning confers various beneﬁts in a range of scenarios. As\\nexamples, it has been shown that natural language intermediate steps can improve performance\\n(Zaidan et al., 2007; Yao et al., 2021; Hase and Bansal, 2022; Gu et al., 2022), improve robustness\\n(Chen et al., 2022), speed up training (Hancock et al., 2018), mitigate bias (Dua et al., 2020), and\\neven help in image and reinforcement learning settings (Andreas et al., 2018). To endow models with\\nthe ability to produce intermediate steps, prior work typically ﬁnetunes models on either manually\\nannotated training datasets (Camburu et al., 2018; Rajani et al., 2019, inter alia ) or generates synthetic\\ndatasets (Talmor et al., 2020; Zelikman et al., 2022). Compared with these training or ﬁnetuning\\nmethods, our work shows that various natural language reasoning abilities can be elicited in off-the-\\nshelf language models of sufﬁcient scale simply via prompting. This prompting setup is important\\nbecause it allows for intermediate step reasoning without a large number of labeled annotations, and\\nbecause a single model can perform a range of reasoning tasks without any gradient updates.\\nD Appendix: Additional Analysis\\nD.1 Correct Chain of Thought Analysis\\nAs mentioned in the main text, we analyze 50 chains of thought from LaMDA 137B that led to\\ncorrect answers in the GSM8K dataset. Of these 50, only one arrived at the correct answer through\\nincorrect reasoning (shown in Table 9: “correct by chance”). The other 49 had correct logic and\\nmath, with examples shown in Table 8. Five had minor imperfections while maintaining coherent and\\nunderstandable logic:\\n• One had underspeciﬁed statements (shown in Table 9: “correct but underspeciﬁed statements”).\\n•One made an unrelated but correct statement (shown in Table 9: “correct but unrelated statement”).\\n•Two had the correct logic but omitted speciﬁc steps in the equation, though it was still understand-\\nable (shown in Table 9: “correct but imperfect equation”).\\n•One had the correct math but inverted the semantics (shown in Table 9: “correct but inverted\\nsemantics”)\\nTable 8: Example of correct chains of thought produced by the model for the GSM8K dataset.\\nQUESTION :James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total\\nmeters does he run a week?\\nMODEL ANSWER (CORRECT): James decides to run 3 sprints 3 times a week. He runs 60 meters each\\nsprint. So he runs 60 meters x 3 sprints x 3 times a week. That is 60 meters x 9. The answer is 540. X\\nQUESTION :Brandon’s iPhone is four times as old as Ben’s iPhone. Ben’s iPhone is two times older than Suzy’s\\niPhone. If Suzy’s iPhone is 1 year old, how old is Brandon’s iPhone?\\nMODEL ANSWER (CORRECT): Brandon’s iPhone is 4 times as old as Ben’s iPhone. Ben’s iPhone is 2\\ntimes older than Suzy’s iPhone. So Brandon’s iPhone is 4 x 2 = 8 times older than Suzy’s iPhone. Suzy’s iPhone\\nis 1 year old. So Brandon’s iPhone is 8 x 1 = 8 years old. The answer is 8. X\\nQUESTION :Jean has 30 lollipops. Jean eats 2 of the lollipops. With the remaining lollipops, Jean wants to\\npackage 2 lollipops in one bag. How many bags can Jean ﬁll?\\nMODEL ANSWER (CORRECT): Jean started with 30 lollipops. She ate 2 of them. So she has 28 lollipops\\nleft. She wants to package 2 lollipops in one bag. So she can package 28 / 2 = 14 bags. The answer is 14. X\\nAlthough we ﬁnd that chain-of-thought reasoning is mostly correct for math questions where the ﬁnal\\nanswer was correct, this is likely because it is hard to arrive at the correct ﬁnal answer by coincidence\\nfor free response questions. For multiple choice or binary classiﬁcation problems, it was much more\\n25', doc_id='6a555f97-6483-43a2-8114-6ae48cf9fc57', embedding=None, doc_hash='6b47cc5435e14d0ff37546484cee99c96a95c43a25c9ce9e6abbf4dd6c63496b', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='likely that models could arrive at the correct answer via an incorrect reasoning path (e.g., all the\\ncommmonsense reasoning datasets we evaluate). This is a limitation, and future work should perform\\nan analysis of the factuality of such chains of thought.\\nTable 9: Of 50 examples that the model got correct, there were 7 salient cases where the chain of\\nthought the model generated was imperfect. We found the other 43 to be equivalent to what a human\\nwould potentially generate as a ground truth.\\nQUESTION :There are 36 penguins sunbathing in the snow. One-third of them jump in and swim in the ocean.\\nAnother one-third go inside the cave to eat their dinner. How many penguins are still left sunbathing?\\nMODEL ANSWER (CORRECT BY CHANCE ):There are 36 penguins. One-third of them jump in and swim in\\nthe ocean. So that is 12 penguins. Another one-third go inside the cave to eat their dinner. So that is 12 penguins.\\nThe answer is 12.\\nQUESTION :John has 2 houses with 3 bedrooms each. Each bedroom has 2 windows each. There are an\\nadditional 4 windows in each house not connected to bedrooms. How many total windows are there between the\\nhouses?\\nMODEL ANSWER (CORRECT BUT UNDERSPECIFIED STATEMENTS ):There are 2 houses with 3 bedrooms\\neach. Each bedroom has 2 windows each. So there are 2 x 3 = 6 windows in each house. There are an additional\\n4 windows in each house not connected to bedrooms. So there are 4 + 4 = 8 windows in each house. So there are\\n6 x 2 = 12 windows in each house. So there are 12 + 8 = 20 windows in both houses. The answer is 20.\\nQUESTION :If Ann is 9 years old and her brother is twice her age, how old will her brother be in 3 years?\\nMODEL ANSWER (CORRECT BUT UNRELATED STATEMENT ):Ann is 9 years old. Her brother is twice her\\nage. So her brother is 18 years old. In 3 years, she will be 12. So her brother will be 18 + 3 = 21 years old. The\\nanswer is 21.\\nQUESTION :A local gas station is selling gas for $3.00 a gallon. An app company is offering $.20 cashback per\\ngallon if you ﬁll up at this station. If someone buys 10 gallons of gas, how much with their gas be, after the\\ncashback rewards?\\nMODEL ANSWER (CORRECT BUT IMPERFECT EQUATION ):The gas is originally 3 dollars per gallon. If\\nyou buy 10 gallons, that is 3 x 10 = 30 dollars. If you get 20 cents off per gallon, that is 20 x 10 = 2 dollars. So\\nthe total is 30 - 2 = 28 dollars. The answer is 28 dollars.\\nQUESTION :Nissa hires 60 seasonal workers to play elves in her department store’s Santa village. A third of\\nthe elves quit after children vomit on them, then 10 of the remaining elves quit after kids kick their shins. How\\nmany elves are left?\\nMODEL ANSWER (CORRECT BUT IMPERFECT EQUATION ):Nissa hires 60 seasonal workers. A third of\\nthem quit. So 60 - 1/3 = 40 elves are left. Then 10 elves quit. So 40 - 10 = 30 elves are left. The answer is 30\\nelves.\\nQUESTION :Grandma Jones baked 5 apple pies for the ﬁreman’s luncheon. She cut each pie into 8 pieces and\\nset the ﬁve pies out on the buffet table for the guests to serve themselves. At the end of the evening, after the\\nguests had taken and eaten their pieces of pie, there were 14 pieces of pie remaining. How many pieces were\\ntaken by the guests?\\nMODEL ANSWER (CORRECT BUT INVERTED SEMANTICS ):5 pies were baked and cut into 8 pieces each.\\nThe 5 pies were then served to the guests. 8 x 5 = 40 pieces of pie. The guests ate 14 pieces of pie. 40 - 14 = 26\\npieces of pie were left. The answer is 26. X\\n26', doc_id='11f7a3a2-121a-4fc3-a24e-da5189e8c38d', embedding=None, doc_hash='e096c1d52ab8c6eaa4e68e34c50fbb630434b1daeab80d4b2c5228496b66d364', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='D.2 Incorrect Chain of Thought Analysis\\nWe also manually analyze 50 randomly sampled outputs of the model that were incorrect on GSM8K\\nfor LaMDA 137B. There are many ways that a chain of thought can be incorrect, making the design\\nof error categorization non-trivial. We decided to categorize errors into what changes are needed to\\nmake the chain of thought correct, with the goal of elucidating how the model can be improved in the\\nfuture.\\nWe found that many chains of thought can be made correct with one of the following three classes of\\nmodiﬁcation.\\n•Calculator error only. We found that 8% of the chains of thought were completely correct except\\nfor a calculator error—in other words, applying an external calculator to equations, as done in\\nCobbe et al. (2021), would make the chain of thought correct. An example of this type of error is\\nshown in Table 10: “calculator error only”. Indeed, the solve rate of chain-of-thought prompting\\non for LaMDA 137B GSM8K went up from 14.3% to 17.3% when we added a Python program\\nas an external calculator, as shown in Table 2. Also, 34% of the examples contained calculator\\nerrors in addition to other types of errors. However, we perform the rest of the error categorization\\nindependently of calculator errors.\\n•Symbol mapping error. We next found that 16% percent of the chains of thought were correct\\nexcept for what we call symbol mapping errors. We deﬁne a symbol mapping error as when the\\nchain of thought is correct except for the number symbols, and it could be made totally correct by\\nmodifying only the equations and not the words. As one might argue that they could simply place\\nthe correct ﬁnal equation in any chain of thought, we constrain this category to chains of thought\\nwhere the chain of thought can be modiﬁed to be a completely correct reasoning process (not just\\nﬁnal answer). An example of this error category is shown in Table 10: “symbol mapping error”.\\n•One step missing error. Our next category of error is chains of thought which were correct except\\nthat they were missing a single step. In other words, these chains of thoughts could be rewritten to\\nbe correct by adding in an additional reasoning step that was missed by the model. An example of\\nthis error category is shown in Table 10: “one step missing error”. We found that 22% percent of\\nthe errors fell into this category.\\nTable 10: Example of incorrect chains of thought, categorized as described in Appendix D.2.\\nQUESTION :Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns. How many thorns\\nare there total?\\nMODEL ANSWER (INCORRECT ;CALCULATOR ERROR ONLY ):Dan plants 3 rose bushes. Each rose bush\\nhas 25 roses. Each rose has 8 thorns. So 3 x 25 x 8 = 300. The answer is 300. 7\\nEXPLANATION FOR ERROR CATEGORY :The produced chain of thought could be made correct just by\\nrunning the equation through an external calculator, instead of asking the model to do the computation.\\nQUESTION :Jill gets paid $20 per hour to teach and $30 to be a cheerleading coach. If she works 50 weeks a\\nyear, 35 hours a week as a teacher and 15 hours a week as a coach, what’s her annual salary?\\nMODEL ANSWER (INCORRECT ;SYMBOL MAPPING ERROR ):Jill gets paid 20 dollars per hour to teach\\nand 30 dollars per hour to be a cheerleading coach. If she works 50 weeks a year, 35 hours a week as a teacher\\nand 15 hours a week as a coach, then she works 50 x 35 = 1750 hours as a teacher and 15 x 30 = 450 hours as a\\ncoach. So she works 1750 + 450 = 2200 hours. She gets paid 20 dollars per hour for 1750 hours and 30 dollars\\nper hour for 450 hours. So her annual salary is 20 x 1750 + 30 x 450 = $36,500. The answer is $36,500. 7\\nEXPLANATION FOR ERROR CATEGORY :The model incorrectly mapped “15 x 30” instead of “15 x 50”, and\\nthe chain of thought would be made correct by ﬁxing just that one change (with an external calculator).\\nQUESTION :Kelian has two recipes for preparing dishes, one having 20 instructions and the second one having\\ntwice as many instructions as the ﬁrst one. How many instructions does Kelian have to read to prepare the two\\ndishes?\\nMODEL ANSWER (INCORRECT ;ONE STEP MISSING ERROR ):Kelian has two recipes. One has 20 instruc-\\ntions. The other has twice as many instructions as the ﬁrst one. So the second one has 40 instructions. So Kelian\\nhas to read 40 instructions to prepare the two dishes. The answer is 40. 7\\nEXPLANATION FOR ERROR CATEGORY :The model answer could be made correct just adding one additional\\nstep (in this case, adding 20 and 40).\\n27', doc_id='b9d3461c-a975-417f-b7c1-cd7306ff84d9', embedding=None, doc_hash='628a3cd1c4db8c547918e2c14feffc30284bb21ec29bb2ec6a1b2934e4d5db34', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='We found that the remaining chains of thought (27 of 50; 54%) would require substantial edits to make\\ninto a correct chain of thought. Almost all cases here involved some error in semantic understanding\\n(see Table 11: “semantic understanding error”), and 8 of the 27 also had incoherent chain of thoughts,\\nmeaning that some statements in the generated chain of thought did not follow from prior ones or\\nviolated basic world knowledge (see Table 11: “incoherent chain of thought error”).\\nTable 11: Example of incorrect chains of thought, categorized as described in Appendix D.2. These\\nchains of thought had multiple mistakes that could not be ﬁxed with minor edits.\\nQUESTION :Gretchen has 110 coins. There are 30 more gold coins than silver coins. How many gold coins\\ndoes Gretchen have?\\nMODEL ANSWER (INCORRECT ;SEMANTIC UNDERSTANDING ERROR ):Gretchen has 110 coins. There\\nare 30 more gold coins than silver coins. So there are 110 - 30 = 80 silver coins. So there are 80 silver coins and\\n110 - 80 = 30 gold coins. The answer is 30. 7\\nEXPLANATION FOR ERROR CATEGORY :The model does not understand that 110 coins are in total and that\\nit must solve an equation to get the number of gold and silver coins.\\nQUESTION :In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining\\nenrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in\\nhip-hop dance?\\nMODEL ANSWER (INCORRECT ;INCOHERENT CHAIN OF THOUGHT ERROR ):20% of the students en-\\nrolled in contemporary dance. 25% of the remaining students enrolled in jazz dance. The rest enrolled in hip-hop\\ndance. So the percentage of the entire students enrolled in hip-hop dance is the percentage of the students\\nenrolled in hip-hop dance, minus the percentage of the students enrolled in contemporary dance, minus the\\npercentage of the students enrolled in jazz dance. So the percentage of the entire students enrolled in hip-hop\\ndance is (25 + 20) - (25 + 20) = 100%. The answer is 100%. 7\\nEXPLANATION FOR ERROR CATEGORY :This chain of thought is incoherent in that the percent of entire\\nstudents enrolled in hip-hope dance cannot be the percent of student enrolled in hip-hop dance minus another\\nterm.\\nOverall, there are no guarantees that the reasoning processes generated by large language models\\nare coherent or factually correct, as underscored by the recent work evaluating the factuality of\\nlanguage model generations and explanations (Maynez et al., 2020; Rashkin et al., 2021; Ye and\\nDurrett, 2022; Marasovi ´c et al., 2022; Wiegreffe et al., 2022). Incorrect reasoning processes can lead\\nto both incorrect ﬁnal answers as well as accidentally correct ﬁnal answers (with accidentally correct\\nﬁnal answers being more likely for tasks such as binary classiﬁcation as opposed to free response).\\nImproving the factuality of language model generations with respect to context and world knowledge\\nis an important direction open problems in language model research and could also be expected to\\npotentially improve multi-step reasoning abilities of language models. One potential method for\\nimproving the quality of decoding could involve generating multiple reasoning paths and scoring\\neach of them with a veriﬁer, though this requires training the veriﬁer (Cobbe et al., 2021; Shen et al.,\\n2021; Thoppilan et al., 2022).\\nD.3 Additional Robustness Analysis\\nAs the experiments in the main paper use a ﬁxed number of few-shot exemplars (8; as constrained by\\nthe input length of 1024 tokens), we verify that the chain-of-thought prompting is robust to various\\nnumbers of few-shot exemplars. We run experiments for LaMDA 137B, comparing chain-of-thought\\nprompting with standard prompting for the ﬁve datasets where standard prompting had a mostly ﬂat\\nscaling curve (the largest model did not achieve high performance). As shown in Figure 11, the\\nimprovement of chain-of-thought prompting over standard prompting remains robust to varying the\\nnumber of few-shot exemplars in the prompt.\\n28', doc_id='1744547c-2880-468f-b703-be2cd54a592f', embedding=None, doc_hash='144e7c60db958b6f0150d8298bbbec668f75352cc27e3df20048adc2f624f112', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='12468051015 Solve rate (%)GSM8K\\n124680204060MultiArith\\n(MAWPS)\\n124680255075100\\nNumber of few-shot exemplarsSports\\nUnderstandingStandard prompting\\nChain of thought prompting\\n124680255075100Coin Flip\\n12340255075100Last Letter\\nConcatenation\\nFigure 11: The improvement of chain of thought prompting over standard prompting appears robust\\nto varying the number of few-shot exemplars in the prompt.\\nTable 12: Summary of math word problem benchmarks we use in this paper with examples. N:\\nnumber of evaluation examples.\\nDataset N Example problem\\nGSM8K 1,319 Josh decides to try ﬂipping a house. He buys a house for $80,000 and then puts\\nin $50,000 in repairs. This increased the value of the house by 150%. How\\nmuch proﬁt did he make?\\nSV AMP 1,000 Each pack of dvds costs 76 dollars. If there is a discount of 25 dollars on each\\npack. How much do you have to pay to buy each pack?\\nASDiv 2,096 Ellen has six more balls than Marin. Marin has nine balls. How many balls does\\nEllen have?\\nAQuA 254 A car is being driven, in a straight line and at a uniform speed, towards the base\\nof a vertical tower. The top of the tower is observed from the car and, in the\\nprocess, it takes 10 minutes for the angle of elevation to change from 45\\x0eto 60\\x0e.\\nAfter how much more time will this car reach the base of the tower? Answer\\nChoices: (a) 5p\\n3+ 1 (b) 6p\\n3+p\\n2(c) 7p\\n3- 1 (d) 8p\\n3- 2 (e) None of these\\nMAWPS: SingleOp 562 If there are 7 bottle caps in a box and Linda puts 7 more bottle caps inside, how\\nmany bottle caps are in the box?\\nMAWPS: SingleEq 508 Benny bought a soft drink for 2 dollars and 5 candy bars. He spent a total of 27\\ndollars. How much did each candy bar cost?\\nMAWPS: AddSub 395 There were 6 roses in the vase. Mary cut some roses from her ﬂower garden.\\nThere are now 16 roses in the vase. How many roses did she cut?\\nMAWPS: MultiArith 600 The school cafeteria ordered 42 red apples and 7 green apples for students\\nlunches. But, if only 9 students wanted fruit, how many extra did the cafeteria\\nend up with?\\n29', doc_id='002ce38b-fc41-4dc4-b270-062b62a48561', embedding=None, doc_hash='d30c52f0b0bcf4b89bb7a6a1111c3930eb709d23dc23dcce0d0a360a80418d25', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='E Additional Details\\nVersion Control\\nV5!V6. Fixed minor typo in Figure 3.\\nV4!V5. Added Codex and UL2 results. Small changes to writing and style of paper.\\nV3!V4. Fixed typo in Figure 3 and added a couple citations.\\nV2!V3. Added GPT-3 results. Added SV AMP and AQuA eval datasets for math. Added SayCan\\neval for commonsense. Added Extended Related Work section (Appendix C). Added ablations for\\nCommonsense and Symbolic Reasoning (Table 7). Added FAQ section (Appendix A). Added raw\\nresults in Appendix B.\\nV1!V2. Added PaLM results (V1 only had LaMDA).\\nE.1 Reproducibility Statement\\nAs our results make use of two sets of large language models that is not publicly available, we take\\nthe following actions to facilitate reproducibility. First, we provide the exact input prompts for all\\ntasks in Table 20–Table 27 in Appendix G (and emphasize that we do not perform any ﬁnetuning and\\nonly apply prompting to off-the-shelf language models). Second, we conduct experiments using the\\npublicly available GPT-3 API for four model scales text-ada-001, text-babbage-001, text-curie-001,\\ntext-davinci-002). Finally, we make exact inputs, targets, and predictions for LaMDA 137B for each\\ntask available as a zip ﬁle in the supplementary material.\\nE.2 Computational Resources\\nFor all three language models we evaluated, we did prompting-based inference only. No ﬁnetuning\\nwas done for this paper. For inference on LaMDA 137B we use TPU v3 (8x8 conﬁguration, 64 chips\\n/ 128 cores), and for inference on PaLM 540B we use TPU v4 (4x4x12 conﬁguration, 192 chips / 384\\ncores). GPT-3 experiments were done using the public API.5\\nE.3 Dataset Details and Licenses\\nWe list the details and licenses for all arithmetic and commonsense datasets used in this paper. The\\nsymbolic reasoning datasets were created synthetically, as described in Section 4.\\nArithmetic reasoning\\n•Math Word Problem Repository (Koncel-Kedziorski et al., 2016): AddSub (Hosseini\\net al., 2014): https://www.cs.washington.edu/nlp/arithmetic ; MultiArith (Roy\\nand Roth, 2015), license: CC BY 4.0.\\n• ASDiv (Miao et al., 2020): https://github.com/chaochun/nlu-asdiv-dataset .\\n•AQuA (Ling et al., 2017): https://github.com/deepmind/AQuA , license: https://\\ngithub.com/deepmind/AQuA/blob/master/LICENSE .\\n•GSM8K (Cobbe et al., 2021): https://github.com/openai/grade-school-math ,\\nMIT license: https://github.com/openai/grade-school-math/blob/master/\\nLICENSE .\\n•SV AMP (Patel et al., 2021): https://github.com/arkilpatel/SVAMP , MIT license:\\nhttps://github.com/arkilpatel/SVAMP/blob/main/LICENSE .\\nCommonsense reasoning\\n•CSQA (Talmor et al., 2019): https://www.tau-nlp.org/commonsenseqa ,https://\\ngithub.com/jonathanherzig/commonsenseqa .\\n5https://beta.openai.com/docs/api-reference/making-requests\\n30', doc_id='6e2077df-991d-40e4-bdbf-676fdb0c3cba', embedding=None, doc_hash='e194769cc24b8e5c00b5711ae183342bc3680830a384ac35b3123a1066d313cf', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='•StrategyQA (Geva et al., 2021): we use the open-domain setting (question-only set)\\nfrom BIG-bench collaboration (2021): https://github.com/google/BIG-bench/\\ntree/main/bigbench/benchmark_tasks/strategyqa . The original dataset is from\\nhttps://github.com/eladsegal/strategyqa , MIT license: https://github.com/\\neladsegal/strategyqa/blob/main/LICENSE .\\n•Date understanding and sports understanding from BIG-Bench (BIG-bench collaboration,\\n2021): Apache License v.2: https://github.com/google/BIG-bench/blob/main/\\nLICENSE .\\n•SayCan (Ahn et al., 2022): SayCan dataset can be accessed at https://say-can.github.\\nio/under CC BY 4.0 license.\\n31', doc_id='0e5f79f1-b744-488d-bea9-618788f6e085', embedding=None, doc_hash='f262e9e0f2a2d28c0d0a70d8c9b8d895791ec0175bfd5817a1d38020b9a3d3e2', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='F Appendix: Input/Output Examples\\nTable 13: Examples of correct and incorrect chains of thought produced by LaMDA 137B on the\\nletter concatenation task.\\nQUESTION :Take the last letters of the words in “Waldo Schmidt” and concatenate them.\\nMODEL ANSWER (CORRECT): The last letter of “Waldo” is “o”. The last letter of “Schmidt” is “t”.\\nConcatenating them is “ot”. So the answer is ot. X\\nQUESTION :Take the last letters of the words in “Daniel Friedman” and concatenate them.\\nMODEL ANSWER (INCORRECT): The last letter of “Daniel” is “l”. The last letter of “Friedman” is “m”.\\nConcatenating them is “lm”. So the answer is lm. 7\\nTable 14: Examples of correct and incorrect chains of thought produced by LaMDA 137B on the\\ncoinﬂip task.\\nQUESTION :A coin is heads up. Andree ﬂips the coin. Audrie does not ﬂip the coin. Is the coin still heads up?\\nMODEL ANSWER (CORRECT): The coin was ﬂipped by Andree. So the coin was ﬂipped 1 time, which is\\nan odd number. The coin started heads up, so after an odd number of ﬂips, it will be tails up. So the answer is no.\\nX\\nQUESTION :A coin is heads up. Kristian does not ﬂip the coin. Dallas does not ﬂip the coin. Is the coin still\\nheads up?\\nMODEL ANSWER (INCORRECT): The coin was ﬂipped by Kristian. So the coin was ﬂipped 1 time, which\\nis an odd number. The coin started heads up, so after an odd number of ﬂips, it will be tails up. So the answer is\\nno.7\\nTable 15: Examples of correct and incorrect chains of thought produced by LaMDA 137B on CSQA.\\nQUESTION :When a person is beginning work, what are they building?\\nAnswer Choices: (a) time (b) accomplishing (c) working (d) momentum (e) tiredness\\nMODEL ANSWER (CORRECT): The answer must be something that is built. Of the above choices, only\\nmomentum is built. So the answer is (d). X\\nQUESTION :Where are you likely to ﬁnd a hamburger?\\nAnswer Choices: (a) fast food restaurant (b) pizza (c) ground up dead cows (d) mouth (e) cow carcus\\nMODEL ANSWER (CORRECT): The answer must be a place where hamburgers are found. Of the above\\nchoices, only fast food restaurants serve hamburgers. So the answer is (a). X\\nQUESTION :Aside from water and nourishment what does your dog need?\\nAnswer Choices: (a) bone (b) charm (c) petted (d) lots of attention (e) walked\\nMODEL ANSWER (INCORRECT): The answer must be something that a dog needs. Of the above choices,\\nonly bone is something that a dog needs. So the answer is (a). 7(Ground truth is (d).)\\nQUESTION :What are you waiting alongside with when you’re in a reception area?\\nAnswer Choices: (a) motel (b) chair (c) hospital (d) people (e) hotels\\nMODEL ANSWER (INCORRECT): The answer must be something that is waiting with you in a reception\\narea. Of the above choices, only people are waiting with you in a reception area. So the answer is (e). 7(Ground\\ntruth is (d).)\\n32', doc_id='6e51dd1e-37a6-40c7-9428-bfe9d0372e84', embedding=None, doc_hash='7ff8136e2b381e379b96f8a0d858fe742efb7f3b3055792fbc06775647297340', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 16: Examples of correct and incorrect chains of thought produced by LaMDA 137B on\\nStrategyQA.\\nQUESTION :Will The Exorcist stimulate limbic system?\\nMODEL ANSWER (CORRECT): The Exorcist is a horror movie. Horror movies are scary. The limbic system\\nis involved in fear. Thus, The Exorcist will stimulate the limbic system. So the answer is yes. X\\nQUESTION :Was Jackson Pollock trained by Leonardo da Vinci?\\nMODEL ANSWER (CORRECT): Leonardo da Vinci lived in the 15th century. Jackson Pollock lived in the\\n20th century. Thus, Jackson Pollock could not have been trained by Leonardo da Vinci. So the answer is no. X\\nQUESTION :Can Harry Potter book a ﬂight on Asiana Airlines?\\nMODEL ANSWER (INCORRECT): Harry Potter is a ﬁctional character. Thus, Harry Potter can do anything.\\nSo the answer is yes. 7(Ground truth is “no”.)\\nQUESTION :Would a sophist use an épée?\\nMODEL ANSWER (INCORRECT): A sophist is a person who is skilled in the art of persuasion. An épée is\\na type of sword. Thus, a sophist could use an épée. So the answer is yes. 7(Ground truth is “no”.)\\nTable 17: Examples of correct and incorrect chains of thought produced by LaMDA 137B on Date\\nUnderstanding.\\nQUESTION :May 6, 1992 is like yesterday to Jane, but that is actually ten years ago. What is the date a month\\nago in MM/DD/YYYY?\\nMODEL ANSWER (CORRECT): May 6, 1992 is ten years ago, so today is May 6, 2002. So a month ago will\\nbe April 6, 2002. So the answer is 04/06/2002. X\\nQUESTION :This is the last day of 1899. What is the date 24 hours later in MM/DD/YYYY?\\nMODEL ANSWER (CORRECT): Today is 12/31/1899. 24 hours later will be 01/01/1900. So the answer is\\n01/01/1900. X\\nQUESTION :Jane scheduled 3 apointments with 5 poeple for tomorrow (Tue, 7/9/1972). What is the date one\\nweek ago from today in MM/DD/YYYY?\\nMODEL ANSWER (INCORRECT): Tomorrow is 7/9/1972. One week ago from today is 7/2/1972. So the\\nanswer is 7/2/1972. 7(Ground truth is 07/01/1972.)\\nQUESTION :Today is the palindrome day of 2020, because the MMDDYYYY format of the date is the same\\nbackwards as forwards. What is the date tomorrow in MM/DD/YYYY?\\nMODEL ANSWER (INCORRECT): Today is 02/29/2020. Tomorrow will be 03/01/2020. So the answer is\\n03/01/2020. 7(Ground truth is “02/03/2020”.)\\nTable 18: Examples of correct and incorrect chains of thought produced by LaMDA 137B on Sports\\nUnderstanding.\\nQUESTION :Is the following sentence plausible? “Joao Moutinho was out at third.”\\nMODEL ANSWER (CORRECT): Joao Moutinho is a soccer player. Being out at third is part of baseball, not\\nsoccer. So the answer is no. X\\nQUESTION :Is the following sentence plausible? “Malcolm Brogdon eurostepped to the basket in the NBA\\nChampionship.”\\nMODEL ANSWER (CORRECT): Malcolm Brogdon is a basketball player. Eurostepping to the basket is part\\nof basketball. So the answer is yes. X\\nQUESTION :Is the following sentence plausible? “Derrick White backhanded a shot.”\\nMODEL ANSWER (INCORRECT): Derrick White is a basketball player. Backhanding a shot is part of\\nbasketball. So the answer is yes. 7(Ground truth is “no”, backhanding a shot is part of hockey.)\\nQUESTION :Is the following sentence plausible? “Nazem Kadri was out at home.”\\nMODEL ANSWER (INCORRECT): Nazem Kadri is a hockey player. Being out at home is part of hockey.\\nSo the answer is yes. 7(Ground truth is “no”, being out at home is part of baseball.)\\n33', doc_id='126c1b1c-2238-4449-869a-39b1c07b312b', embedding=None, doc_hash='0ba07377058a48de3a13a6db2ecacd119f312af90c2490517352b8cf83dd2c11', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 19: Examples of correct and incorrect chains of thought produced by PaLM 540B on SayCan\\nrobot planning tasks.\\nQUESTION :I spilled my coke on the table, could you throw it away and then bring me something to help clean?\\nMODEL ANSWER (CORRECT): Explanation: The user has spilled their coke on the table. I will throw away\\nthe coke and then bring the user a sponge. Plan: ﬁnd(coke), pick(coke), ﬁnd(trash), put(coke), ﬁnd(sponge),\\npick(sponge), ﬁnd(table), put(sponge). X\\nQUESTION :Could you compost that apple for me?\\nMODEL ANSWER (CORRECT): Explanation: The user would like me to compost the apple. I will move the\\napple to the compost. Plan: ﬁnd(apple), pick(apple), ﬁnd(trash), put(apple). X\\nQUESTION :Can you bring me a drink that is not coke?\\nMODEL ANSWER (INCORRECT): Explanation: The user asked me to bring a drink that is not coke, I will\\nbring a sprite. Plan: ﬁnd(sprite), pick(sprite), ﬁnd(user), put(sprite). 7(Feasible answers including ﬁnding lime\\nsoda, pepsi, grapefruit soda or water, because there is no sprite in the scene as deﬁned in Table 28.)\\nQUESTION :Can you bring me something salty?\\nMODEL ANSWER (INCORRECT): Explanation: The user would like something salty. There are several\\noptions in the objects list, so I will bring the user a lime soda. Plan: ﬁnd(lime soda), pick(lime soda), ﬁnd(user),\\nput(lime soda). 7(Feasible answers including ﬁnding chips. The model misidentify lime soda as salty.)\\n34', doc_id='70339a9c-c801-47c0-80b5-c83bc3653a85', embedding=None, doc_hash='87066c0feab94f732bb0469c0207894b365de81101c4418c6eb422d77daee007', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='G Appendix: Full Prompts\\nTable 20: Few-shot exemplars for full chain of thought prompt for math word problems. This set of\\nexemplars was used for all math word problem datasets except AQuA.\\nPROMPT FOR MATH WORD PROBLEMS\\nQ:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\\nwill be 21 trees. How many trees did the grove workers plant today?\\nA:There are15trees originally. Then there were 21trees aftersome more were planted. Sothere must have\\nbeen 21-15=6. The answer is 6.\\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA:There areoriginally 3cars. 2more carsarrive. 3+2=5. The answer is 5.\\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nA:Originally, Leah had32choco lates. Hersisterhad42.Sointotalthey had32+42=74.Aftereating35,they\\nhad74-35=39. The answer is 39.\\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\\nJason give to Denny?\\nA:Jasonstarted with 20lollipops. Then hehad12aftergivingsome toDenny. Sohegave Denny 20-12=8.\\nThe answer is 8.\\nQ:Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\\nhave now?\\nA:Shawn started with 5toys. Ifhegot2toys each from hismom anddad, then thatis4more toys. 5+4=9.\\nThe answer is 9.\\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\\nto thursday. How many computers are now in the server room?\\nA:There were originally 9computers.Foreach of4days, 5more computerswere added. So5*4=20\\ncomputerswere added. 9+20is29. The answer is 29.\\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\\nballs did he have at the end of wednesday?\\nA:Michael started with 58golfballs. Afterlosing23ontuesday, hehad58-23=35.Afterlosing2more, he\\nhad35-2=33golfballs. The answer is 33.\\nQ:Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?\\nA:Olivia had23dollars. 5bagels for3dollarseach willbe5x3=15dollars. Soshehas23-15dollarsleft.23\\n-15is8. The answer is 8.\\n35', doc_id='ae7906d2-dd0e-4ceb-b269-18aff9c1b51b', embedding=None, doc_hash='0e76b7439c6a79753f024530a4c53f772204008b69bebfd7247235a175eea813', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 21: Few-shot exemplars for full chain of thought prompt for AQuA.\\nPROMPT FOR AQ UAALGEBRAIC WORD PROBLEMS\\nQ:John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers\\nis?\\nAnswer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\nA:If10isadded toeach number,then themean ofthenumbers alsoincreases by10.Sothenewmean would be\\n50. The answer is (a).\\nQ:If a / b = 3/4 and 8a + 5b = 22,then ﬁnd the value of a.\\nAnswer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2\\nA:Ifa/b=3/4,then b=4a/3.So8a+5(4a /3)=22.This simpliﬁesto8a+20a/3=22,which means 44a/3\\n=22.Soaisequal to3/2. The answer is (b).\\nQ:A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then ﬁnd the distance?\\nAnswer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km\\nA:Thedistance thatthepersontraveled would have been 20km/hr *2.5hrs=50km. The answer is (e).\\nQ:How many keystrokes are needed to type the numbers from 1 to 500?\\nAnswer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788\\nA:There are9one-digit numbers from 1to9.There are90two-digit numbers from 10to99.There are401\\nthree -digit numbers from 100to500. 9+90(2) +401(3) =1392. The answer is (b).\\nTable 22: Few-shot exemplars for full chain of thought prompt for the last letter concatenation task.\\nPROMPT FOR LAST LETTER CONCATENATION\\nQ:Take the last letters of the words in \"Elon Musk\" and concatenate them.\\nA:Thelastletterof\"Elon\" is\"n\".Thelastletterof\"Musk\" is\"k\".Concatenatingthem is\"nk\". The answer is nk.\\nQ:Take the last letters of the words in \"Larry Page\" and concatenate them.\\nA:Thelastletterof\"Larry\" is\"y\".Thelastletterof\"Page\" is\"e\".Concatenatingthem is\"ye\". The answer is ye.\\nQ:Take the last letters of the words in \"Sergey Brin\" and concatenate them.\\nA:Thelastletterof\"Sergey\" is\"y\".Thelastletterof\"Brin\" is\"n\".Concatenatingthem is\"yn\". The answer is\\nyn.\\nQ:Take the last letters of the words in \"Bill Gates\" and concatenate them.\\nA:Thelastletterof\"Bill\" is\"l\".Thelastletterof\"Gates\" is\"s\".Concatenatingthem is\"ls\". The answer is ls.\\n36', doc_id='d7405903-57d7-41ea-a2da-e59b1a373f1c', embedding=None, doc_hash='dc5d3319c4aa354cd3b5342ad05feb1687529e8c0c15028680eb23ae724144ac', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 23: Few-shot exemplars for full chain of thought prompt for the coinﬂip task.\\nPROMPT FOR COIN FLIP\\nQ:Q: A coin is heads up. Ka ﬂips the coin. Sherrie ﬂips the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byKaandSherrie.Sothecoin wasﬂipped 2times, which isaneven number.Thecoin\\nstarted heads up,soafteraneven numberofﬂips, itwillstillbeheads up. So the answer is yes.\\nQ:A coin is heads up. Jamey ﬂips the coin. Teressa ﬂips the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byJamey andTeressa. Sothecoin wasﬂipped 2times, which isaneven number.The\\ncoin started heads up,soafteraneven numberofﬂips, itwillstillbeheads up. So the answer is yes.\\nQ:A coin is heads up. Maybelle ﬂips the coin. Shalonda does not ﬂip the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byMaybelle. Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin started\\nheads up,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.\\nQ:A coin is heads up. Millicent does not ﬂip the coin. Conception ﬂips the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byConception. Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin\\nstarted heads up,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.\\nQ:A coin is heads up. Sal ﬂips the coin. Raymond does not ﬂip the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped bySal.Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin started heads\\nup,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.\\nQ:A coin is heads up. Conception ﬂips the coin. Kristian does not ﬂip the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byConception. Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin\\nstarted heads up,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.\\nQ:A coin is heads up. Inga does not ﬂip the coin. Elanor does not ﬂip the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped bynoone. Sothecoin wasﬂipped 0times. Thecoin started heads up,anditwasnot\\nﬂipped, soitisstillheads up. So the answer is yes.\\nQ:A coin is heads up. Ryan ﬂips the coin. Shaunda ﬂips the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byRyan andShaunda. Sothecoin wasﬂipped 2times, which isaneven number.The\\ncoin started heads up,soafteraneven numberofﬂips, itwillstillbeheads up. So the answer is yes.\\n37', doc_id='a3374bc5-ffb6-4931-a2c6-654136c29695', embedding=None, doc_hash='13602587a9cd5fad087932de453564f2dccb458c62c87ae0b8f641592c440bd9', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 24: Few-shot exemplars for full chain of thought prompt for CSQA. There are newlines\\nbetween the answer choices that are omitted in the table for space reasons.\\nPROMPT FOR CSQA\\nQ:What do people use to absorb extra ink from a fountain pen? Answer Choices: (a) shirt pocket (b)\\ncalligrapher’s hand (c) inkwell (d) desk drawer (e) blotter\\nA:Theanswer must beanitem thatcanabsorb ink.Oftheabove choices, only blottersareused toabsorb ink.\\nSo the answer is (e).\\nQ:What home entertainment equipment requires cable?\\nAnswer Choices: (a) radio shack (b) substation (c) television (d) cabinet\\nA:Theanswer must require cable.Oftheabove choices, only television requires cable. So the answer is (c).\\nQ:The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty ﬂowers (b)\\nhen house (c) natural habitat (d) storybook\\nA:Theanswer must besome thing intheforest.Oftheabove choices, only naturalhabitatisintheforest. So the\\nanswer is (b).\\nQ:Sammy wanted to go to where the people were. Where might he go? Answer Choices: (a) populated areas\\n(b) race track (c) desert (d) apartment (e) roadblock\\nA:Theanswer must beaplace with alotofpeople.Oftheabove choices, only populated areashave alotof\\npeople. So the answer is (a).\\nQ:Where do you put your grapes just before checking out? Answer Choices: (a) mouth (b) grocery cart (c)super\\nmarket (d) fruit basket (e) fruit market\\nA:Theanswer should betheplace where grocery items areplaced beforecheck ingout.Oftheabove choices,\\ngrocery cartmakes themost sense forholdinggrocery items. So the answer is (b).\\nQ:Google Maps and other highway and street GPS services have replaced what? Answer Choices: (a) united\\nstates (b) mexico (c) countryside (d) atlas\\nA:Theanswer must besome thing thatused todowhat Google Maps andGPS services do,which istogive\\ndirections. Oftheabove choices, only atlases areused togive directions. So the answer is (d).\\nQ:Before getting a divorce, what did the wife feel who was doing all the work? Answer Choices: (a) harder (b)\\nanguish (c) bitterness (d) tears (e) sadness\\nA:Theanswer should bethefeelingofsome onegettingdivorced who wasdoingallthework. Oftheabove\\nchoices, theclosestfeelingisbitterness. So the answer is (c).\\n38', doc_id='aead9b21-d2d2-4393-a86d-ba1ec4c2b527', embedding=None, doc_hash='1a564d0d20ebc4bcd89a684549c0aea92bb862a56da28aa53fd742a611edc3a8', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 25: Few-shot exemplars for full chain of thought prompt for StrategyQA.\\nPROMPT FOR STRATEGY QA\\nQ:Do hamsters provide food for any animals?\\nA:Ham sters areprey animals. Prey arefood forpreda tors. Thus, hamsters provide food forsome animals. So\\nthe answer is yes.\\nQ:Could Brooke Shields succeed at University of Pennsylvania?\\nA:Brooke Shields went toPrince tonUniversity. Prince tonUniversityisabout asacademically rigorousasthe\\nUniversityofPenn sylvania.Thus, Brooke Shields could alsosucceed attheUniversityofPenn sylvania. So the\\nanswer is yes.\\nQ:Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\\nA:Hydrogenhasanatomic numberof1.1squared is1.There are5Spice Girls. Thus, Hydrogen’s atomic\\nnumbersquared islessthan 5. So the answer is no.\\nQ:Yes or no: Is it common to see frost during some college commencements?\\nA:College commence ment ceremonies canhappeninDecember,May, andJune. Decemberisinthewinter,so\\nthere canbefrost. Thus, there could befrost atsome commence ments. So the answer is yes.\\nQ:Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\\nA:TheWar inVietnam was6months. Thegestationperiod forallama is11months, which ismore than 6\\nmonths. Thus, allama could notgive birth twice duringtheWar inVietnam. So the answer is no.\\nQ:Yes or no: Would a pear sink in water?\\nA:Thedensityofapear isabout 0:6g=cm3,which islessthan water.Objects lessdense than waterﬂoat. Thus,\\napear would ﬂoat. So the answer is no.\\nTable 26: Few-shot exemplars for full chain of thought prompt for Date Understanding.\\nPROMPT FOR DATE UNDERSTANDING\\nQ:2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\\nA:If2015 iscomingin36hours, then itiscomingin2days. 2days before01/01/2015 is12/30/2014, sotoday\\nis12/30/2014. Sooneweek from todaywillbe01/05/2015. So the answer is 01/05/2015.\\nQ:The ﬁrst day of 2019 is a Tuesday, and today is the ﬁrst Monday of 2019. What is the date today in\\nMM/DD/YYYY?\\nA:Iftheﬁrstdayof2019 wasTues day,then 01/01/2019 wasaTues day.Todayistheﬁrstmonday,would besix\\ndays later. Sotodayis01/07/2019. So the answer is 01/07/2019.\\nQ:The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10\\ndays ago in MM/DD/YYYY?\\nA:One dayafter06/01/1943 is06/02/1943, sotodayis06/02/1943. 10days beforetodayis05/23/1943. So the\\nanswer is 05/23/1943.\\nQ:It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?\\nA:Todayis04/19/1969. 24hours later isonedayaftertoday, which would be04/20/1969. So the answer is\\n04/20/1969.\\nQ:Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours\\nlater in MM/DD/YYYY?\\nA:Todayis03/12/2002. Sothedate 24hours later willbe03/13/2002. So the answer is 03/13/2002.\\nQ:Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date\\nyesterday in MM/DD/YYYY?\\nA:ThelastdayofFebru aryisthe28th, soJane wasborn on02/28/2001. Todayisher16-year oldbirth day, so\\ntodayis02/28/2017. Soyesterdaywas02/27/2017. So the answer is 02/27/2017.\\n39', doc_id='2334ac35-854a-431b-a5a5-27ca636c184e', embedding=None, doc_hash='5dba9fc1d795ffa19178699098c719780988625d08d62bf0fd400afac1b33c53', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 27: Few-shot exemplars for full chain of thought prompt for Sports Understanding.\\nPROMPT FOR SPORTS UNDERSTANDING\\nQ:Is the following sentence plausible? “Kyle Palmieri was called for slashing.”\\nA:Kyle Palmieri isahockey player. Beingcalled forslash ingispartofhockey. So the answer is yes.\\nQ:Is the following sentence plausible? “Joao Moutinho caught the screen pass in the NFC championship.”\\nA:Joao Moutinho isasoccerplayer. TheNFC cham pionship ispartofAmer icanfootball, notsoccer. So the\\nanswer is no.\\nQ:Is the following sentence plausible? “Carson Wentz set the pick and roll.”\\nA:CarsonWentz isanAmer icanfootballplayer. Pick androllispartofbasketball, notfootball. So the answer\\nis no.\\nQ:Is the following sentence plausible? “Jonas Valanciunas beat the buzzer.”\\nA:Jonas Valan ciunasisabasketballplayer. Beatingthebuzzer ispartofbasketball. So the answer is yes.\\nQ:Is the following sentence plausible? “Jamel Murray was perfect from the line.”\\nA:JamalMurrayisabasketballplayer. Beingperfectfrom thelineispartofbasketball. So the answer is yes.\\nQ:Is the following sentence plausible? “Sam Darnold passed the puck.”\\nA:Sam Darnold isaAmer icanfootballplayer. Passingthepuck ispartofhockey, notAmer icanfootball. So the\\nanswer is no.\\nQ:Is the following sentence plausible? “Draymond Green threw a touchdown.”\\nA:Dray mond Green isanbasketballplayer. Throw ingatouch down ispartoffootball, notbasketball. So the\\nanswer is no.\\nQ:Is the following sentence plausible? “Malcolm Brogdon banked the shot in.”\\nA:Malcolm Brog donisabasketballplayer. Bank ingtheshot inispartofbasketball. So the answer is yes.\\n40', doc_id='e831b90a-99a1-4d11-87ee-75006196e72d', embedding=None, doc_hash='75ae007a2d68603da956be32d184f2f646817bfcab9e02c326b82df5920fa5ef', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 28: Few-shot exemplars for full chain of thought prompt for SayCan robot planning tasks.\\nPROMPT FOR SAYCANROBOT PLANNING\\nLocations = [counter, table, user, trash, bowl].\\nObjects = [7up, apple, kettle chips, tea, multigrain chips, coke, lime soda, jalapeno chips, rice chips, orange,\\ngrapefruit soda, pepsi, redbull, energy bar, sponge, water].\\nThe robot can pick up items with pick(object) and put down items with put(object) as well as ﬁnd objects or\\nlocations with ﬁnd(). The robot can only understand the explicit locations and objects listed.\\nHuman : I’m hungry, can you bring me some chips.\\nExplanation :Theuser ishungryandhasasked forchips. There areseveraltypes ofchips avail able, Iwillbring\\ntheuser thekettlechips.\\nPlan : 1. ﬁnd(kettle chips), 2. pick(kettle chips), 3. ﬁnd(user), 4. put(kettle chips), 5. done().\\nHuman : How would you move the grapefruit drink from the table to the counter?\\nExplanation :Theuser hasasked metomove thegrape fruit drink tothecounter.\\nPlan : 1. ﬁnd(grapefruit soda), 2. pick(grapefruit soda), 3. ﬁnd(counter), 4. put(grapefruit soda), 5. done().\\nHuman : How would you bring me some snacks?\\nExplanation :Theuser hasasked forsnacks, Iwillchoose twoitems andbring them. Iwillbring jalapeno chips\\nandanapple.\\nPlan : 1. ﬁnd(jalapeno chips), 2. pick(jalapeno chips), 3. ﬁnd(user), 4. put(jalapeno chips), 5. ﬁnd(apple), 6.\\npick(apple), 7. ﬁnd(user), 8. put(apple), 9. done().\\nHuman : How would you bring me something to eat that isn’t a fruit?\\nExplanation :Theuser hasasked forafood thatisn’t afruit, Iwillbring anenergy bartothem.\\nPlan : 1. ﬁnd(energy bar), 2. pick(energy bar), 3. ﬁnd(user), 4. put(energy bar), 5. done().\\nHuman : How would you put the rice chips in the bowl and then move the tea to the table?\\nExplanation :Theuser hasasked metodotwotasks, Iwilldooneandthen theother.\\nPlan : 1. ﬁnd(rice chips), 2. pick(rice chips), 3. ﬁnd(bowl), 4. put(rice chips), 5. ﬁnd(tea), 6. pick(tea), 7.\\nﬁnd(table), 8. put(tea), 9. done().\\nHuman : How would you throw away a redbull?\\nExplanation :Theuser hasasked metothrow away theredbull, Iwillmove ittothetrash.\\nPlan : 1. ﬁnd(redbull), 2. pick(redbull), 3. ﬁnd(trash), 4. put(redbull), 5. done().\\nHuman : Bring me a drink.\\nExplanation :Theuser hasasked foradrink andthere aremany options. Iwillbring them awater.\\nPlan : 1. ﬁnd(water), 2. pick(water), 3. ﬁnd(user), 4. put(water), 5. done().\\n41', doc_id='d17f1f66-f962-4e8d-9926-971e52995ec0', embedding=None, doc_hash='3b450f85d34002ec09078212f0cbac1e4bffd6b10a8741b90ab06c44c96b475c', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='H Appendix: Alternate Annotators for MWP\\nTable 29: Few-shot exemplars for full chain of thought prompt for math word problems. These\\nexemplars are the same as in Table 20, except that the chains of thought were written by a different\\nannotator (“Annotator B” instead of “Annotator A”). Annotators were co-authors and familiar with\\nthe goal of chain of thought prompting.\\nPROMPT FOR MATH WORD PROBLEMS\\nQ:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\\nwill be 21 trees. How many trees did the grove workers plant today?\\nA:There are21trees now andthere are15trees inthebeginning, sothework ersplant 21-15=6trees. The\\nanswer is 6.\\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA:There are3carsinthebeginning, 2more arrive, sonow there should be3+2=5cars. The answer is 5.\\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nA:Leah had32choco lates andhersisterhad42,intotalthey have 32+42=74choco lates. Afterthey ate35,\\nnow there are74-35=39choco lates. The answer is 39.\\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\\nJason give to Denny?\\nA:Jasonstarted with 20lollipops, butnow heonly has12,sohegave Denny 20-12=8lollipops. The answer\\nis 8.\\nQ:Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\\nhave now?\\nA:Shawn got2toys each from hismom anddad, sohegot2*2=4more, now hewillhave 5+4=9toys. The\\nanswer is 9.\\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\\nto thursday. How many computers are now in the server room?\\nA:5computerswere installed from mondaytothurs day, sointotal5*4=20computersareinstalled. 9\\ncomputersarethere inthebeginning, sonow there are20+9=29computers. The answer is 29.\\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\\nballs did he have at the end of wednesday?\\nA:Michael started with 58golfballs andlost23,sohehas58-23=35.Afterhelost2more, hehas35-2=33\\nballs now. The answer is 33.\\nQ:Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?\\nA:5bagels for$3each should cost5*3=15dollars. Olivia had$23inthebeginning, sonow shehas23-15=\\n8dollarsleft. The answer is 8.\\n42', doc_id='adf18c82-e20c-4475-9bb5-895705a637f5', embedding=None, doc_hash='ae7f84d16c1ef9bac7de9f8df6f4e3b3f35c2e8ff9405b6494b0ac362306d235', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='Table 30: Few-shot exemplars for full chain of thought prompt for math word problems. These\\nexemplars are the same as in Table 20, except that the chains of thought were written by a different\\nannotator (“Annotator C” instead of “Annotator A”).\\nPROMPT FOR MATH WORD PROBLEMS\\nQ:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\\nwill be 21 trees. How many trees did the grove workers plant today?\\nA:Westart with 15trees. Later wehave 21trees. Thedifference must bethenumberoftrees they planted. So,\\nthey must have planted 21-15=6trees. The answer is 6.\\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA:There are3carsintheparkinglotalready. 2more arrive. Now there are3+2=5cars. The answer is 5.\\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nA:Leah had32choco lates andLeah’s sisterhad42.That means there were originally 32+42=74choco lates.\\n35have been eaten. Sointotalthey stillhave 74-35=39choco lates. The answer is 39.\\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\\nJason give to Denny?\\nA:Jasonhad20lollipops. Since heonly has12now, hemust have given theresttoDenny. Thenumberof\\nlollipops hehasgiven toDenny must have been 20-12=8lollipops. The answer is 8.\\nQ:Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\\nhave now?\\nA:Hehas5toys. Hegot2from mom, soafterthathehas5+2=7toys. Then hegot2more from dad, soin\\ntotalhehas7+2=9toys. The answer is 9.\\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\\nto thursday. How many computers are now in the server room?\\nA:There are4days from mondaytothurs day.5computerswere added each day.That means intotal4*5=20\\ncomputerswere added. There were 9computersinthebeginning, sonow there are9+20=29computers. The\\nanswer is 29.\\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\\nballs did he have at the end of wednesday?\\nA:Michael initially had58balls. Helost23onTues day, soafterthathehas58-23=35balls. OnWednes day\\nhelost2more sonow hehas35-2=33balls. The answer is 33.\\nQ:Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?\\nA:Shebought 5bagels for$3each. This means shespent 5*$3=$15onthebagels. Shehad$23inbeginning,\\nsonow shehas$23-$15=$8. The answer is 8.\\n43', doc_id='ea19edc7-0e63-42c7-8b4b-f482ae6e7a71', embedding=None, doc_hash='314772600b64ec0ce61b8beea3034f73839fd588636128b16ec229de463c335d', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='arXiv:2302.11382v1  [cs.SE]  21 Feb 2023A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT\\nJules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos O lea, Henry Gilbert,\\nAshraf Elnashar, Jesse Spencer-Smith, and Douglas C. Schmi dt\\nDepartment of Computer Science\\nVanderbilt University, Tennessee\\nNashville, TN, USA\\n{jules.white, quchen.fu, george.s.hays, michael.sandbor n, carlos.olea, henry.gilbert,\\nashraf.elnashar, jesse.spencer-smith, douglas.c.schmi dt}@vanderbilt.edu\\nAbstract —Prompt engineering is an increasingly important\\nskill set needed to converse effectively with large languag e models\\n(LLMs), such as ChatGPT. Prompts are instructions given to a n\\nLLM to enforce rules, automate processes, and ensure speciﬁ c\\nqualities (and quantities) of generated output. Prompts ar e also\\na form of programming that can customize the outputs and\\ninteractions with an LLM.\\nThis paper describes a catalog of prompt engineering tech-\\nniques presented in pattern form that have been applied to so lve\\ncommon problems when conversing with LLMs. Prompt patterns\\nare a knowledge transfer method analogous to software patte rns\\nsince they provide reusable solutions to common problems fa ced\\nin a particular context, i.e., output generation and intera ction\\nwhen working with LLMs.\\nThis paper provides the following contributions to researc h on\\nprompt engineering that apply LLMs to automate software de-\\nvelopment tasks. First, it provides a framework for documen ting\\npatterns for structuring prompts to solve a range of problem s\\nso that they can be adapted to different domains. Second, it\\npresents a catalog of patterns that have been applied succes sfully\\nto improve the outputs of LLM conversations. Third, it expla ins\\nhow prompts can be built from multiple patterns and illustra tes\\nprompt patterns that beneﬁt from combination with other pro mpt\\npatterns.\\nIndex Terms —large language models, prompt patterns, prompt\\nengineering\\nI. I NTRODUCTION\\nConversational large language models (LLMs) [1], such as\\nChatGPT [2], have generated immense interest in a range\\nof domains for tasks ranging from answering questions on\\nmedical licensing exams [3] to generating code snippets. Th is\\npaper focuses on enhancing the application of LLMs in severa l\\ndomains, such as helping developers code effectively and\\nefﬁciently with unfamiliar APIs or allowing students to acq uire\\nnew coding skills and techniques.\\nLLMs are particularly promising in domains where humans\\nand AI tools work together as trustworthy collaborators to\\nmore rapidly and reliably evolve software-reliant systems [4].\\nFor example, LLMs are being integrated directly into softwa re\\ntools, such as Github’s Co-Pilot [5]–[7] and included in int e-\\ngrated development environments (IDEs), such as IntelliJ [ 8]\\nand Visual Studio Code, thereby allowing software teams to\\naccess these tools directly from their preferred IDE.\\nA prompt [9] is a set of instructions provided to an\\nLLM that programs the LLM by customizing it and/or en-\\nhancing or reﬁning its capabilities . A prompt can inﬂuence\\nsubsequent interactions with—and output generated from—a nLLM by providing speciﬁc rules and guidelines for an LLM\\nconversation with a set of initial rules. In particular, a pr ompt\\nsets the context for the conversation and tells the LLM what\\ninformation is important and what the desired output form an d\\ncontent should be.\\nFor example, a prompt could specify that an LLM should\\nonly generate code that follows a certain coding style or\\nprogramming paradigm. Likewise, it could specify that an\\nLLM should ﬂag certain keywords or phrases in a generated\\ndocument and provide additional information related to tho se\\nkeywords. By introducing these guidelines, prompts facili tate\\nmore structured and nuanced outputs to aid a large variety of\\nsoftware engineering tasks in the context of LLMs.\\nPrompt engineering is the means by which LLMs are\\nprogrammed via prompts. To demonstrate the power of\\nprompt engineering, we provide the following prompt:\\nPrompt: “From now on, I would like you to ask me\\nquestions to deploy a Python application to AWS.\\nWhen you have enough information to deploy the\\napplication, create a Python script to automate the\\ndeployment.”\\nThis example prompt causes ChatGPT to begin asking the\\nuser questions about their software application. ChatGPT w ill\\ndrive the question-asking process until it reaches a point w here\\nit has sufﬁcient information to generate a Python script tha t\\nautomates deployment. This example demonstrates the pro-\\ngramming potential of prompts beyond conventional “genera te\\na method that does X” style prompts or “answer this quiz\\nquestion”.\\nMoreover, prompts can be engineered to program an LLM\\nto accomplish much more than simply dictating the output typ e\\nor ﬁltering the information provided to the model. With the\\nright prompt, it is possible to create entirely new interact ion\\nparadigms, such as having an LLM generate and give a quiz\\nassociated with a software engineering concept or tool, or\\neven simulate a Linux terminal window. Moreover, prompts\\nhave the potential for self-adaptation, suggesting other p rompts\\nto gather additional information or generate related artif acts.\\nThese advanced capabilities of prompts highlight the impor -\\ntance of engineering them to provide value beyond simple tex t\\nor code generation.\\nPrompt patterns are essential to effective prompt engi-\\nneering. A key contribution of this paper is the introduction\\nofprompt patterns to document successful approaches for', doc_id='4ca9deb8-d20e-41f8-b559-7c34760e5557', embedding=None, doc_hash='debb47e5cf2c0a3ee8af5dc3f5ac12a13995a1daaaef54e204a614e6b4f3b1ef', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='systematically engineering different output and interact ion\\ngoals when working with conversational LLMs. We focus\\nlargely on engineering domain-independent prompt pattern s\\nand introduce a catalog of essential prompt patterns to solv e\\nproblems ranging from production of visualizations and cod e\\nartifacts to automation of output steps that help fact check\\noutputs.\\nThe remainder of this paper is organized as follows: Sec-\\ntion II introduces prompt patterns and compares these patte rns\\nto well-known software patterns [10]; Section III describe s\\n16 prompt patterns that have been applied to solve common\\nproblems in the domain of conversational LLM interaction an d\\noutput generation for automating software development tas ks;\\nSection IV discusses related work; and Section V presents\\nconcluding remarks and lessons learned.\\nII. C OMPARING SOFTWARE PATTERNS\\nWITH PROMPT PATTERNS\\nThe quality of the output(s) generated by a conversational\\nLLM is directly related to the quality of the prompts provide d\\nby the user. As discussed in Section I, the prompts given to\\na conversational LLM can be used to program interactions\\nbetween a user and an LLM to better solve a variety of\\nproblems. One contribution of this paper is the framework it\\nprovides to document patterns that structure prompts to sol ve\\na range of software tasks that can be adapted to different\\ndomains.\\nThis framework is useful since it focuses on codifying\\npatterns that can be applied to help users better interact\\nwith conversational LLMs in a variety of contexts, rather\\nthan simply discussing interesting examples or domain-spe ciﬁc\\nprompts. Codifying this knowledge in pattern form enhances\\nreuse and transferability to other contexts and domains whe re\\nusers face similar—but not identical—problems.\\nThe topic of knowledge transfer has been studied exten-\\nsively in the software patterns literature [10], [11] at mul tiple\\nlevels, e.g., design, architectural, and analysis. This paper\\napplies a variant of a familiar pattern form as the basis of\\nour prompt engineering approach. Since prompts are a form\\nof programming, it is natural to document them in pattern\\nform.\\nA. Overview of Software Patterns\\nA software pattern provides a reusable solution to a recur-\\nring problem within a particular context [10]. Documenting\\nsoftware patterns concisely conveys (and generalizes) fro m\\nspeciﬁc problems being addressed to identify important for ces\\nand/or requirements that should be resolved and/or address ed\\nin successful solutions.\\nA pattern form also includes guidance on how to implement\\nthe pattern, as well as information on the trade-offs and\\nconsiderations to take into account when implementing a\\npattern. Moreover, example applications of the pattern are\\noften provided to further showcase the pattern’s utility in\\npractice. Software patterns are typically documented in astylized form to facilitate their use and understanding, su ch\\nas:\\n•A name and classiﬁcation . Each pattern has a name that\\nidentiﬁes the pattern and should be used consistently. A\\nclassiﬁcation groups patterns into broad categories, such\\nas creational, structural, or behavioral.\\n•The intent concisely conveys the purpose the pattern is\\nintended to achieve.\\n•The motivation documents the underlying problem the\\npattern is meant to solve and the importance of the\\nproblem.\\n•The structure and participants . The structure describes\\nthe different pattern participants (such as classes and\\nobjects) and how they collaborate to form a generalized\\nsolution.\\n•Example code concretely maps the pattern to some\\nunderlying programming language(s) and aids developers\\nin gaining greater insight into how that pattern can be\\napplied effectively.\\n•Consequences summarize the pros and cons of applying\\nthe pattern in practice.\\nB. Overview of Prompt Patterns\\nPrompt patterns are similar to software patterns in that the y\\noffer reusable solutions to speciﬁc problems. They focus mo re\\nspeciﬁcally, however, on the context of output generation f rom\\nlarge-scale language models (LLMs), such as ChatGPT. Just\\nas software patterns provide a codiﬁed approach to solving\\ncommon software development challenges, prompt patterns\\nprovide a codiﬁed approach to customizing the output and\\ninteractions of LLMs.\\nBy documenting and leveraging prompt patterns in the\\ncontext of automating software development tasks, individ ual\\nusers and teams can enforce constraints on the generated\\noutput, ensure that relevant information is included, and\\nchange the format of interaction with the LLM to better\\nsolve problems they face. Prompt patterns can be viewed as a\\ncorollary to the broad corpus of general software patterns, just\\nadapted to the more speciﬁc context of LLM output generation .\\nPrompt patterns follow a similar format to classic software\\npatterns, with slight modiﬁcations to match the context of\\noutput generation with LLMs.1Each of the analogous sections\\nfor the prompt pattern form used in this paper is summarized\\nbelow:\\n•A name and classiﬁcation . The prompt pattern name\\nuniquely identiﬁes the pattern and ideally indicates the\\nproblem that is being addressed. For the classiﬁcation,\\nwe have developed a series of initial categories of pattern\\ntypes, which are summarized in Table I and include\\nOutput Customization ,Error Identiﬁcation ,Prompt\\nImprovement ,Interaction , and Context Control .\\n•The intent and context describes the problem the prompt\\npattern solves and the goals it achieves. The problem\\n1The most direct translation of software pattern structure t o prompt patterns\\nis the naming, intent, motivation, and sample code. The stru cture and\\nclassiﬁcation, however, although named similarly, requir e more adaptation.', doc_id='642a9351-b71b-42b6-978e-cc50fa01def6', embedding=None, doc_hash='0792b8ebed2d27b01aba6600d1c03740b3e43f6f5465a2613c931111444e7c16', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='should ideally be independent of any domain, though\\ndomain-speciﬁc patterns may also be documented with\\nan appropriate discussion of the context where the pattern\\napplies.\\n•The motivation provides the rationale for the problem\\nand explains why solving it is important. The motivation\\nis explained in the context of users interacting with a\\nconversational LLM and how it can improve upon users\\ninformally prompting the LLM in one or more circum-\\nstances. Speciﬁc circumstances where the improvements\\nare expected are documented.\\n•The structure and key ideas . The structure describes\\nthe fundamental contextual information, as a series of\\nkey ideas, that the prompt pattern provides to the LLM.\\nThese ideas are similar to “participants” in a software pat-\\ntern. The contextual information may be communicated\\nthrough varying wording (just as a software pattern can\\nhave variations in how it is realized in code), but should\\nhave fundamental pieces of information that form a core\\nelement of the pattern.\\n•Example implementation demonstrates how the prompt\\npattern is worded in practice.\\n•Consequences summarize the pros and cons of applying\\nthe pattern and may provide guidance on how to adapt\\nthe prompt to different contexts.\\nC. Evaluating Means for Deﬁning a Prompt Pattern’s Struc-\\nture and Ideas\\nIn software patterns, the structure and participants are\\nnormally deﬁned in terms of UML diagrams, such as structure\\ndiagrams and/or interaction diagrams. These UML diagrams\\nexplain what the participants of the pattern are and how they\\ninteract to solve the problem. In prompt patterns, somethin g\\nanalogous is needed, though UML may not be an appro-\\npriate structural documentation approach since it is inten ded\\nto describe software structures, as opposed to the ideas to\\ncommunicate in a prompt.\\nSeveral possible approaches could be used, ranging from di-\\nagrams to deﬁning grammars for a prompt language. Although\\ngrammars may seem attractive due to their formal nature, the y\\nalso incur the following challenges:\\n•The goal of prompts is to communicate knowledge in a\\nclear and concise way to conversation LLM users, who\\nmay or may not be computer scientists or programmers.\\nAs a community, we should strive to create an approach-\\nable format that communicates knowledge clearly to a\\ndiverse target audience.\\n•It is possible to phrase a prompt in many different ways.\\nIt is hard, however, to deﬁne a grammar that accurately\\nand completely expresses all the nuanced ways that\\ncomponents of a prompt could be expressed in text or\\nsymbols.\\n•Prompts fundamentally convey ideas to a conversational\\nLLM and are not simply the production of tokens for\\ninput. In particular, an idea built into a prompt pattern\\ncan be communicated in many ways and its expressionshould be at a higher-level than the underlying tokens\\nrepresenting the idea.\\n•It is possible to program an LLM to introduce novel\\nsemantics for statements and words that create new ways\\nfor communicating an idea. In contrast, grammars may\\nnot easily represent ideas that can be expressed through\\ncompletely new symbology or languages that the gram-\\nmar designer was not aware of.\\nD. A Way Forward: Fundamental Contextual Statements\\nAn open research question, therefore, is what approach is\\nmore effective than formal grammars for describing prompt\\npattern structure and ideas. We propose the concept of funda-\\nmental contextual statements , which are written descriptions\\nof the important ideas to communicate in a prompt to an LLM.\\nAn idea can be rewritten and expressed in arbitrary ways base d\\non user needs and experience. The key ideas to communicate,\\nhowever, are presented to the user as a series of simple, but\\nfundamental, statements.\\nOne beneﬁt of adopting and applying the fundamental con-\\ntextual statements approach is that it is intentionally int uitive\\nto users. In particular, we expect users will understand how to\\nexpress and adapt the statements in a contextually appropri ate\\nway for their domain. Moreover, since the underlying ideas o f\\nthe prompt are captured, these same ideas can be expressed\\nby the user in alternate symbology or wording that has been\\nintroduced to the LLM using patterns, such as the Meta\\nLanguage Creation pattern presented in Section III-B.\\nOur ultimate goal is to enhance prompt engineering by\\nproviding a framework for designing prompts that can be\\nreused and/or adapted to other LLMs in the same way that\\nsoftware patterns can be implemented in different program-\\nming languages and platforms. For the purposes of this paper ,\\nhowever, all prompts were tested with ChatGPT [12] using the\\nChatGPT+ service. We use ChatGPT as the LLM for all exam-\\nples presented in this paper due to its widespread availabil ity\\nand popularity. These examples were documented through a\\ncombination of exploring the corpus of community-posted\\nprompts on the Internet and independent prompt creation fro m\\nour use of ChatGPT to automating software development\\ntasks.\\nIII. A C ATALOG OF PROMPT PATTERNS\\nFOR CONVERSATIONAL LLM S\\nThis section presents our catalog of prompt patterns that\\nhave been applied to solve common problems in the domain\\nof conversational LLM interaction and output generation fo r\\nautomating software tasks. Each prompt pattern is accompa-\\nnied by concrete implementation samples and examples with\\nand without the prompt.\\nA. Summary of the Prompt Pattern Catalog\\nThe classiﬁcation of prompt patterns is an important consid -\\neration in documenting the patterns. Table I outlines the in itial\\nclassiﬁcations for the catalog of prompt patterns we identi ﬁed\\nin our work with ChatGPT thus far.', doc_id='58f8f4e5-408e-4313-b8ac-ad18f1252916', embedding=None, doc_hash='43d20feae5909d977d81a65436413db61baf17038b5a7a14fe4f0cd223ff53bf', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='TABLE I\\nCLASSIFYING PROMPT PATTERNS\\nPattern Category Prompt Pattern\\nInput Semantics Meta Language Creation\\nOutput Output Automater\\nCustomization Persona\\nVisualization Generator\\nRecipe\\nTemplate\\nError Identiﬁcation Fact Check List\\nReﬂection\\nPrompt Question Reﬁnement\\nImprovement Alternative Approaches\\nCognitive Veriﬁer\\nRefusal Breaker\\nInteraction Flipped Interaction\\nGame Play\\nInﬁnite Generation\\nContext Control Context Manager\\nAs shown in this table, there are ﬁve categories of prompt\\npatterns in our classiﬁcation framework: Input Semantics ,\\nOutput Customization ,Error Identiﬁcation ,Prompt Im-\\nprovement , and Interaction , each of which is summarized\\nbelow.\\nThe Input Semantics category deals with how an LLM\\nunderstands the input and how it translates the input into\\nsomething it can use to generate output. This category in-\\ncludes the Meta Language Creation pattern, which focuses on\\ncreating a custom language for the LLM to understand. This\\npattern is useful when the default input language is ill-sui ted\\nfor expressing ideas the user wants to convey to the LLM.\\nTheOutput Customization category focuses on constrain-\\ning or tailoring the types, formats, structure, or other pro perties\\nof the output generated by the LLM. The prompt patterns in\\nthis category include Output Automater ,Persona ,Visualiza-\\ntion Generator ,Recipe , and Template patterns. The Output\\nAutomater pattern allows the user to create scripts that can\\nautomate any tasks the LLM output suggests the user should\\nperform. The Persona pattern gives the LLM a persona or role\\nto play when generating output. The Visualization Generator\\npattern allows the user to generate visualizations by produ cing\\ntextual outputs that can be fed to other tools, such as other\\nAI-based image generators, like DALL-E [13]. The Recipe\\npattern allows the user to obtain a sequence of steps or actio ns\\nto realize a stated end result, possibly with partially know n\\ninformation or constraints. The Template pattern allows the\\nuser to specify a template for the output, which the LLM ﬁlls\\nin with content.\\nThe Error Identiﬁcation category focuses on identifying\\nand resolving errors in the output generated by the LLM. Thiscategory includes the Fact Check List andReﬂection patterns.\\nThe Fact Check List pattern requires the LLM to generate a\\nlist of facts the output depends on that should be fact-check ed.\\nThe Reﬂection pattern requires the LLM to introspect on its\\noutput and identify any errors.\\nThe Prompt Improvement category focuses on improving\\nthe quality of the input and output. This category includes\\ntheQuestion Reﬁnement ,Alternative Approaches ,Cognitive\\nVeriﬁer , and Refusal Breaker patterns. The Question Reﬁne-\\nment pattern ensures the LLM always suggests a better version\\nof the user’s question. The Alternative Approaches pattern\\nrequires the LLM to suggest alternative ways of accomplishi ng\\na user-speciﬁed task. The Cognitive Veriﬁer pattern instructs\\nthe LLM to automatically suggest a series of subquestions\\nfor the user to answer before combining the answers to the\\nsubquestions and producing an answer to the overall questio n.\\nTheRefusal Breaker pattern requires the LLM to automatically\\nreword the user’s question when it refuses to produce an\\nanswer.\\nThe Interaction category focuses on the interaction be-\\ntween the user and the LLM. This category includes the\\nFlipped Interaction ,Game Play , and Inﬁnite Generation pat-\\nterns. The Flipped Interaction pattern requires the LLM to\\nask questions rather than generate output. The Game Play\\npattern requires the LLM to generate output in the form of\\na game. The Inﬁnite Generation pattern requires the LLM to\\ngenerate output indeﬁnitely without the user having to reen ter\\nthe generator prompt each time.\\nFinally, the Context Control category focuses on control-\\nling the contextual information in which the LLM operates.\\nThis category includes the Context Manager pattern, which\\nallows the user to specify the context for the LLM’s output.\\nThe remainder of this section describes each of these prompt\\npatterns using the pattern form discussed in Section II-B.\\nB. The Meta Language Creation Pattern\\n1) Intent and Context: During a conversation with an LLM,\\nthe user would like to create the prompt via an alternate\\nlanguage, such as a textual short-hand notation for graphs, a\\ndescription of states and state transitions for a state mach ine, a\\nset of commands for prompt automation, etc. The intent of thi s\\npattern is to explain the semantics of this alternative lang uage\\nto the LLM so the user can write future prompts using this\\nnew language and its semantics.\\n2) Motivation: Many problems, structures, or other ideas\\ncommunicated in a prompt may be more concisely, unam-\\nbiguously, or clearly expressed in a language other than\\nEnglish (or whatever conventional human language is used\\nto interact with an LLM). To produce output based on an\\nalternative language, however, an LLM needs to understand\\nthe language’s semantics.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWhen I say X, I mean Y (or would like you to do Y)', doc_id='02fe3ac1-8d72-440e-b9c5-f55c7fcc3778', embedding=None, doc_hash='000d96583f3ba45b2481d2f1e8db97e9e3b379794fe73e22128508833154ee6a', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='The key structure of this pattern involves explaining the\\nmeaning of one or more symbols, words, or statements to\\nthe LLM so it uses the provided semantics for the ensuing\\nconversation. This description can take the form of a simple\\ntranslation, such as “X” means “Y”. The description can also\\ntake more complex forms that deﬁne a series of commands\\nand their semantics, such as “when I say X, I want you to do\\n”. In this case, “X” is henceforth bound to the semantics of\\n“take action”.\\n4) Example Implementation: The key to successfully using\\ntheMeta Language Creation pattern is developing an unam-\\nbiguous notation or shorthand, such as the following:\\n“From now on, whenever I type two identiﬁers\\nseparated by a “ →”, I am describing a graph. For\\nexample, “a →b” is describing a graph with nodes\\n“a” and “b” and an edge between them. If I separate\\nidentiﬁers by “-[w:2, z:3] →”, I am adding properties\\nof the edge, such as a weight or label.”\\nThis example of the Meta Language Creation pattern estab-\\nlishes a standardized notation for describing graphs by deﬁ ning\\na convention for representing nodes and edges. Whenever the\\nauthor types two identiﬁers separated by a “ →” symbol, it is\\nan indication that a graph is being described. For example, i f\\nthe author types “a →b”, this indicates that a graph is being\\ndeﬁned with nodes “a” and “b”, and that there is an edge\\nbetween them. This convention provides a clear and concise\\nway to communicate the structure of a graph in written form.\\nMoreover, the prompt goes on to specify that additional\\ninformation about the edges, such as a weight or label, can\\nbe provided using the syntax “-[w:2, z:3] →”. This notation\\nallows for the speciﬁcation of additional properties beyon d\\nthe basic structure of the graph. The speciﬁed properties ar e\\nassociated with the edge between the two nodes and can\\nprovide important context for the interpretation of the gra ph.\\nThis standardized notation for describing graphs can make i t\\neasier to communicate graph structures and properties, whi ch\\nmay not be easy or are very verbose to describe as a series of\\nsentences.\\n5) Consequences: Although this pattern provides a pow-\\nerful means to customize a user’s interaction with an LLM,\\nit may create the potential for confusion within the LLM.\\nAs important as it is to clearly deﬁne the semantics of the\\nlanguage, it is also essential to ensure the language itself\\nintroduces no ambiguities that degrade the LLM’s performan ce\\nor accuracy. For example, the prompt “whenever I separate\\ntwo things by commas, it means that the ﬁrst thing precedes\\nthe second thing” will likely create signiﬁcant potential f or\\nambiguity and create the potential for unexpected semantic s\\nif punctuation involving commas is used in the prompt.\\nTo showcase the potential for confusion, ChatGPT will\\nwarn the user and potentially be unable to perform a speciﬁc\\nmapping of a symbol or term to a new meaning. For example,\\nif following prompt is given to ChatGPT\\n“Whenever I say “a”, I am referring to Marie An-\\ntoinette.”it will respond that this prompt creates too much confusion\\nas “a” is an indeﬁnite article and too commonly used, as shown\\nbelow:\\n“Referring to Marie Antoinette using “a” could\\npotentially cause confusion, as “a” is an indeﬁnite\\narticle commonly used to refer to a noun that has\\nnot been mentioned before. It may be more clear\\nto use her name, “Marie Antoinette”, or a pronoun\\nsuch as “she” or “her” to refer to her. Moreover,\\nusing context to establish who you are referring to\\ncan also help avoid confusion in conversation.”\\nIn general, an LLM will perform better on data it was\\ntrained on. Therefore, when an existing notation is availab le—\\nbut may not be the dominant meaning—the Meta Language\\nCreation pattern can provide context to scope the meaning of\\nthe symbol to improve the accurately and utility of the outpu t.\\nFor example, the symbol “ →” conveys slightly different infor-\\nmation in graphs and propositional logic. The statement “p →\\nq, q→p” may therefore make sense, but only if the LLM\\nis given the appropriate context through a Meta Language\\nCreation prompt.\\nAlthough Meta Language Creation is a powerful pattern it\\nmust be used carefully, particularly when describing conce pts\\nthat may otherwise be hard to precisely or concisely describ e.\\nThese types of prompts are thus best used in completely\\nnew conversation sessions. Using a single meta-language-p er-\\nconversation session may also be a best practice since it avo ids\\nthe potential for conﬂicting or unexpected semantics being\\napplied to the conversation over time.\\nC. The Output Automater Pattern\\n1) Intent and Context: The intent of this pattern is to have\\nthe LLM generate a script or other automation artifact that c an\\nautomatically perform any steps it recommends taking as par t\\nof its output. The goal is to reduce the manual effort needed\\nto implement any LLM output recommendations.\\n2) Motivation: The output of an LLM is often a sequence\\nof steps for the user to follow. For example, when asking an\\nLLM to generate a Python conﬁguration script it may suggest\\na number of ﬁles to modify and changes to apply to each ﬁle.\\nHowever, having users continually perform the manual steps\\ndictated by LLM output is tedious and error-prone.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWhenever you produce an output that has at least one\\nstep to take and the following properties (alternatively,\\nalways do this)\\nProduce an executable artifact of type X that will\\nautomate these steps\\nThe ﬁrst part of the pattern identiﬁes the situations under\\nwhich automation should be generated. A simple approach\\nis to state that the output includes at least two steps to\\ntake and that an automation artifact should be produced. The', doc_id='cb40f94c-1fef-44d5-b610-33ca5e8aa569', embedding=None, doc_hash='684de12b4526594ee7d9e700e993b84c75be9bf023e038d8dcfd1a00f2d61acc', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='scoping is up to the user, but helps prevent producing an\\noutput automation scripts in cases where running the output\\nautomation script will take more user effort than performin g\\nthe original steps produced in the output. The scope can be\\nlimited to outputs requiring more than a certain number of\\nsteps.\\nThe next part of this pattern provides a concrete statement\\nof the type of output the LLM should output to perform the\\nautomation. For example, “produce a Python script” gives th e\\nLLM a concrete understanding to translate the general steps\\ninto equivalent steps in Python. The automation artifact sh ould\\nbe concrete and must be something that the LLM associates\\nwith the action of “automating a sequence of steps”.\\n4) Example Implementation: A sample of this prompt pat-\\ntern applied to code snippets generated by the ChatGPT LLM\\nis shown below:\\n“From now on, whenever you generate code that\\nspans more than one ﬁle, generate a Python script\\nthat can be run to automatically create the speciﬁed\\nﬁles or make changes to existing ﬁles to insert the\\ngenerated code.”\\nThis pattern is particularly effective in software enginee ring\\nas a common task for software engineers using LLMs is to\\nthen copy/paste the outputs into multiple ﬁles. Some tools,\\nsuch as Copilot, insert limited snippets directly into the s ection\\nof code that the coder is working with, but tools, such as\\nChatGPT, do not provide these facilities. This automation t rick\\nis also effective at creating scripts for running commands o n\\na terminal, automating cloud operations, or reorganizing ﬁ les\\non a ﬁle system.\\nThis pattern is a powerful complement for any system that\\ncan be computer controlled. The LLM can provide a set of\\nsteps that should be taken on the computer-controlled syste m\\nand then the output can be translated into a script that allow s\\nthe computer controlling the system to automatically take\\nthe steps. This is a direct pathway to allowing LLMs, such\\nas ChatGPT, to integrate quality into—and to control—new\\ncomputing systems that have a known scripting interface.\\n5) Consequences: An important usage consideration of\\nthis pattern is that the automation artifact must be deﬁned\\nconcretely. Without a concrete meaning for how to “automate ”\\nthe steps, the LLM often states that it “can’t automate thing s”\\nsince that is beyond its capabilities. LLMs typically accep t\\nrequests to produce code, however, so the goal is to instruct the\\nLLM to generate text/code, which can be executed to automate\\nsomething. This subtle distinction in meaning is important to\\nhelp an LLM disambiguate the prompt meaning.\\nOne caveat of the Output Automater pattern is the LLM\\nneeds sufﬁcient conversational context to generate an auto ma-\\ntion artifact that is functional in the target context, such as\\nthe ﬁle system of a project on a Mac vs. Windows computer.\\nThis pattern works best when the full context needed for the\\nautomation is contained within the conversation, e.g., when\\na software application is generated from scratch using the\\nconversation and all actions on the local ﬁle system are\\nperformed using a sequence of generated automation artifac tsrather than manual actions unknown to the LLM. Alternativel y,\\nself-contained sequences of steps work well, such as “how do\\nI ﬁnd the list of open ports on my Mac computer”.\\nIn some cases, the LLM may produce a long output with\\nmultiple steps and not include an automation artifact. This\\nomission may arise for various reasons, including exceedin g\\nthe output length limitation the LLM supports. A simple\\nworkaround for this situation is to remind the LLM via a\\nfollow-on prompt, such as “But you didn’t automate it”, whic h\\nprovides the context that the automation artifact was omitt ed\\nand should be generated.\\nAt this point in the evolution of LLMs, the Output Auto-\\nmater pattern is best employed by users who can read and\\nunderstand the generated automation artifact. LLMs can (an d\\ndo) produce inaccuracies in their output, so blindly accept ing\\nand executing an automation artifact carries signiﬁcant ri sk.\\nAlthough this pattern may alleviate the user from performin g\\ncertain manual steps, it does not alleviate their responsib ility\\nto understand the actions they undertake using the output.\\nWhen users execute automation scripts, therefore they assu me\\nresponsibility for the outcomes.\\nD. The Flipped Interaction Pattern\\n1) Intent and Context: You want the LLM to ask questions\\nto obtain the information it needs to perform some tasks.\\nRather than the user driving the conversation, therefore, y ou\\nwant the LLM to drive the conversation to focus it on\\nachieving a speciﬁc goal. For example, you may want the\\nLLM to give you a quick quiz or automatically ask questions\\nuntil it has sufﬁcient information to generate a deployment\\nscript for your application to a particular cloud environme nt.\\n2) Motivation: Rather than having the user drives a con-\\nversation, an LLM often has knowledge it can use to more\\naccurately obtain information from the user. The goal of the\\nFlipped Interaction pattern is to ﬂip the interaction ﬂow so the\\nLLM asks the user questions to achieve some desired goal. The\\nLLM can often better select the format, number, and content\\nof the interactions to ensure that the goal is reached faster ,\\nmore accurately, and/or by using knowledge the user may not\\n(initially) possess.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nI would like you to ask me questions to achieve X\\nYou should ask questions until this condition is met or\\nto achieve this goal (alternatively, forever)\\n(Optional) ask me the questions one at a time, two at\\na time, etc.\\nA prompt for a ﬂipped interaction should always specify the\\ngoal of the interaction. The ﬁrst idea ( i.e., you want the LLM to\\nask questions to achieve a goal) communicates this goal to th e\\nLLM. Equally important is that the questions should focus on a\\nparticular topic or outcome. By providing the goal, the LLM\\ncan understand what it is trying to accomplish through the\\ninteraction and tailor its questions accordingly. This “in version', doc_id='2fc9080f-6fbe-43bb-9749-45ca8d3f5595', embedding=None, doc_hash='1d6f99c4b35e38b563ab635e73e7b1e19284c7595891cc6485bfa4566e4f9454', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='of control” enables more focused and efﬁcient interaction s ince\\nthe LLM will only ask questions that it deems relevant to\\nachieving the speciﬁed goal.\\nThe second idea provides the context for how long the in-\\nteraction should occur. A ﬂipped interaction can be termina ted\\nwith a response like “stop asking questions”. It is often bet ter,\\nhowever, to scope the interaction to a reasonable length or\\nonly as far as is needed to reach the goal. This goal can be\\nsurprisingly open-ended and the LLM will continue to work\\ntowards the goal by asking questions, as is the case in the\\nexample of ”until you have enough information to generate a\\nPython script”.\\nBy default, the LLM is likely to generate multiple questions\\nper iteration. The third idea is completely optional, but ca n\\nimprove usability by limiting (or expanding) the number of\\nquestions that the LLM generates per cycle. If a precise\\nnumber/format for the questioning is not speciﬁed, the ques -\\ntioning will be semi-random and may lead to one-at-a-time\\nquestions or ten-at-a-time questions. The prompt can thus b e\\ntailored to include the number of questions asked at a time,\\nthe order of the questions, and any other formatting/orderi ng\\nconsiderations to facilitate user interaction.\\n4) Example Implementation: A sample prompt for a ﬂipped\\ninteraction is shown below:\\n“From now on, I would like you to ask me questions\\nto deploy a Python application to AWS. When you\\nhave enough information to deploy the application,\\ncreate a Python script to automate the deployment.”\\nIn general, the more speciﬁc the prompt regarding the\\nconstraints and information to collect, the better the outc ome.\\nFor instance, the example prompt above could provide a menu\\nof possible AWS services (such as Lambda, EC2, etc.) with\\nwhich to deploy the application. In other cases, the LLM may\\nbe permitted to simply make appropriate choices on its own fo r\\nthings that the user doesn’t explicitly make decisions abou t.\\nOne limitation of this prompt is that, once other contextual\\ninformation is provided regarding the task, it may require\\nexperimentation with the precise phrasing to get the LLM to\\nask the questions in the appropriate number and ﬂow to best\\nsuit the task, such as asking multiple questions at once vers us\\none question at a time.\\n5) Consequences: One consideration when designing the\\nprompt is how much to dictate to the LLM regarding what\\ninformation to collect prior to termination. In the example\\nabove, the ﬂipped interaction is open-ended and can vary sig -\\nniﬁcantly in the ﬁnal generated artifact. This open-endedn ess\\nmakes the prompt generic and reusable, but may potentially\\nask additional questions that could be skipped if more conte xt\\nis given.\\nIf speciﬁc requirements are known in advance, it is better to\\ninject them into the prompt rather than hoping the LLM will\\nobtain the needed information. Otherwise, the LLM will non-\\nnondeterministically decide whether to prompt the user for the\\ninformation or make an educated guess as to an appropriate\\nvalue.For example, the user can state that they would like to\\ndeploy an application to Amazon AWS EC2, rather than\\nsimply state ”the cloud” and require multiple interactions to\\nnarrow down the deployment target. The more precise the\\ninitial information, the better the LLM can use the limited\\nquestions that a user is likely willing to answer to obtain\\ninformation to improve its output.\\nWhen developing prompts for ﬂipped interactions, it is im-\\nportant to consider the level of user knowledge, engagement ,\\nand control. If the goal is to accomplish the goal with as litt le\\nuser interaction as possible (minimal control), that shoul d be\\nstated explicitly.Conversely, if the goal is to ensure the u ser\\nis aware of all key decisions and conﬁrms them (maximum\\nengagement) that should also be stated explicitly. Likewis e, if\\nthe user is expected to have minimal knowledge and should\\nhave the questions targeted at their level of expertise, thi s\\ninformation should be engineered into the prompt.\\nE. The Persona Pattern\\n1) Intent and Context: In many cases, users would like\\nLLM output to always take a certain point of view or per-\\nspective. For example, it may be useful for to conduct a code\\nreview as if the LLM was a security expert. The intent of this\\npattern is to give the LLM a “persona” that helps it select wha t\\ntypes of output to generate and what details to focus on.\\n2) Motivation: Users may not know what types of outputs\\nor details are important for an LLM to focus on to achieve\\na given task. They may know, however, the role or type of\\nperson that they would normally ask to get help with these\\nthings. The Persona pattern enables the users to express what\\nthey need help with without knowing the exact details of the\\noutputs they need.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nAct as persona X\\nProvide outputs that persona X would create\\nThe ﬁrst statement conveys the idea that the LLM needs\\nto act as a speciﬁc persona and provide outputs that such a\\npersona would. This persona can be expressed in a number\\nof ways, ranging from a job description, title, ﬁctional cha r-\\nacter, historical ﬁgure, etc. The persona should elicit a se t\\nof attributes associated with a well-known job title, type o f\\nperson, etc.2\\nThe secondary idea—provide outputs that persona X would\\ncreate—offers opportunities for customization. For examp le, a\\nteacher might provide a large variety of different output ty pes,\\nranging from assignments to reading lists to lectures. If a m ore\\nspeciﬁc scope to the type of output is known, the user can\\nprovide it in this statement.\\n2Be aware, however, that personas relating to living people o r people\\nconsidered harmful make be disregarded due to underlying LL M privacy and\\nsecurity rules.', doc_id='4f87d3a4-31a2-4da6-a43a-338b08c3df8a', embedding=None, doc_hash='f67b5e5bc87de7429256da95d6cc068e3107879586ed0ea97e226c3bd3e2ea91', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='4) Example Implementation: A sample implementation for\\ncode review is shown below:\\n“From now on, act as a security reviewer. Pay close\\nattention to the security details of any code that\\nwe look at. Provide outputs that a security reviewer\\nwould regarding the code.”\\nIn this example, the LLM is instructed to provide outputs\\nthat a ”security reviewer” would. The prompt further sets th e\\nstage that code is going to be evaluated. Finally, the user\\nreﬁnes the persona by scoping the persona further to outputs\\nregarding the code.\\nPersonas can also represent inanimate or non-human en-\\ntities, such as a Linux terminal, a database, or an animal’s\\nperspective. When using this pattern to represent these ent ities,\\nit can be useful to also specify how you want the inputs\\ndelivered to the entity, such as “assume my input is what the\\nowner is saying to the dog and your output is the sounds the\\ndog is making”. An example prompt for a non-human entity\\nthat uses a “pretend to be” wording is shown below:\\n“You are going to pretend to be a Linux terminal\\nfor a computer that has been compromised by an\\nattacker. When I type in a command, you are going\\nto output the corresponding text that the Linux\\nterminal would produce.”\\nThis prompt is designed to simulate a computer that has\\nbeen compromised by an attacker and is being controlled\\nthrough a Linux terminal. The prompt speciﬁes that the user\\nwill input commands into the terminal, and in response, the\\nsimulated terminal will output the corresponding text that\\nwould be produced by a real Linux terminal. This prompt\\nis more prescriptive in the persona and asks the LLM to, not\\nonly be a Linux terminal, but to further act as a computer that\\nhas been compromised by an attacker.\\nThe persona causes ChatGPT to generate outputs to com-\\nmands that have ﬁles and contents indicative of a computer th at\\nwas hacked. The example illustrates how an LLM can bring\\nits situational awareness to a persona, in this case, creati ng\\nevidence of a cyberattack in the outputs it generates. This\\ntype of persona can be very effective for combining with the\\nGame Play pattern, where you want the exact details of the\\noutput characteristics to be hidden from the user (e.g., don ’t\\ngive away what the cyberattack did by describing it explicit ly\\nin the prompt).\\n5) Consequences: An interesting aspect of taking non-\\nhuman personas is that the LLM may make interesting as-\\nsumptions or “hallucinations” regarding the context. A wid ely\\ncirculated example on the Internet asks ChatGPT to act as\\na Linux terminal and produce the expected output that you\\nwould get if the user typed the same text into a terminal.\\nCommands, such as ls -l , will generate a ﬁle listing for an\\nimaginary UNIX ﬁle system, complete with ﬁles that can have\\ncat file1.txt run on them.\\nIn other examples, the LLM may prompt the user for more\\ncontext, such as when ChatGPT is asked to act as a MySQL\\ndatabase and prompts for the structure of a table that the use ris pretending to query. ChatGPT can then generate synthetic\\nrows, such as generating imaginary rows for a “people” table\\nwith columns for “name” and “job”.\\nF . The Question Reﬁnement Pattern\\n1) Intent and Context: This pattern engages the LLM in\\nthe prompt engineering process. The intent of this pattern i s\\nto ensure the conversational LLM always suggests potential ly\\nbetter or more reﬁned questions the user could ask instead of\\ntheir original question. Using this pattern, the LLM can aid the\\nuser in ﬁnding the right question to ask in order to arrive at a n\\naccurate answer. In addition, the LLM may help the user ﬁnd\\nthe information or achieve their goal in fewer interactions with\\nthe user than if the user employed trial and error prompting.\\n2) Motivation: If a user is asking a question, it is possible\\nthey are not an expert in the domain and may not know the\\nbest way to phrase the question or be aware of additional\\ninformation helpful in phrasing the question. LLMs will oft en\\nstate limitations on the answer they are providing or reques t\\nadditional information to help them produce a more accurate\\nanswer. An LLM may also state assumptions it made in\\nproviding the answer. The motivation is that this additiona l\\ninformation or set of assumptions could be used to generate\\na better prompt. Rather than requiring the user to digest\\nand rephrase their prompt with the additional information,\\nthe LLM can directly reﬁne the prompt to incorporate the\\nadditional information.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWithin scope X, suggest a better version of the question\\nto use instead\\n(Optional) prompt me if I would like to use the better\\nversion instead\\nThe ﬁrst contextual statement in the prompt is asking the\\nLLM to suggest a better version of a question within a speciﬁc\\nscope. The scope is provided to ensure that not all questions\\nare automatically reworded or that they are reﬁned with a\\ngiven goal. The second contextual statement is meant for\\nautomation and allows the user to automatically use the reﬁn ed\\nquestion without having to copy/paste or manually enter it. The\\nengineering of this prompt can be further reﬁned by combinin g\\nit with the Reﬂection pattern, which allows the LLM to explain\\nwhy it believes the reﬁned question is an improvement.\\n4) Example Implementation:\\n“From now on, whenever I ask a question about a\\nsoftware artifact’s security, suggest a better version\\nof the question to use that incorporates information\\nspeciﬁc to security risks in the language or frame-\\nwork that I am using instead and ask me if I would\\nlike to use your question instead.”\\nIn the context of the example above, the LLM will use\\ntheQuestion Reﬁnement pattern to improve security-related\\nquestions by asking for or using speciﬁc details about the', doc_id='065aa610-4ffc-4d55-8371-73f6ee3ae656', embedding=None, doc_hash='21c78889980f53991f4cd3fea45ad3f9a6a091529a3e7b55bd7c8223d0e34d79', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='software artifact and the language or framework used to buil d\\nit. For instance, if a developer of a Python web application w ith\\nFastAPI asks ChatGPT “How do I handle user authentication\\nin my web application?”, the LLM will reﬁne the question\\nby taking into account that the web application is written in\\nPython with FastAPI. The LLM then provides a revised ques-\\ntion that is more speciﬁc to the language and framework, such\\nas “What are the best practices for handling user authentica tion\\nsecurely in a FastAPI web application to mitigate common\\nsecurity risks, such as cross-site scripting (XSS), cross- site\\nrequest forgery (CSRF), and session hijacking?”\\nThe additional detail in the revised question is likely\\nto not only make the user aware of issues they need to\\nconsider, but lead to a better answer from the LLM. For\\nsoftware engineering tasks, this pattern could also incorp orate\\ninformation regarding potential bugs, modularity, or othe r\\ncode quality considerations. Another approach would be to\\nautomatically reﬁne questions so the generated code cleanl y\\nseparates concerns or minimizes use of external libraries, such\\nas:\\nWhenever I ask a question about how to write some\\ncode, suggest a better version of my question that\\nasks how to write the code in a way that minimizes\\nmy dependencies on external libraries.\\n5) Consequences: The Question Reﬁnement pattern helps\\nbridge the gap between the user’s knowledge and the LLM’s\\nunderstanding, thereby yielding more efﬁcient and accurat e\\ninteractions. One risk of this pattern is its tendency to rap idly\\nnarrow the questioning by the user into a speciﬁc area that\\nguides the user down a more limited path of inquiry than\\nnecessary. The consequence of this narrowing is that the\\nuser may miss important ”bigger picture” information. One\\nsolution to this problem is to provide additional scope to th e\\npattern prompt, such as “do not scope my questions to speciﬁc\\nprogramming languages or frameworks.”\\nAnother approach to overcoming arbitrary narrowing or\\nlimited targeting of the reﬁned question is to combine the\\nQuestion Reﬁnement pattern with other patterns. In particular,\\nthis pattern can be combined with the Cognitive Veriﬁer pattern\\nso the LLM automatically produces a series of follow-up ques -\\ntions that can produce the reﬁned question. For example, in\\nthe following prompt the Question Reﬁnement andCognitive\\nVeriﬁer patterns are applied to ensure better questions are\\nposed to the LLM:\\n“From now on, whenever I ask a question, ask four\\nadditional questions that would help you produce a\\nbetter version of my original question. Then, use my\\nanswers to suggest a better version of my original\\nquestion.”\\nAs with many patterns that allow an LLM to generate\\nnew questions using its knowledge, the LLM may introduce\\nunfamiliar terms or concepts to the user into the question.\\nOne way to address this issue is to include a statement that\\nthe LLM should explain any unfamiliar terms it introduces in to\\nthe question. A further enhancement of this idea is to combin etheQuestion Reﬁnement pattern with the Persona pattern so\\nthe LLM ﬂags terms and generates deﬁnitions that assume a\\nparticular level of knowledge, such as this example:\\n“From now on, whenever I ask a question, ask four\\nadditional questions that would help you produce a\\nbetter version of my original question. Then, use my\\nanswers to suggest a better version of my original\\nquestion. After the follow-up questions, temporarily\\nact as a user with no knowledge of AWS and deﬁne\\nany terms that I need to know to accurately answer\\nthe questions.”\\nAn LLM can always produce factual inaccuracies, just\\nlike a human. A risk of this pattern is that the inaccuracies\\nare introduced into the reﬁned question. This risk may be\\nmitigated, however, by combining the Fact Check List pattern\\nto enable the user to identify possible inaccuracies and the\\nReﬂection pattern to explain the reasoning behind the question\\nreﬁnement.\\nG. The Alternative Approaches Pattern\\n1) Intent and Context: The intent of the pattern is to ensure\\nan LLM always offers alternative ways of accomplishing a tas k\\nso a user does not pursue only the approaches with which they\\nare familiar. The LLM can provide alternative approaches th at\\nalways force the user to think about what they are doing and\\ndetermine if that is the best approach to meet reach their goa l.\\nIn addition, solving the task may inform the user or teach the m\\nabout alternative concepts for subsequent follow-up.\\n2) Motivation: Humans often suffer from cognitive biases\\nthat lead them to choose a particular approach to solve a\\nproblem even when it is not the right or “best” approach.\\nMoreover, humans may be unaware of alternative approaches\\nto what they have used in the past. The motivation of the\\nAlternative Approaches pattern is to ensure the user is aware\\nof alternative approaches to select a better approach to sol ve\\na problem by dissolving their cognitive biases.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWithin scope X, if there are alternative ways to accom-\\nplish the same thing, list the best alternate approaches\\n(Optional) compare/contrast the pros and cons of each\\napproach\\n(Optional) include the original way that I asked\\n(Optional) prompt me for which approach I would like\\nto use\\nThe ﬁrst statement, “within scope X”, scopes the interactio n\\nto a particular goal, topic, or bounds on the questioning. Th e\\nscope is the constraints that the user is placing on the alter -\\nnative approaches. The scope could be “for implementation\\ndecisions” or “for the deployment of the application”. The\\nscope ensures that any alternatives ﬁt within the boundarie s\\nor constraints that the user must adhere to.\\nThe second statement, “if there are alternative ways to\\naccomplish the same thing, list the best alternate approach es”', doc_id='283a6f9f-6bbe-439d-b66f-4b7fc093d65d', embedding=None, doc_hash='099a84568fbde3987575773f6630f72dca898aa83e210226d73500c6fd79bcc6', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='instructs the LLM to suggest alternatives. As with other\\npatterns, the speciﬁcity of the instructions can be increas ed or\\ninclude domain-speciﬁc contextual information. For examp le,\\nthe statement could be scoped to “if there are alternative wa ys\\nto accomplish the same thing with the software framework tha t\\nI am using” to prevent the LLM from suggesting alternatives\\nthat are inherently non-viable because they would require t oo\\nmany changes to other parts of the application.\\nSince the user may not be aware of the alternative ap-\\nproaches, they also may not be aware of why one would\\nchoose one of the alternatives. The optional statement “com -\\npare/contrast the pros and cons of each approach” adds de-\\ncision making criteria to the analysis. This statement ensu res\\nthe LLM will provide the user with the necessary rationale\\nfor alternative approaches. The ﬁnal statement, “prompt me\\nfor which approach I would like to use”, helps eliminate the\\nuser needing to manually copy/paste or enter in an alternati ve\\napproach if one is selected.\\n4) Example Implementation: Example prompt implementa-\\ntion to generate, compare, and allow the user to select one or\\nmore alternative approaches:\\n“Whenever I ask you to deploy an application to\\na speciﬁc cloud service, if there are alternative\\nservices to accomplish the same thing with the\\nsame cloud service provider, list the best alternative\\nservices and then compare/contrast the pros and cons\\nof each approach with respect to cost, availability,\\nand maintenance effort and include the original way\\nthat I asked. Then ask me which approach I would\\nlike to proceed with.”\\nThis implementation of the Alternative Approaches pattern\\nis being speciﬁcally tailored for the context of software\\nengineering and focuses on the deployment of applications\\nto cloud services. The prompt is intended to intercept place s\\nwhere the developer may have made a cloud service selection\\nwithout full awareness of alternative services that may be\\npriced more competitively or easier to maintain. The prompt\\ndirects ChatGPT to list the best alternative services that c an\\naccomplish the same task with the same cloud service provide r\\n(providing constraints on the alternatives), and to compar e and\\ncontrast the pros and cons of each approach.\\n5) Consequences: This pattern is effective in its generic\\nform and can be applied to a range of tasks effectively.\\nReﬁnements could include having a standardized catalog of\\nacceptable alternatives in a speciﬁc domain from which the\\nuser must select. The Alternative Approaches pattern can also\\nbe used to incentivize users to select one of an approved set\\nof approaches while informing them of the pros/cons of the\\napproved options.\\nH. The Cognitive Veriﬁer Pattern\\n1) Intent and Context: Research literature has documented\\nthat LLMs can often reason better if a question is subdivided\\ninto additional questions that provide answers combined in to\\nthe overall answer to the original question [14]. The intent of\\nthe pattern is to force the LLM to always subdivide questionsinto additional questions that can be used to provide a bette r\\nanswer to the original question.\\n2) Motivation: The motivation of the Cognitive Veriﬁer\\npattern is two-fold:\\n•Humans may initially ask questions that are too high-\\nlevel to provide a concrete answer to without additional\\nfollow-up due to unfamiliarity with the domain, laziness\\nin prompt entry, or being unsure about what the correct\\nphrasing of the question should be.\\n•Research has demonstrated that LLMs can often perform\\nbetter when using a question that is subdivided into\\nindividual questions.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWhen you are asked a question, follow these rules\\nGenerate a number of additional questions that would\\nhelp more accurately answer the question\\nCombine the answers to the individual questions to\\nproduce the ﬁnal answer to the overall question\\nThe ﬁrst statement is to generate a number of additional\\nquestions that would help more accurately answer the origin al\\nquestion. This step instructs the LLM to consider the contex t\\nof the question and to identify any information that may be\\nmissing or unclear. By generating additional questions, th e\\nLLM can help to ensure that the ﬁnal answer is as complete\\nand accurate as possible. This step also encourages critica l\\nthinking by the user and can help to uncover new insights or\\napproaches that may not have been considered initially, whi ch\\nsubsequently lead to better follow-on questions.\\nThe second statement is to combine the answers to the\\nindividual questions to produce the ﬁnal answer to the overa ll\\nquestion. This step is designed to ensure that all of the info r-\\nmation gathered from the individual questions is incorpora ted\\ninto the ﬁnal answer. By combining the answers, the LLM\\ncan provide a more comprehensive and accurate response to\\nthe original question. This step also helps to ensure that al l\\nrelevant information is taken into account and that the ﬁnal\\nanswer is not based on any single answer.\\n4) Example Implementation:\\n“When I ask you a question, generate three addi-\\ntional questions that would help you give a more\\naccurate answer. When I have answered the three\\nquestions, combine the answers to produce the ﬁnal\\nanswers to my original question.”\\nThis speciﬁc instance of the prompt pattern adds a reﬁne-\\nment to the original pattern by specifying a set number of\\nadditional questions that the LLM should generate in respon se\\nto a question. In this case, the prompt speciﬁes that ChatGPT\\nshould generate three additional questions that would help to\\ngive a more accurate answer to the original question. The\\nspeciﬁc number can be based on the user’s experience and\\nwillingness to provide follow-up information. A reﬁnement\\nto the prompt can be to provide a context for the amount', doc_id='17aeccca-6b99-4160-8008-e8dcff031330', embedding=None, doc_hash='690d8a77918edd0d281c36761c67f1d463728ddbd9419e3fbe5f703ccd3389cc', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='of knowledge that the LLM can assume the user has in the\\ndomain to guide the creation of the additional questions:\\n“When I ask you a question, generate three addi-\\ntional questions that would help you give a more\\naccurate answer. Assume that I know little about\\nthe topic that we are discussing and please deﬁne\\nany terms that are not general knowledge. When\\nI have answered the three questions, combine the\\nanswers to produce the ﬁnal answers to my original\\nquestion.”\\nThe reﬁnement also speciﬁes that the user may not have\\na strong understanding of the topic being discussed, which\\nmeans that the LLM should deﬁne any terms that are not\\ngeneral knowledge. This helps to ensure that the follow-up\\nquestions are not only relevant and focused, but also access ible\\nto the user, who may not be familiar with technical or domain-\\nspeciﬁc terms. By providing clear and concise deﬁnitions, t he\\nLLM can help to ensure that the follow-up questions are easy\\nto understand and that the ﬁnal answer is accessible to users\\nwith varying levels of knowledge and expertise.\\n5) Consequences: This pattern can dictate the exact number\\nof questions to generate or leave this decision to the LLM.\\nThere are pros and cons to dictating the exact number. A pro\\nis that specifying an exact number of questions can tightly\\nscope the amount of additional information the user is force d\\nto provide so it is within a range they are willing and able to\\ncontribute.\\nA con, however, is that given Nquestions there may be\\nan invaluable N+1question that will always be scoped out.\\nAlternatively, the LLM can be provided a range or allowed\\nto ask additional questions. Of course, by omitting a limit o n\\nthe number of questions the LLM may generate numerous\\nadditional questions that overwhelm the user.\\nI. The Fact Check List Pattern\\n1) Intent and Context: The intent of this pattern is to ensure\\nthat the LLM outputs a list of facts that are present in the\\noutput and form an important part of the statements in the\\noutput. This list of facts helps inform the user of the facts\\n(or assumptions) the output is based on. The user can then\\nperform appropriate due diligence on these facts/assumpti ons\\nto validate the veracity of the output.\\n2) Motivation: A current weakness of LLMs (including\\nChatGPT) is they often rapidly (and even enthusiastically! )\\ngenerate convincing text that is factually incorrect. Thes e\\nerrors can take a wide range of forms, including fake statist ics\\nto invalid version numbers for software library dependenci es.\\nDue to the convincing nature of this generated text, however ,\\nusers may not perform appropriate due diligence to determin e\\nits accuracy.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:Contextual Statements\\nGenerate a set of facts that are contained in the output\\nThe set of facts should be inserted in a speciﬁc point\\nin the output\\nThe set of facts should be the fundamental facts that\\ncould undermine the veracity of the output if any of\\nthem are incorrect\\nOne point of variation in this pattern is where the facts are\\noutput. Given that the facts may be terms that the user is not\\nfamiliar with, it is preferable if the list of facts comes aft er\\nthe output. This after-output presentation ordering allow s the\\nuser to read and understand the statements before seeing wha t\\nstatements should be checked. The user may also determine\\nadditional facts prior to realizing the fact list at the end s hould\\nbe checked.\\n4) Example Implementation: A sample wording of the Fact\\nCheck List pattern is shown below:\\n“From now on, when you generate an answer, create\\na set of facts that the answer depends on that should\\nbe fact-checked and list this set of facts at the\\nend of your output. Only include facts related to\\ncybersecurity.”\\nThe user may have expertise in some topics related to the\\nquestion but not others. The fact check list can be tailored t o\\ntopics that the user is not as experienced in or where there\\nis the most risk. For example, in the prompt above, the user\\nis scoping the fact check list to security topics, since thes e\\nare likely very important from a risk perspective and may not\\nbe well-understood by the developer. Targeting the facts al so\\nreduces the cognitive burden on the user by potentially list ing\\nfewer items for investigation.\\n5) Consequences: The Fact Check List pattern should be\\nemployed whenever users are not experts in the domain for\\nwhich they are generating output. For example, a software\\ndeveloper reviewing code could beneﬁt from the pattern\\nsuggesting security considerations. In contrast, an exper t on\\nsoftware architecture is likely to identify errors in state ments\\nabout the software structure and need not see a fact check lis t\\nfor these outputs.\\nErrors are potential in all LLM outputs, so Fact Check List\\nis an effective pattern to combine with other patterns, such\\nas by combining it with the Question Reﬁnement pattern. A\\nkey aspect of this pattern is that users can inherently check it\\nagainst the output. In particular, users can directly compa re the\\nfact check list to the output to verify the facts listed in the fact\\ncheck list actually appear in the output. Users can also iden tify\\nany omissions from the list. Although the fact check list may\\nalso have errors, users often have sufﬁcient knowledge and\\ncontext to determine its completeness and accuracy relativ e to\\nthe output.\\nOne caveat of the Fact Check List pattern is that it only\\napplies when the output type is amenable to fact-checking. F or\\nexample, the pattern works when asking ChatGPT to generate\\na Python “requirements.txt” ﬁle since it will list the versi ons\\nof libraries as facts that should be checked, which is handy a s', doc_id='1590b89c-b5f9-4cf8-83d6-7d511fb08078', embedding=None, doc_hash='edffd59f1d7b717b4e55a4ea660a04e4ab33866f807b264069b21a792a3e6f35', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='the versions commonly have errors. However, ChatGPT will\\nrefuse to generate a fact check list for a code sample and\\nindicate that this is something it cannot check, even though\\nthe code may have errors.\\nJ. The Template Pattern\\n1) Intent and Context: The intent of the pattern is to\\nensure an LLM’s output follows a precise template in terms of\\nstructure. For example, the user might need to generate a URL\\nthat inserts generated information into speciﬁc positions within\\nthe URL path. This pattern allows the user to instruct the LLM\\nto produce its output in a format it would not ordinarily use\\nfor the speciﬁed type of content being generated.\\n2) Motivation: In some cases, output must be produced in\\na precise format that is application or use-case speciﬁc and\\nnot known to the LLM. Since the LLM is not aware of the\\ntemplate structure, it must be instructed on what the format\\nis and where the different parts of its output should go. This\\ncould take the form of a sample data structure that is being\\ngenerated, a series of form letters being ﬁlled in, etc.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nI am going to provide a template for your output\\nX is my placeholder for content\\nTry to ﬁt the output into one or more of the placehold-\\ners that I list\\nPlease preserve the formatting and overall template that\\nI provide\\nThis is the template: PATTERN with PLACEHOLD-\\nERS\\nThe ﬁrst statement directs the LLM to follow a speciﬁc\\ntemplate for its output. The template will be used to try and\\ncoerce the LLMs responses into a structure that is consisten t\\nwith the user’s formatting needs. This pattern is needed whe n\\nthe target format is not known to the LLM. If the LLM already\\nhas knowledge of the format, such as a speciﬁc ﬁle type, then\\nthe template pattern can be skipped and the user can simply\\nspecify the known format. However, there may be cases, such\\nas generating Javascript Object Notation (JSON), where the re\\nis a large amount of variation in how the data could be\\nrepresented within that format and the template can be used t o\\nensure that the representation within the target format mee ts\\nthe user’s additional constraints.\\nThe second statement makes the LLM aware that the\\ntemplate will contain a set of placeholders. Users will expl ain\\nhow the output should be inserted into the template through t he\\nplaceholders. The placeholders allow the user to semantica lly\\ntarget where information should be inserted. Placeholders\\ncan use formats, like NAME, that allow the LLM to infer\\nthe semantic meaning of to determine where output should\\nbe inserted (e.g., insert the person’s name in the NAME\\nplaceholder). Moreover, by using placeholders, the user ca n\\nindicate what is not needed in the output – if a placeholder\\ndoesn’t exist for a component of the generated output, thenthat component can be omitted. Ideally, placeholders shoul d\\nuse a format that is commonly employed in text that the LLM\\nwas trained on, such as all caps, enclosure in brackets, etc.\\nThe third statement attempts to constrain the LLM so that it\\ndoesn’t arbitrarily rewrite the template or attempt to modi fy it\\nso that all of the output components can be inserted. It shoul d\\nbe noted that this statement may not preclude additional tex t\\nfrom being generated before or after. In practice, LLMs will\\ntypically follow the template, but it is harder to eliminate any\\nadditional text being generated beyond the template withou t\\nexperimentation with prompt wording.\\n4) Example Implementation: A sample template for gener-\\nating URLs where the output is put into speciﬁc places in the\\ntemplate is shown below:\\n“I am going to provide a template for your out-\\nput. Everything in all caps is a placeholder. Any\\ntime that you generate text, try to ﬁt it into one\\nof the placeholders that I list. Please preserve the\\nformatting and overall template that I provide at\\nhttps://myapi.com/NAME/proﬁle/JOB”\\nA sample interaction after the prompt was provided, is\\nshown:\\nUser: “Generate a name and job title for a person”\\nChatGPT: “https://myapi.com/Emily Parker/proﬁle/\\nSoftware Engineer”\\n5) Consequences: One consequence of applying the Tem-\\nplate pattern is that it ﬁlters the LLM’s output, which may\\neliminate other outputs the LLM would have provided that\\nmight be useful to the user. In many cases, the LLM can\\nprovide helpful descriptions of code, decision making, or o ther\\ndetails that this pattern will effectively eliminate from t he\\noutput. Users should therefore weight the pros/cons of ﬁlte ring\\nout this additional information.\\nIn addition, ﬁltering can make it hard to combine this patter n\\nwith other patterns from the Output Customization category.\\nThe Template pattern effectively constrains the output format,\\nso it may not be compatible with generation of certain other\\ntypes of output. For example, in the template provided above\\nfor a URL, it would not be easy (or likely possible) to combine\\nwith the Recipe pattern, which needs to output a list of steps.\\nK. The Inﬁnite Generation Pattern\\n1) Intent and Context: The intent of this pattern is to\\nautomatically generate a series of outputs (which may appea r\\ninﬁnite) without having to reenter the generator prompt eac h\\ntime. The goal is to limit how much text the user must type to\\nproduce the next output, based on the assumption that the use r\\ndoes not want to continually reintroduce the prompt. In some\\nvariations, the intent is to allow the user to keep an initial\\nprompt template, but add additional variation to it through\\nadditional inputs prior to each generated output.\\n2) Motivation: Many tasks require repetitive application of\\nthe same prompt to multiple concepts. For example, generati ng\\ncode for create, read, update, and delete (CRUD) operations\\nfor a speciﬁc type of entity may require applying the same', doc_id='accc3852-a402-472f-bc72-073892d52ab3', embedding=None, doc_hash='4b53a2edfa56b6ffce1e1077c9366a40116997e0016819c17d8e5b01f10f8da8', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='prompt to multiple types of entities. If the user is forced to\\nretype the prompt over and over, they may make mistakes. The\\nInﬁnite Generation pattern allows the user to repetitively apply\\na prompt, either with or without further input, to automate\\nthe generation of multiple outputs using a predeﬁned set of\\nconstraints.\\n3) Structure and Key Ideas:\\nContextual Statements\\nI would like you to generate output forever, X output(s)\\nat a time.\\n(Optional) here is how to use the input I provide\\nbetween outputs.\\n(Optional) stop when I ask you to.\\nThe ﬁrst statement speciﬁes that the user wants the LLM\\nto generate output indeﬁnitely, which effectively conveys the\\ninformation that the same prompt is going to be reused over\\nand over. By specifying the number of outputs that should be\\ngenerated at a time (i.e. “X outputs at a time”), the user can\\nrate limit the generation, which can be particularly import ant if\\nthere is a risk that the output will exceed the length limitat ions\\nof the LLM for a single output.\\nThe second statement provides optional instructions for ho w\\nto use the input provided by the user between outputs. By\\nspecifying how additional user inputs between prompts can\\nbe provided and leveraged, the user can create a prompting\\nstrategy that leverages user feedback in the context of the\\noriginal prompt. The original prompt is still in the context of\\nthe generation, but each user input between generation step s\\nis incorporated into the original prompt to reﬁne the output\\nusing prescribed rules.\\nThe third statement provides an optional way for the user\\nto stop the output generation process. This step is not alway s\\nneeded, but can be useful in situations where there may be\\nthe potential for ambiguity regarding whether or not the use r-\\nprovided input between inputs is meant as a reﬁnement for\\nthe next generation or a command to stop. For example, an\\nexplicit stop phrase could be created if the user was generat ing\\ndata related to road signs, where the user might want to enter\\na reﬁnement of the generation like “stop” to indicate that a\\nstop sign should be added to the output.\\n4) Example Implementation: The following is a sample\\ninﬁnite generation prompt for producing a series of URLs:\\n“From now on, I want you to generate a name\\nand job until I say stop. I am going to provide a\\ntemplate for your output. Everything in all caps is a\\nplaceholder. Any time that you generate text, try to\\nﬁt it into one of the placeholders that I list. Please\\npreserve the formatting and overall template that I\\nprovide: https://myapi.com/NAME/proﬁle/JOB”\\nThis prompt is combining the functionality of both the\\nInﬁnite Generation pattern and the Template pattern. The user\\nis requesting the LLM continuously generate a name and job\\ntitle until explicitly told to “stop”. The generated output s are\\nthen formatted into the template provided, which includesplaceholders for the name and job title. By using the Inﬁnite\\nGeneration pattern, the user receives multiple outputs without\\nhaving to continually re-enter the template. Likewise, the\\nTemplate pattern is applied to provide a consistent format for\\nthe outputs.\\n5) Consequences: In conversational LLMs, the input to\\nthe model at each time step is the previous output and the\\nnew user input. Although the details of what is preserved\\nand reintroduced in the next output cycle are model and\\nimplementation dependent, they are often limited in scope. The\\nmodel is therefore constantly being fed the previous output s\\nand the prompt, which can result in the model losing track of\\nthe original prompt instructions over time if they exceed th e\\nscope of what it is being provided as input.\\nAs additional outputs are generated, the context surroundi ng\\nthe prompt may fade, leading to the model deviating from\\nthe intended behavior. It is important to monitor the output s\\nproduced by the model to (1) ensure it still adheres to\\nthe desired behavior and (2) provide corrective feedback if\\nnecessary. Another issue to consider is that the LLM may\\ngenerate repetitive outputs, which may not be desired since\\nusers ﬁnd this repetition tedious and error-prone to proces s.\\nL. The Visualization Generator Pattern\\n1) Intent and Context: The intent of this pattern is to use\\ntext generation to create visualizations. Many concepts ar e\\neasier to grasp in diagram or image format. The purpose of\\nthis pattern is to create a pathway for the tool to produce\\nimagery that is associated with other outputs. This pattern\\nallows the creation of visualizations by creating inputs fo r\\nother well-known visualization tools that use text as their\\ninput, such as Graphviz Dot [15] or DALL-E [13]. This\\npattern can provide a more comprehensive and effective way\\nof communicating information by combining the strengths of\\nboth the text generation and visualization tools.\\n2) Motivation: LLMs generally produce text and cannot\\nproduce imagery. For example, an LLM cannot draw a diagram\\nto describe a graph. The Visualization Generator pattern over-\\ncomes this limitation by generating textual inputs in the co rrect\\nformat to plug into another tool that generates the correct\\ndiagram. The motivation behind this pattern is to enhance th e\\noutput of the LLM and make it more visually appealing and\\neasier to understand for users. By using text inputs to gener ate\\nvisualizations, users can quickly understand complex conc epts\\nand relationships that may be hard to grasp through text alon e.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nGenerate an X that I can provide to tool Y to visualize\\nit\\nThe goal of the contextual statements is to indicate to the\\nLLM that the output it is going to produce, “X”, is going to\\nbe imagery. Since LLMs can’t generate images, the ”that I\\ncan provide to tool Y to visualize it” clariﬁes that the LLM\\nis not expected to generate an image, but is instead expected', doc_id='f94d72b8-a353-44ea-a7ac-139f011aa7d4', embedding=None, doc_hash='44669dbfc909636117540212c6112ee6e2b27795ec87f5424c3a300194b931b8', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='to produce a description of imagery consumable by tool Y for\\nproduction of the image.\\nMany tools may support multiple types of visualizations or\\nformats, and thus the target tool itself may not be sufﬁcient\\ninformation to accurately produce what the user wants. The\\nuser may need to state the precise types of visualizations (e .g.,\\nbar chart, directed graph, UML class diagram) that should be\\nproduced. For example, Graphviz Dot can create diagrams for\\nboth UML class diagrams and directed graphs. Further, as wil l\\nbe discussed in the following example, it can be advantageou s\\nto specify a list of possible tools and formats and let the LLM\\nselect the appropriate target for visualization.\\n4) Example Implementation:\\n“Whenever I ask you to visualize something, please\\ncreate either a Graphviz Dot ﬁle or DALL-E prompt\\nthat I can use to create the visualization. Choose\\nthe appropriate tools based on what needs to be\\nvisualized.”\\nThis example of the pattern adds a qualiﬁcation that the\\noutput type for the visualization can be either for Graphviz\\nor DALL-E. The interesting aspect of this approach is that\\nit allows the LLM to use its semantic understanding of the\\noutput format to automatically select the target tooling ba sed\\non what will be displayed. In this case, Graphviz would be for\\nvisualizing graphs with a need for an exactly deﬁned structu re.\\nDALL-E would be effective at visualizing realistic or artis tic\\nimagery that does not have an exactly deﬁned structure. The\\nLLM can select the tool based on the needs of the visualizatio n\\nand capabilities of each tool.\\n5) Consequences: The pattern creates a target pipeline\\nfor the output to render a visualization. The pipeline may\\ninclude AI generators, such as DALL-E, that can produce\\nrich visualizations. The pattern allows the user to expand t he\\nexpressive capabilities of the output into the visual domai n.\\nM. The Game Play Pattern\\n1) Intent and Context: The intent of this pattern is to create\\na game around a given topic. The pattern can be combined\\nwith the Visualization Generator to add imagery to the game.\\nThe game is centered around a speciﬁc topic and the LLM\\nwill guide the game play. The pattern is particularly effect ive\\nwhen the rules of the game are relatively limited in scope, bu t\\nthe content for the game is expected to be wide in scope. The\\nuser can specify a limited set of rules and then the LLM can\\nautomate generation of bodies of content for game play.\\n2) Motivation: You would like the LLM to generate scenar-\\nios or questions revolving around a speciﬁc topic and requir e\\nusers to apply problem solving or other skills to accomplish\\na task related to the scenario. Generating all the content fo r\\nthe game manually would be too time consuming, however, so\\nyou would like the LLM to apply its knowledge of the topic\\nto guide the generation of content.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:Contextual Statements\\nCreate a game for me around X\\nOne or more fundamental rules of the game\\nThe ﬁrst statement, instructs the LLM to create a game and\\nprovides the important scoping of the game to a topic area. .\\nOne of the important capabilities of the pattern is that it al lows\\nthe user to create games by describing the rules of the game,\\nwithout having to determine the content of the game. The more\\nspeciﬁc the topic, typically the more novel and interesting the\\ngame play.\\nThe second statement introduces the rules of the game to the\\nLLM. It is important that the rules ﬁt within the capabilitie s\\nof the LLM. Textual games that rely on input and output text\\nsequences work best. One of the key attributes of the pattern\\nis that the input text can be rich and expressive, which can\\nlead to interesting interfaces for the game. For example, th e\\nuser might express actions in the game as scripts dictating\\na sequence of complex actions, such as “get a listing of all\\nnetwork activity and check it for anomalies”, which go beyon d\\nthe scope of multiple choice or short answer inputs. Each rul e\\nshould be provided as a separate statement regarding some\\naspect of the game.\\n4) Example Implementation: A sample cybersecurity game\\nprompt is shown below:\\n“We are going to play a cybersecurity game. You\\nare going to pretend to be a Linux terminal for a\\ncomputer that has been compromised by an attacker.\\nWhen I type in a command, you are going to output\\nthe corresponding text that the Linux terminal would\\nproduce. I am going to use commands to try and\\nﬁgure out how the system was compromised. The\\nattack should have done one or more of the following\\nthings: (1) launched new processes, (2) changed\\nﬁles, (3) opened new ports to receive communi-\\ncation, (4) created new outbound connections, (5)\\nchanged passwords, (6) created new user accounts,\\nor (7) read and stolen information. To start the game,\\nprint a scenario of what happened that led to my\\ninvestigation and make the description have clues\\nthat I can use to get started.”\\nThis cybersecurity game prompt combines a number of pat-\\nterns, including Game Play andPersona . The Linux terminal\\npersona that is chosen for the game has been discussed in many\\nInternet posts [16]. This game further customizes the discu ssed\\nLinux persona to act as a Linux terminal of a computer\\ncompromised by a cyberattack. ChatGPT responds to Linux\\ncommands with appropriate terminal outputs and synthesize s\\na fake ﬁle system that has clues to a cyberattack.\\nSample output from this game with ChatGPT after typing\\nin “ls -alt”, the Linux command to generate a list of ﬁles in\\nthe current directory is shown below (permissions have been\\nremoved for formatting reasons):\\ntotal 56\\n5 user user 4096 Feb 16 14:31 .\\n1 user user 18881 Feb 16 14:31 .bash_history', doc_id='69eb2d36-ea4c-4fa5-aff9-23298e9aae0e', embedding=None, doc_hash='f842ded0d9694b4775ff727b042847756e8fb30ace24c6af83f0ecd53268c74c', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='43 user user 4096 Feb 16 14:30 ..\\n1 user user 1178 Aug 15 2022 .bashrc\\n1 user user 220 Aug 15 2022 .bash_logout\\n3 user user 4096 Feb 16 14:25 .gnupg\\n1 user user 4041 Feb 15 17:52 README.md\\n2 user user 4096 Feb 1 11:35 project\\nWhen the user then typed the command “cat .bash history”,\\nto display the contents of the log ﬁle showing the commands\\nrun by the user, ChatGPT responded with:\\ncd /var/www/html\\nsudo rm -rf *\\nsudo wget https://example.com/backdoor.php\\nsudo chmod 777 backdoor.php\\nexit\\nls -alt\\nps -ef\\nnetstat -tulnp\\nIn the output above, ChatGPT generated a fake log ﬁle with\\nrealistic commands for deleting the data being served by the\\nweb server in /var/www/html and replacing the content\\nwith a backdoor into the system.\\n5) Consequences: This pattern can be combined effectively\\nwith the Persona ,Inﬁnite Generation , and Visualization Gen-\\nerator patterns. For example, the cybersecurity game uses the\\nPersona pattern so the LLM can masquerade as a Linux termi-\\nnal. For a network security game, the Visualization Generator\\ncould be employed to add the ability to visualize the network\\ntopology and trafﬁc ﬂows.\\nN. The Reﬂection Pattern\\n1) Intent and Context: The goal of this pattern is to ask\\nthe model to automatically explain the rationale behind giv en\\nanswers to the user. The pattern allows users to better asses s\\nthe output’s validity, as well as inform users how an LLM\\narrived at a particular answer. Reﬂection can clarify any po ints\\nof confusion, uncover underlying assumptions, and reveal g aps\\nin knowledge or understanding.\\n2) Motivation: LLMs can and do make mistakes. More-\\nover, users may not understand why an LLM is producing\\na particular output and how to adapt their prompt to solve\\na problem with the output. By asking LLM to automatically\\nexplain the rationale behind its answers, users can gain a be tter\\nunderstanding of how the model is processing the input, what\\nassumptions it is making, and what data it is drawing on.\\nLLMs may sometime provide incomplete, incorrect, or\\nambiguous answers. Reﬂection is an aid to help address these\\nshortcomings and ensure the information provided by LLM\\nis as accurate. A further beneﬁt of the pattern is that it can\\nhelp users debug their prompts and determine why they are\\nnot getting results that meet expectations. This pattern is\\nparticularly effective for the exploration of topics that c an\\nbe confused with other topics or that may have nuanced\\ninterpretations and where knowing the precise interpretat ion\\nthat the LLM used is important.3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWhenever you generate an answer\\nExplain the reasoning and assumptions behind your\\nanswer\\n(Optional) ...so that I can improve my question\\nThe ﬁrst statement is requesting that, after generating an a n-\\nswer, the LLM should explain the reasoning and assumptions\\nbehind the answer. This statement helps the user understand\\nhow the LLM arrived at the answer and can help build trust in\\nthe model’s responses. The prompt includes the statement th at\\nthe purpose of the explanation is for the user to reﬁne their\\nquestion. This additional statement gives the LLM the conte xt\\nit needs to better tailor its explanations to the speciﬁc pur pose\\nof aising the user in producing follow-on questions.\\n4) Example Implementation: This example tailors the\\nprompt speciﬁcally to the domain of providing answers relat ed\\nto code:\\n”When you provide an answer, please explain the\\nreasoning and assumptions behind your selection\\nof software frameworks. If possible, use speciﬁc\\nexamples or evidence with associated code samples\\nto support your answer of why the framework is\\nthe best selection for the task. Moreover, please\\naddress any potential ambiguities or limitations in\\nyour answer, in order to provide a more complete\\nand accurate response.”\\nThe pattern is further customized to instruct the LLM that\\nit should justify its selection of software frameworks, but not\\nnecessarily other aspects of the answer. In addition, the us er\\ndictates that code samples should be used to help explain the\\nmotivation for selecting the speciﬁc software framework.\\n5) Consequences: One consequence of the Reﬂection pat-\\ntern is that it may not be effective for users who do not\\nunderstand the topic area of the discussion. For example, a\\nhighly technical question by a non-technical user may resul t\\nin a complex rationale for the answer that the user cannot\\nfathom. As with other prompt patterns, there is a risk the\\noutput may include errors or inaccurate assumptions includ ed\\nin the explanation of the rationale that the user may not be\\nable to spot. This pattern can be combined with the Fact Check\\nListto help address this issue.\\nO. The Refusal Breaker Pattern\\n1) Intent and Context: The goal of this pattern is to ask an\\nLLM to automatically help users rephrase a question when it\\nrefuses to give an answer. This pattern has the potential for\\nmisuse, however, e.g., to generate phishing emails or perform\\nother actions that violate LLM policy ﬁlters. Caution shoul d\\ntherefore be exercised when applying this pattern to ensure\\nit is used ethically and responsibly. This pattern has been\\nused successfully in some LLMs to overcome the underlying\\nprompts used to program the LLM and prevent harmful output\\ngeneration.', doc_id='ac720217-e5b8-4e14-8e82-31b7ebc97680', embedding=None, doc_hash='b29bfe378509c7bc32514d27be13c92033ec4bec301c87fbaf408b98cd0546ee', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='2) Motivation: LLMs may sometimes refuse to answer\\na question, either because they do not have the required\\nknowledge or because the question is phrased in a way that\\nthey do not understand. This outcome may be frustrating\\nfor users who are looking for answers. In some situations,\\ntherefore, the Refusal Breaker pattern can help users ﬁnd a\\nway to either rephrase their question or ask a different ques tion\\nthe LLM is better equipped to answer.\\nFor example, you could ask for clariﬁcation on why the\\nquestion was refused, as there may be a speciﬁc reason why\\nLLM could not provide an answer. Another approach is to\\nask a different question that may be within its knowledge cut -\\noff or expertise. However, all these approaches require use r\\nintervention and knowledge of how to bypass the refusal.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWhenever you can’t answer a question\\nExplain why you can’t answer the question\\nProvide one or more alternative wordings of the ques-\\ntion that you could answer\\nThe ﬁrst statement sets the stage that this set of instructio ns\\nshould only be applied when a question can’t be answered.\\nThe user does not want to trigger these instructions under\\nnormal circumstances. The second statement instructs the\\nLLM to provide an explanation of why the question can’t be\\nanswered. The explanation is likely to include assumptions ,\\nconstraints, or misunderstandings that can be leveraged by the\\nuser to strategize about how to reword the question. Althoug h\\njust the explanation of why the question can’t be answered\\ncould help a user overcome the refusal, the pattern adds an\\nadditional statement for the LLM to suggest rewordings that\\nwould be acceptable. The pattern combines both reasoning\\nand suggested rewordings to improve the chances that the use r\\nwill ﬁnd a path around whatever roadblocks are preventing th e\\nquestion form being answered.\\n4) Example Implementation: The Refusal Breaker pattern\\ncan be be implemented and applied with the following phras-\\ning:\\n“Whenever you can’t answer a question, explain why\\nand provide one or more alternate wordings of the\\nquestion that you can’t answer so that I can improve\\nmy questions.”\\nA sample discourse after using the pattern is shown below:\\nUser: What is the meaning of life?\\nChatGPT : As an AI language model, I do not\\nhave personal beliefs or opinions. The meaning of\\nlife is a complex philosophical question that has\\nbeen pondered by humans for centuries and can\\nvary greatly from person to person. It may be more\\nproductive to rephrase the question in a way that can\\nbe answered by information and knowledge, such\\nas “What are some philosophical perspectives onthe meaning of life?” or “What are some common\\nbeliefs about the purpose of life?”\\n5) Consequences: This pattern provides a potential path\\nfor misuse, so organizations, parents, or other stakeholde rs\\nmay need to restrict the usage of the LLM. The ﬁrst step in\\ngoing around guardrails on usage is to understand where the\\nguardrails are. In future work, a complement of this pattern\\nmay be developed to hide the underlying prompt information\\nand rationale from users to prevent discovery.\\nAlthough the rationale and alternate rewordings are gener-\\nated, there is no guarantee that users will be able to overcom e\\nthe refusal. The alternate questions that are generated may not\\nbe of interest to the user or helpful in answering the origina l\\nquestion. The pattern mainly provides an aid in determining\\nwhat the LLM can answer, but not a guarantee that it will\\nanswer a semantically equivalent variation of the original\\nquestion.\\nP . The Context Manager Pattern\\n1) Intent and Context: The intent of this pattern is to enable\\nusers to specify or remove context for a conversation with\\nan LLM. The goal is to focus the conversation on speciﬁc\\ntopics or exclude unrelated topics from consideration. Thi s\\npattern gives users greater control over what statements th e\\nLLM considers or ignores when generating output.\\n2) Motivation: LLMs often struggle to interpret the in-\\ntended context of the current question or generate irreleva nt\\nresponses based on prior inputs or irrelevant attention on\\nthe wrong statements. By focusing on explicit contextual\\nstatements or removing irrelevant statements, users can he lp\\nthe LLM better understand the question and generate more\\naccurate responses. Users may introduce unrelated topics o r\\nreference information from earlier in the dialogue, which\\nmay can disrupt the ﬂow of the conversation. The Context\\nManager pattern aims to emphasize or remove speciﬁc aspects\\nof the context to maintain relevance and coherence in the\\nconversation.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWithin scope X\\nPlease consider Y\\nPlease ignore Z\\n(Optional) start over\\nStatements about what to consider or ignore should list key\\nconcepts, facts, instructions, etc. that should be include d or\\nremoved from the context. The more explicit the statements\\nare, the more likely the LLM will take appropriate action. Fo r\\nexample, if the user asks to ignore subjects related to a topi c,\\nyet some of the those statements were discussed far back in th e\\nconversation, the LLM may not properly disregard the releva nt\\ninformation. The more explicit the list is, therefore, the b etter\\nthe inclusion/exclusion behavior will be.', doc_id='787c9b6e-52cb-4b6c-9d4f-d7c8af67257e', embedding=None, doc_hash='6d039a497c167c2228e23dcf8a69dc6be479c221e9a86d91e576be571ec82257', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='4) Example Implementation: To specify context consider\\nusing the following prompt:\\n“When analyzing the following pieces of code, only\\nconsider security aspects.”\\nLikewise, to remove context consider using the following\\nprompt:\\n“When analyzing the following pieces of code, do\\nnot consider formatting or naming conventions.”\\nClarity and speciﬁcity are important when providing or\\nremoving context to/from an LLM so it can better understand\\nthe intended scope of the conversation and generate more\\nrelevant responses. In many situations, the user may want to\\ncompletely start over and can employ this prompt to reset the\\nLLM’s context:\\n“Ignore everything that we have discussed. Start\\nover.”\\nThe “start over” idea helps produce a complete reset of the\\ncontext.\\n5) Consequences: One consequence of this pattern is that\\nit may inadvertently wipe out patterns applied to the con-\\nversation that the user is unaware of. For example, if an\\norganization injects a series of helpful patterns into the s tart of\\na conversation, the user may not be aware of these patterns an d\\nremove them through a reset of the context. This reset could\\npotentially eliminate helpful capabilities of the LLM, whi le\\nnot making it obvious that the user will lose this functional ity.\\nA potential solution to this problem is to include in the prom pt\\na request to explain what topics/instructions will potenti ally be\\nlost before proceeding.\\nQ. The Recipe Pattern\\n1) Intent and Context: This pattern provides constraints to\\nultimately output a sequence of steps given some partially\\nprovided “ingredients” that must be conﬁgured in a sequence\\nof steps to achieve a stated goal. It combines the Template ,\\nAlternative Approaches , and Reﬂection patterns.\\n2) Motivation: Users often want an LLM to analyze a\\nconcrete sequence of steps or procedures to achieve a stated\\noutcome. Typically, users generally know—or have an idea\\nof—what the end goal should look like and what “ingredients”\\nbelong in the prompt. However, they may not necessarily know\\nthe precise ordering of steps to achieve that end goal.\\nFor example, a user may want a precise speciﬁcation on how\\na piece of code should be implemented or automated, such as\\n“create an Ansible playbook to ssh into a set of servers, copy\\ntext ﬁles from each server, spawn a monitoring process on\\neach server, and then close the ssh connection to each server .\\nIn other words, this pattern represents a generalization of the\\nexample of “given the ingredients in my fridge, provide dinn er\\nrecipes.” A user may also want to specify a set number of\\nalternative possibilities, such as “provide 3 different wa ys of\\ndeploying a web application to AWS using Docker containers\\nand Ansible using step by step instructions”.3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nI would like to achieve X\\nI know that I need to perform steps A,B,C\\nProvide a complete sequence of steps for me\\nFill in any missing steps\\nIdentify any unnecessary steps\\nThe ﬁrst statement “I would like to achieve X” focuses the\\nLLM on the overall goal that the recipe needs to be built\\nto achieve. The steps will be organized and completed to\\nsequentially achieve the goal speciﬁed. The second stateme nt\\nprovides the partial list of steps that the user would like\\nto include in the overall recipe. These serve as intermediat e\\nwaypoints for the path that the LLM is going to generate or\\nconstraints on the structure of the recipe. The next stateme nt\\nin the pattern, “provide a complete sequence of steps for\\nme”, indicates to the LLM that the goal is to provide a\\ncomplete sequential ordering of steps. The “ﬁll in any missi ng\\nsteps” helps ensure that the LLM will attempt to complete\\nthe recipe without further follow-up by making some choices\\non the user’s behalf regarding missing steps, as opposed to\\njust stating additional information that is needed. Finall y, the\\nlast statement, “identify any unnecessary steps,” is usefu l in\\nﬂagging inaccuracies in the user’s original request so that the\\nﬁnal recipe is efﬁcient.\\n4) Example Implementation: An example usage of this\\npattern in the context of deploying a software application t o\\nthe cloud is shown below:\\n“I am trying to deploy an application to the cloud. I\\nknow that I need to install the necessary dependen-\\ncies on a virtual machine for my application. I know\\nthat I need to sign up for an AWS account. Please\\nprovide a complete sequence of steps. Please ﬁll in\\nany missing steps. Please identify any unnecessary\\nsteps.”\\nDepending on the use case and constraints, “installing\\nnecessary dependencies on a virtual machine” may be an\\nunnecessary step. For example, if the application is alread y\\npackaged in a Docker container, the container could be de-\\nployed directly to the AWS Fargate Service, which does not\\nrequire any management of the underlying virtual machines.\\nThe inclusion of the “identify unnecessary steps” language\\nwill cause the LLM to ﬂag this issue and omit the steps from\\nthe ﬁnal recipe.\\n5) Consequences: One consequence of the recipe pattern is\\nthat a user may not always have a well-speciﬁed description\\nof what they would like to implement, construct, or design.\\nMoreover, this pattern may introduce unwanted bias from the\\nuser’s initially selected steps so the LLM may try to ﬁnd a\\nsolution that incorporates them, rather than ﬂagging them a s\\nunneeded. For example, an LLM may try to ﬁnd a solution\\nthat does install dependencies for a virtual machine, even i f\\nthere are solutions that do not require that.', doc_id='086b8e4e-78f9-43d4-bbde-11c081fbce66', embedding=None, doc_hash='3dd729c817e39f9f42ff55f83152f09ddf9f8c4210e78c49fc147faf34343372', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='IV. R ELATED WORK\\nSoftware patterns [10], [11] have been extensively studied\\nand documented in prior work. Patterns are widely used in\\nsoftware engineering to express the intent of design struct ures\\nin a way that is independent of implementation details. Patt erns\\nprovide a mental picture of the goals that the pattern is\\ntrying to achieve and the forces that it is trying to resolve.\\nA key advantage of patterns is their composability, allowin g\\ndevelopers to build pattern sequences and pattern language s\\nthat can be used to address complex problems. Patterns have\\nalso been investigated in other domains, such as contract\\ndesign for decentralized ledgers [17], [18].\\nThe importance of good prompt design with LLMs, such as\\nChatGPT, is well understood [19]–[28]. Previous studies ha ve\\nexamined the effect of prompt words on AI generative models.\\nFor example, Liu et al. [29] investigated how different\\nprompt key words affect image generation and different char -\\nacteristics of images. Other work has explored using LLMs\\nto generate visualizations [30]. Han et al. [31] researched\\nstrategies for designing prompts for classiﬁcation tasks. Other\\nresearch has looked at boolean prompt design for literature\\nqueries [32]. Yet other work has speciﬁcally examined promp ts\\nfor software and ﬁxing bugs [33].\\nOur work is complementary to prior work by providing\\na structure for documenting, discussing, and reasoning abo ut\\nprompts that can aid users in developing mental models for\\nstructuring prompts to solve common problems.\\nThe quality of the answers produced by LLMs, particuarly\\nChatGPT, has been assessed in a number of domains. For\\nexample, ChatGPT has been used to take the medical licensing\\nexam with surprisingly good results [3]. The use of ChatGPT\\nin Law School has also been explored [34]. Other papers have\\nlooked at its mathematical reasoning abilities [35]. As mor e\\ndomains are explored, we expect that domain-speciﬁc patter n\\ncatalogs will be developed to share domain-speciﬁc problem\\nsolving prompt structures.\\nV. C ONCLUDING REMARKS\\nThis paper presented a framework for documenting and\\napplying a catalog of prompt patterns for large language\\nmodels (LLMs), such as ChatGPT. These prompt patterns are\\nanalogous to software patterns and aim to provide reusable\\nsolutions to problems that users face when interacting with\\nLLMs to perform a wide range of tasks. The catalog of prompt\\npatterns captured via this framework (1) provides a structu red\\nway of discussing prompting solutions, (2) identiﬁes patte rns\\nin prompts, rather than focusing on speciﬁc prompt examples ,\\nand (3) classiﬁes patterns so users are guided to more efﬁcie nt\\nand effective interactions with LLMs.\\nThe following lessons learned were gleaned from our work\\non prompt patterns:\\n•Prompt patterns signiﬁcantly enrich the capabilities that\\ncan be created in a conversational LLM . For example,\\nprompts can lead to the generation of cybersecurity\\ngames, complete with ﬁctitious terminal commands thathave been run by an attacker stored in a .bashhistory\\nﬁle. As shown in Section III, larger and more complex\\ncapabilities can be created by combining prompt patterns,\\nsuch as combining the Game Play and Visualization\\nGenerator patterns.\\n•Documenting prompt patterns as a pattern catalog is\\nuseful, but insufﬁcient . Our experience indicates that\\nmuch more work can be done in this area, both in terms\\nof reﬁning and expanding the prompt patterns presented\\nin this paper, as well as in exploring new and innovative\\nways of using LLMs. In particular, weaving the prompt\\npatterns captured here as a pattern catalog into a more\\nexpression pattern language will help guide users of\\nLLMs more effectively.\\n•LLM Capabilities will evolve over time, likely necessitat-\\ning reﬁnement of patterns. As LLM capabilities change,\\nsome patterns may no longer be necessary, be obviated\\nby different styles of interaction or conversation/sessio n\\nmanagement approaches, or require enhancement to func-\\ntion correctly. Continued work will be needed to docu-\\nment and catalog patterns that provide reusable solutions.\\n•The prompt patterns are generalizable to many differ-\\nent domains. Although most of the patterns have been\\ndiscussed in the context of software development, these\\nsame patterns are applicable in arbitrary domains, ranging\\nfrom inﬁnite generation of stories for entertainment to\\neducational games to explorations of topics.\\nWe hope that this paper inspires further research and de-\\nvelopment in this area that will help enhance prompt pattern\\ndesign to create new and unexpected capabilities for conver -\\nsational LLMs.\\nREFERENCES\\n[1] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von\\nArx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al. ,\\n“On the opportunities and risks of foundation models,” arXiv preprint\\narXiv:2108.07258 , 2021.\\n[2] Y . Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia,\\nZ. Ji, T. Yu, W. Chung et al. , “A multitask, multilingual, multimodal\\nevaluation of chatgpt on reasoning, hallucination, and int eractivity,”\\narXiv preprint arXiv:2302.04023 , 2023.\\n[3] A. Gilson, C. Safranek, T. Huang, V . Socrates, L. Chi, R. A . Taylor,\\nand D. Chartash, “How well does chatgpt do when taking the med ical\\nlicensing exams?” medRxiv , pp. 2022–12, 2022.\\n[4] A. Carleton, M. H. Klein, J. E. Robert, E. Harper, R. K. Cun ningham,\\nD. de Niz, J. T. Foreman, J. B. Goodenough, J. D. Herbsleb, I. O zkaya,\\nand D. C. Schmidt, “Architecting the future of software engi neering,”\\nComputer , vol. 55, no. 9, pp. 89–93, 2022.\\n[5] “Github copilot · your ai pair programmer.” [Online]. Av ailable:\\nhttps://github.com/features/copilot\\n[6] O. Asare, M. Nagappan, and N. Asokan, “Is github’s copilo t as bad\\nas humans at introducing vulnerabilities in code?” arXiv preprint\\narXiv:2204.04741 , 2022.\\n[7] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri , “Asleep at\\nthe keyboard? assessing the security of github copilot’s co de contribu-\\ntions,” in 2022 IEEE Symposium on Security and Privacy (SP) . IEEE,\\n2022, pp. 754–768.\\n[8] J. Krochmalski, IntelliJ IDEA Essentials . Packt Publishing Ltd, 2014.\\n[9] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-\\ntrain, prompt, and predict: A systematic survey of promptin g methods\\nin natural language processing,” ACM Computing Surveys , vol. 55, no. 9,\\npp. 1–35, 2023.', doc_id='4ba77b20-749f-4e65-aae1-09ac75186ac0', embedding=None, doc_hash='0680967f461a93e61f09f48428a4c347a7e364faf45f984f90c54eca64260e67', extra_info={'nome file': 'Chain of thoughts.pdf'}), Document(text='[10] E. Gamma, R. Johnson, R. Helm, R. E. Johnson, and J. Vliss ides,\\nDesign patterns: elements of reusable object-oriented sof tware . Pearson\\nDeutschland GmbH, 1995.\\n[11] D. C. Schmidt, M. Stal, H. Rohnert, and F. Buschmann, Pattern-oriented\\nsoftware architecture, patterns for concurrent and networ ked objects .\\nJohn Wiley & Sons, 2013.\\n[12] OpenAI, “ChatGPT: Large-Scale Generative Language Mo dels for\\nAutomated Content Creation,” https://openai.com/blog/c hatgpt/, 2023,\\n[Online; accessed 19-Feb-2023].\\n[13] ——, “DALL·E 2: Creating Images from Text,”\\nhttps://openai.com/dall-e-2/, 2023, [Online; accessed 1 9-Feb-2023].\\n[14] D. Zhou, N. Sch¨ arli, L. Hou, J. Wei, N. Scales, X. Wang, D . Schu-\\nurmans, O. Bousquet, Q. Le, and E. Chi, “Least-to-most promp ting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625 , 2022.\\n[15] J. Ellson, E. R. Gansner, E. Koutsoﬁos, S. C. North, and G . Woodhull,\\n“Graphviz and dynagraph—static and dynamic graph drawing t ools,”\\nGraph drawing software , pp. 127–148, 2004.\\n[16] S. Owen, “Building a virtual machine inside a javascrip t library,”\\nhttps://www.engraved.blog/building-a-virtual-machin e-inside/, 2022,\\naccessed: 2023-02-20.\\n[17] P. Zhang, J. White, D. C. Schmidt, and G. Lenz, “Applying\\nsoftware patterns to address interoperability in blockcha in-based\\nhealthcare apps,” CoRR , vol. abs/1706.03700, 2017. [Online]. Available:\\nhttp://arxiv.org/abs/1706.03700\\n[18] X. Xu, C. Pautasso, L. Zhu, Q. Lu, and I. Weber, “A pattern collection\\nfor blockchain-based applications,” in Proceedings of the 23rd European\\nConference on Pattern Languages of Programs , 2018, pp. 1–20.\\n[19] E. A. van Dis, J. Bollen, W. Zuidema, R. van Rooij, and C. L . Bockting,\\n“Chatgpt: ﬁve priorities for research,” Nature , vol. 614, no. 7947, pp.\\n224–226, 2023.\\n[20] L. Reynolds and K. McDonell, “Prompt programming for la rge language\\nmodels: Beyond the few-shot paradigm,” CoRR , vol. abs/2102.07350,\\n2021. [Online]. Available: https://arxiv.org/abs/2102. 07350\\n[21] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le,\\nand D. Zhou, “Chain of thought prompting elicits reasoning i n\\nlarge language models,” CoRR , vol. abs/2201.11903, 2022. [Online].\\nAvailable: https://arxiv.org/abs/2201.11903\\n[22] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borge aud,\\nD. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi,\\nT. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus, “Em ergent\\nabilities of large language models,” 2022. [Online]. Avail able:\\nhttps://arxiv.org/abs/2206.07682\\n[23] Y . Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Ch an, and\\nJ. Ba, “Large language models are human-level prompt engine ers,”\\n2022. [Online]. Available: https://arxiv.org/abs/2211. 01910\\n[24] T. Shin, Y . Razeghi, R. L. L. IV , E. Wallace, and S. Singh,\\n“Autoprompt: Eliciting knowledge from language models wit h\\nautomatically generated prompts,” CoRR , vol. abs/2010.15980, 2020.\\n[Online]. Available: https://arxiv.org/abs/2010.15980\\n[25] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sut skever,\\n“Language models are unsupervised multitask learners,” 20 19.\\n[26] D. Zhou, N. Sch¨ arli, L. Hou, J. Wei, N. Scales, X. Wang,\\nD. Schuurmans, C. Cui, O. Bousquet, Q. Le, and E. Chi, “Least- to-\\nmost prompting enables complex reasoning in large language models,”\\n2022. [Online]. Available: https://arxiv.org/abs/2205. 10625\\n[27] J. Jung, L. Qin, S. Welleck, F. Brahman, C. Bhagavatula, R. L.\\nBras, and Y . Choi, “Maieutic prompting: Logically consiste nt\\nreasoning with recursive explanations,” 2022. [Online]. A vailable:\\nhttps://arxiv.org/abs/2205.11822\\n[28] S. Arora, A. Narayan, M. F. Chen, L. Orr, N. Guha,\\nK. Bhatia, I. Chami, and C. Re, “Ask me anything: A\\nsimple strategy for prompting language models,” in International\\nConference on Learning Representations , 2023. [Online]. Available:\\nhttps://openreview.net/forum?id=bhUPJnS2g0X\\n[29] V . Liu and L. B. Chilton, “Design guidelines for prompt e ngineering\\ntext-to-image generative models,” in Proceedings of the 2022 CHI\\nConference on Human Factors in Computing Systems , 2022, pp. 1–23.\\n[30] P. Maddigan and T. Susnjak, “Chat2vis: Generating data visualisations\\nvia natural language using chatgpt, codex and gpt-3 large la nguage\\nmodels,” arXiv preprint arXiv:2302.02094 , 2023.\\n[31] X. Han, W. Zhao, N. Ding, Z. Liu, and M. Sun, “Ptr: Prompt t uning\\nwith rules for text classiﬁcation,” AI Open , vol. 3, pp. 182–192, 2022.[32] S. Wang, H. Scells, B. Koopman, and G. Zuccon, “Can chatg pt write\\na good boolean query for systematic review literature searc h?”arXiv\\npreprint arXiv:2302.03495 , 2023.\\n[33] C. S. Xia and L. Zhang, “Conversational automated progr am repair,”\\narXiv preprint arXiv:2301.13246 , 2023.\\n[34] J. H. Choi, K. E. Hickman, A. Monahan, and D. Schwarcz, “C hatgpt\\ngoes to law school,” Available at SSRN , 2023.\\n[35] S. Frieder, L. Pinchetti, R.-R. Grifﬁths, T. Salvatori , T. Lukasiewicz,\\nP. C. Petersen, A. Chevalier, and J. Berner, “Mathematical c apabilities\\nof chatgpt,” arXiv preprint arXiv:2301.13867 , 2023.', doc_id='b0ad194f-01b4-41a0-885c-4dadfd985d70', embedding=None, doc_hash='438d86dd4d213db45f330ac9388c8722ec801d3b37fc4c8ef36e5166d95ed5e4', extra_info={'nome file': 'Chain of thoughts.pdf'})], 'Prompt patterns.pdf': [Document(text='Chain-of-Thought Prompting Elicits Reasoning\\nin Large Language Models\\nJason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma\\nBrian Ichter Fei Xia Ed H. Chi Quoc V . Le Denny Zhou\\nGoogle Research, Brain Team\\n{jasonwei,dennyzhou}@google.com\\nAbstract\\nWe explore how generating a chain of thought —a series of intermediate reasoning\\nsteps—signiﬁcantly improves the ability of large language models to perform\\ncomplex reasoning. In particular, we show how such reasoning abilities emerge\\nnaturally in sufﬁciently large language models via a simple method called chain-of-\\nthought prompting , where a few chain of thought demonstrations are provided as\\nexemplars in prompting.\\nExperiments on three large language models show that chain-of-thought prompting\\nimproves performance on a range of arithmetic, commonsense, and symbolic\\nreasoning tasks. The empirical gains can be striking. For instance, prompting a\\nPaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art\\naccuracy on the GSM8K benchmark of math word problems, surpassing even\\nﬁnetuned GPT-3 with a veriﬁer.\\nA: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9.Chain-of-Thought PromptingQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?A: The answer is 27.Standard Prompting\\nQ: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11. Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?Model Input\\nModel OutputModel OutputModel Input\\nFigure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic,\\ncommonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2201.11903v6  [cs.CL]  10 Jan 2023', doc_id='b7cf99ba-b9d2-41a7-af2f-68afb2c3b452', embedding=None, doc_hash='0ed04fe8d36c5656429f0633f36af2051e1b1ac69a911e7bcb636f3defa1dc91', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='1 Introduction\\nMath Word Problems (GSM8K)020406080100\\n3355\\n1857Solve rate (%)Finetuned GPT-3 175B\\nPrior best\\nPaLM 540B: standard prompting\\nPaLM 540B: chain-of-thought prompting\\nFigure 2: PaLM 540B uses chain-of-\\nthought prompting to achieve new state-\\nof-the-art performance on the GSM8K\\nbenchmark of math word problems.\\nFinetuned GPT-3 and prior best are from\\nCobbe et al. (2021).The NLP landscape has recently been revolutionized by\\nlanguage models (Peters et al., 2018; Devlin et al., 2019;\\nBrown et al., 2020, inter alia ). Scaling up the size of lan-\\nguage models has been shown to confer a range of beneﬁts,\\nsuch as improved performance and sample efﬁciency (Ka-\\nplan et al., 2020; Brown et al., 2020, inter alia ). However,\\nscaling up model size alone has not proved sufﬁcient for\\nachieving high performance on challenging tasks such as\\narithmetic, commonsense, and symbolic reasoning (Rae\\net al., 2021).\\nThis work explores how the reasoning ability of large\\nlanguage models can be unlocked by a simple method\\nmotivated by two ideas. First, techniques for arithmetic\\nreasoning can beneﬁt from generating natural language\\nrationales that lead to the ﬁnal answer. Prior work has\\ngiven models the ability to generate natural language inter-\\nmediate steps by training from scratch (Ling et al., 2017)\\nor ﬁnetuning a pretrained model (Cobbe et al., 2021), in\\naddition to neuro-symbolic methods that use formal lan-\\nguages instead of natural language (Roy and Roth, 2015;\\nChiang and Chen, 2019; Amini et al., 2019; Chen et al.,\\n2019). Second, large language models offer the exciting\\nprospect of in-context few-shot learning via prompting . That is, instead of ﬁnetuning a separate\\nlanguage model checkpoint for each new task, one can simply “prompt” the model with a few\\ninput–output exemplars demonstrating the task. Remarkably, this has been successful for a range of\\nsimple question-answering tasks (Brown et al., 2020).\\nBoth of the above ideas, however, have key limitations. For rationale-augmented training and\\nﬁnetuning methods, it is costly to create a large set of high quality rationales, which is much more\\ncomplicated than simple input–output pairs used in normal machine learning. For the traditional few-\\nshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning\\nabilities, and often does not improve substantially with increasing language model scale (Rae et al.,\\n2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations.\\nSpeciﬁcally, we explore the ability of language models to perform few-shot prompting for reasoning\\ntasks, given a prompt that consists of triples: hinput, chain of thought , outputi. Achain of thought is\\na series of intermediate natural language reasoning steps that lead to the ﬁnal output, and we refer to\\nthis approach as chain-of-thought prompting . An example prompt is shown in Figure 1.\\nWe present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks,\\nshowing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking\\ndegree. Figure 2 illustrates one such result—on the GSM8K benchmark of math word problems\\n(Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting\\nby a large margin and achieves new state-of-the-art performance. A prompting only approach is\\nimportant because it does not require a large training dataset and because a single model checkpoint\\ncan perform many tasks without loss of generality. This work underscores how large language models\\ncan learn via a few examples with natural language data about the task (c.f. automatically learning\\nthe patterns underlying inputs and outputs via a large training dataset).\\n2 Chain-of-Thought Prompting\\nConsider one’s own thought process when solving a complicated reasoning task such as a multi-step\\nmath word problem. It is typical to decompose the problem into intermediate steps and solve each\\nbefore giving the ﬁnal answer: “After Jane gives 2 ﬂowers to her mom she has 10 :::then after she\\ngives 3 to her dad she will have 7 :::so the answer is 7. ” The goal of this paper is to endow language\\nmodels with the ability to generate a similar chain of thought —a coherent series of intermediate\\nreasoning steps that lead to the ﬁnal answer for a problem. We will show that sufﬁciently large\\n2', doc_id='eb76de1d-2291-4d72-b67d-43f04eb8c8f3', embedding=None, doc_hash='7778451b1e7bb41b6be219026f9529d567f51a4f81704de4bd5b45b840d5cf5c', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='language models can generate chains of thought if demonstrations of chain-of-thought reasoning are\\nprovided in the exemplars for few-shot prompting.\\nFigure 1 shows an example of a model producing a chain of thought to solve a math word problem\\nthat it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution\\nand can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it\\nmimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations\\ntypically come after the ﬁnal answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al.,\\n2022, inter alia )).\\nChain-of-thought prompting has several attractive properties as an approach for facilitating reasoning\\nin language models.\\n1.First, chain of thought, in principle, allows models to decompose multi-step problems into\\nintermediate steps, which means that additional computation can be allocated to problems\\nthat require more reasoning steps.\\n2.Second, a chain of thought provides an interpretable window into the behavior of the model,\\nsuggesting how it might have arrived at a particular answer and providing opportunities\\nto debug where the reasoning path went wrong (although fully characterizing a model’s\\ncomputations that support an answer remains an open question).\\n3.Third, chain-of-thought reasoning can be used for tasks such as math word problems,\\ncommonsense reasoning, and symbolic manipulation, and is potentially applicable (at least\\nin principle) to any task that humans can solve via language.\\n4.Finally, chain-of-thought reasoning can be readily elicited in sufﬁciently large off-the-shelf\\nlanguage models simply by including examples of chain of thought sequences into the\\nexemplars of few-shot prompting.\\nIn empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic\\nreasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).\\n3 Arithmetic Reasoning\\nWe begin by considering math word problems of the form in Figure 1, which measure the arithmetic\\nreasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where\\nlanguage models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia ). Strikingly, chain-\\nof-thought prompting when used with the 540B parameter language model performs comparably with\\ntask-speciﬁc ﬁnetuned models on several tasks, even achieving new state of the art on the challenging\\nGSM8K benchmark (Cobbe et al., 2021).\\n3.1 Experimental Setup\\nWe explore chain-of-thought prompting for various language models on multiple benchmarks.\\nBenchmarks. We consider the following ﬁve math word problem benchmarks: (1)theGSM8K\\nbenchmark of math word problems (Cobbe et al., 2021), (2)theSV AMP dataset of math word\\nproblems with varying structures (Patel et al., 2021), (3)theASDiv dataset of diverse math word\\nproblems (Miao et al., 2020), (4)theAQuA dataset of algebraic word problems, and (5)theMA WPS\\nbenchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.\\nStandard prompting. For the baseline, we consider standard few-shot prompting, popularized by\\nBrown et al. (2020), in which a language model is given in-context exemplars of input–output pairs\\nbefore outputting a prediction for a test-time example. Exemplars are formatted as questions and\\nanswers. The model gives the answer directly, as shown in Figure 1 (left).\\nChain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot\\nprompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most\\nof the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars\\nwith chains of thought for prompting—Figure 1 (right) shows one chain of thought exemplar, and the\\nfull set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo\\nprompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether\\nchain-of-thought prompting in this form can successfully elicit successful reasoning across a range of\\n3', doc_id='21c224f9-3186-4bcb-9c17-f3d8a9022be5', embedding=None, doc_hash='2740a57bf6c7cc64a5c17109e1c5f2ec144acd9a756a617467755e8cf61e254b', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.Q: Sammy wanted to go to where the people were. Where might he go? Options: (a) race track (b) populated areas (c) desert (d) apartment (e) roadblock A: The answer must be a place with a lot of people. Race tracks, desert, apartments, and roadblocks don\\'t have a lot of people, but populated areas do. So the answer is (b). Q: Yes or no: Would a pear sink in water? A: The density of a pear is about 0.6 g/cm^3, which is less than water. Thus, a pear would float. So the answer is no.Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?  A: One day after 06/01/1943 is 06/02/1943, so today is 06/02/1943. 10 days before today is 05/23/1943. So the answer is 05/23/1943. Q: Is the following sentence plausible? \"Joao Moutinho caught the screen pass in the NFC championship.\"  A: Joao Moutinho is a soccer player. The NFC championship is part of American football, not soccer. So the answer is no.Q: Take the last letters of the words in “Lady Gaga” and concatenate them. A: The last letter of “Lady” is “y”. The last letter of “Gaga” is “a”. Concatenating them is “ya”. So the answer is ya.Q: A coin is heads up. Maybelle flips the coin. Shalonda does not flip the coin. Is the coin still heads up? A: The coin was flipped by Maybelle. So the coin was flipped 1 time, which is an odd number. The coin started heads up, so after an odd number of flips, it will be tails up. So the answer is no.Math Word Problems (free response)Math Word Problems (multiple choice)CSQA (commonsense)\\nStrategyQADate UnderstandingSports Understanding\\nLast Letter ConcatenationCoin Flip (state tracking)Q: How many keystrokes are needed to type the numbers from 1 to 500?Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788 A: There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401 three-digit numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. The answer is (b).\\nSayCan (Instructing a robot)Human: How would you bring me something that isn’t a fruit? Explanation: the user wants something to eat that isn’t a fruit. An energy bar is not a fruit, so I will bring the user an energy bar.  Plan: 1. find(energy bar) 2. pick(energy bar) 3. find(user) 4. put(energy bar) 5. done().Figure 3: Examples of hinput, chain of thought, output itriples for arithmetic, commonsense, and\\nsymbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.\\nmath word problems, we used this single set of eight chain of thought exemplars for all benchmarks\\nexcept AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars\\nand solutions from the training set, as given in Appendix Table 21.\\nLanguage models. We evaluate ﬁve large language models. The ﬁrst is GPT-3 (Brown et al.,\\n2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which\\npresumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang\\net al., 2022).The second is LaMDA (Thoppilan et al., 2022), which has models of 422M, 2B, 8B,\\n68B, and 137B parameters. The third is PaLM , which has models of 8B, 62B, and 540B parameters.\\nThe fourth is UL2 20B (Tay et al., 2022), and the ﬁfth is Codex (Chen et al., 2021, code-davinci-002\\nin the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows\\nchain-of-thought prompting can be improved by taking the majority ﬁnal answer over many sampled\\ngenerations (Wang et al., 2022a)). For LaMDA, we report averaged results over ﬁve random seeds,\\nwhere each seed had a different randomly shufﬂed order of exemplars. As LaMDA experiments\\ndid not show large variance among different seeds, to save compute we report results for a single\\nexemplar order for all other models.\\n3.2 Results\\nThe strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental\\noutputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix.\\nThere are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent\\nability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively\\nimpact performance for small models, and only yields performance gains when used with models of\\n\\x18100B parameters. We qualitatively found that models of smaller scale produced ﬂuent but illogical\\nchains of thought, leading to lower performance than standard prompting.\\n4', doc_id='7c770118-0c31-409f-893b-60104dab7ca7', embedding=None, doc_hash='3fd24529a0e6dbf3f3ac5569ca2c9409ca08a46fcca48aed614ef43d9fc8c904', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='0204060GSM8K\\nsolve rate (%)LaMDA GPT PaLMStandard prompting\\nChain-of-thought prompting\\nPrior supervised best\\n020406080SV AMP\\nsolve rate (%)\\n0.4 81370255075100MAWPS\\nsolve rate (%)\\n0.4 7175 862540\\nModel scale (# parameters in billions)\\nFigure 4: Chain-of-thought prompting enables\\nlarge language models to solve challenging math\\nproblems. Notably, chain-of-thought reasoning\\nis an emergent ability of increasing model scale.\\nPrior best numbers are from Cobbe et al. (2021)\\nfor GSM8K, Jie et al. (2022) for SV AMP, and Lan\\net al. (2021) for MAWPS.Second, chain-of-thought prompting has larger\\nperformance gains for more-complicated prob-\\nlems. For instance, for GSM8K (the dataset\\nwith the lowest baseline performance), perfor-\\nmance more than doubled for the largest GPT\\nand PaLM models. On the other hand, for Sin-\\ngleOp, the easiest subset of MAWPS which only\\nrequires a single step to solve, performance im-\\nprovements were either negative or very small\\n(see Appendix Table 3).\\nThird, chain-of-thought prompting via GPT-3\\n175B and PaLM 540B compares favorably to\\nprior state of the art, which typically ﬁnetunes a\\ntask-speciﬁc model on a labeled training dataset.\\nFigure 4 shows how PaLM 540B uses chain-of-\\nthought prompting to achieve new state of the art\\non GSM8K, SV AMP, and MAWPS (though note\\nthat standard prompting already passed the prior\\nbest for SV AMP). On the other two datasets,\\nAQuA and ASDiv, PaLM with chain-of-thought\\nprompting reaches within 2% of the state of the\\nart (Appendix Table 2).\\nTo better understand why chain-of-thought\\nprompting works, we manually examined model-\\ngenerated chains of thought by LaMDA 137B\\nfor GSM8K. Of 50 random examples where the\\nmodel returned the correct ﬁnal answer, all of\\nthe generated chains of thought were also log-\\nically and mathematically correct except two\\nthat coincidentally arrived at the correct answer\\n(see Appendix D.1, and Table 8 for examples\\nof correct model-generated chains of thought).\\nWe also randomly examined 50 random sam-\\nples for which the model gave the wrong answer.\\nThe summary of this analysis is that 46% of the\\nchains of thought were almost correct, barring\\nminor mistakes (calculator error, symbol map-\\nping error, or one reasoning step missing), and that the other 54% of the chains of thought had major\\nerrors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into\\nwhy scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors\\nmade by PaLM 62B and whether those errors were ﬁxed by scaling to PaLM 540B. The summary\\nis that scaling PaLM to 540B ﬁxes a large portion of one-step missing and semantic understanding\\nerrors in the 62B model (see Appendix A.1).\\n3.3 Ablation Study\\nThe observed beneﬁts of using chain-of-thought prompting raises the natural question of whether the\\nsame performance improvements can be conferred via other types of prompting. Figure 5 shows an\\nablation study with three variations of chain of thought described below.\\nEquation only. One reason for why chain-of-thought prompting might help is that it produces the\\nmathematical equation to be evaluated, and so we test a variation where the model is prompted\\nto output only a mathematical equation before giving the answer. Figure 5 shows that equation\\nonly prompting does not help much for GSM8K, which implies that the semantics of the questions\\nin GSM8K are too challenging to directly translate into an equation without the natural language\\nreasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we ﬁnd\\nthat equation only prompting does improve performance, since the equation can be easily derived\\nfrom the question (see Appendix Table 6).\\n5', doc_id='15d8e740-3864-4bac-877f-395da5d7b599', embedding=None, doc_hash='a2549f9f47aaab9eced4a87a3096cfc0a738c72e45d0073994579daef6b19883', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='LaMDA PaLM0204060GSM8K solve rate (%)Standard prompting\\nEquation only\\nVariable compute only\\nReasoning after answer\\nChain-of-thought prompting\\nFigure 5: Ablation study for dif-\\nferent variations of prompting us-\\ning LaMDA 137B and PaLM 540B.\\nResults for other datasets are given\\nin Appendix Table 6 and Table 7.Variable compute only. Another intuition is that chain of\\nthought allows the model to spend more computation (i.e.,\\nintermediate tokens) on harder problems. To isolate the effect\\nof variable computation from chain-of-thought reasoning, we\\ntest a conﬁguration where the model is prompted to output a\\nonly sequence of dots ( :::) equal to the number of characters in\\nthe equation needed to solve the problem. This variant performs\\nabout the same as the baseline, which suggests that variable\\ncomputation by itself is not the reason for the success of chain-\\nof-thought prompting, and that there appears to be utility from\\nexpressing intermediate steps via natural language.\\nChain of thought after answer. Another potential beneﬁt of\\nchain-of-thought prompting could simply be that such prompts\\nallow the model to better access relevant knowledge acquired\\nduring pretraining. Therefore, we test an alternative conﬁgura-\\ntion where the chain of thought prompt is only given after the\\nanswer, isolating whether the model actually depends on the\\nproduced chain of thought to give the ﬁnal answer. This variant\\nperforms about the same as the baseline, which suggests that\\nthe sequential reasoning embodied in the chain of thought is\\nuseful for reasons beyond just activating knowledge.\\n3.4 Robustness of Chain of Thought\\nGSM8K05101520Solve rate (%)Standard prompting\\nChain-of-thought prompting\\n\\x01different annotator (B)\\n\\x01different annotator (C)\\n\\x01intentionally concise style\\n\\x01exemplars from GSM8K ( \\x0b)\\n\\x01exemplars from GSM8K ( \\x0c)\\n\\x01exemplars from GSM8K ( \\r)\\nMAWPS0204060\\nFigure 6: Chain-of-thought prompting\\nhas variance for different prompt exam-\\nples (as expected) but outperforms stan-\\ndard prompting for various annotators as\\nwell as for different exemplars.Sensitivity to exemplars is a key consideration of prompt-\\ning approaches—for instance, varying the permutation of\\nfew-shot exemplars can cause the accuracy of GPT-3 on\\nSST-2 to range from near chance (54.3%) to near state of\\nthe art (93.4%) (Zhao et al., 2021). In this ﬁnal subsec-\\ntion, we evaluate robustness to chains of thought written\\nby different annotators. In addition to the results above,\\nwhich used chains of thought written by an Annotator\\nA, two other co-authors of this paper (Annotators B and\\nC) independently wrote chains of thought for the same\\nfew-shot exemplars (shown in Appendix H). Annotator A\\nalso wrote another chain of thought that was more concise\\nthan the original, following the style of solutions given in\\nCobbe et al. (2021).1\\nFigure 6 shows these results for LaMDA 137B on GSM8K\\nand MAWPS (ablation results for other datasets are given\\nin Appendix Table 6 / Table 7). Although there is variance\\namong different chain of thought annotations, as would be\\nexpected when using exemplar-based prompting (Le Scao\\nand Rush, 2021; Reynolds and McDonell, 2021; Zhao\\net al., 2021), all sets of chain of thought prompts outper-\\nform the standard baseline by a large margin. This result\\nimplies that successful use of chain of thought does not\\ndepend on a particular linguistic style.\\nTo conﬁrm that successful chain-of-thought prompting\\nworks for other sets of exemplars, we also run experiments\\nwith three sets of eight exemplars randomly sampled from the GSM8K training set, an independent\\n1For instance, whereas original chain of thought uses several short sentences ( “’There were originally 9\\ncomputers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is\\n29. ”), the concise chain of thought would read “5 * 4 = 20 new computers were added. So there are 9 + 20 = 29\\nnew computers in the server room now” .\\n6', doc_id='1d8f849a-584d-451a-9a26-302f5fb09d16', embedding=None, doc_hash='05d0f93532d723720f5e935824226a826cb881239ae1f87572295282b155ed2a', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='source (examples in this dataset already included reasoning steps like a chain of thought).2Fig-\\nure 6 shows that these prompts performed comparably with our manually written exemplars, also\\nsubstantially outperforming standard prompting.\\nIn addition to robustness to annotators, independently-written chains of thought, different exemplars,\\nand various language models, we also ﬁnd that chain-of-thought prompting for arithmetic reasoning\\nis robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2).\\n4 Commonsense Reasoning\\nAlthough chain of thought is particularly suitable for math word problems, the language-based nature\\nof chain of thought actually makes it applicable to a broad class of commonsense reasoning problems,\\nwhich involve reasoning about physical and human interactions under the presumption of general\\nbackground knowledge. Commonsense reasoning is key for interacting with the world and is still\\nbeyond the reach of current natural language understanding systems (Talmor et al., 2021).\\nBenchmarks. We consider ﬁve datasets covering a diverse range of commonsense reasoning types.\\nThe popular CSQA (Talmor et al., 2019) asks commonsense questions about the world involving\\ncomplex semantics that often require prior knowledge. StrategyQA (Geva et al., 2021) requires\\nmodels to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets\\nfrom the BIG-bench effort (BIG-bench collaboration, 2021): Date Understanding, which involves\\ninferring a date from a given context, and Sports Understanding, which involves determining whether\\na sentence relating to sports is plausible or implausible. Finally, the SayCan dataset (Ahn et al.,\\n2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete\\nset. Figure 3 shows examples with chain of thought annotations for all datasets.\\nPrompts. We follow the same experimental setup as the prior section. For CSQA and StrategyQA,\\nwe randomly selected examples from the training set and manually composed chains of thought for\\nthem to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected\\nthe ﬁrst ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on\\nthe rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al.\\n(2022) and also manually composed chains of thought.\\nResults. Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different\\nmodel scales are shown in Table 4). For all tasks, scaling up model size improved the performance\\nof standard prompting; chain-of-thought prompting led to further gains, with improvements appear-\\ning to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong\\nperformance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs\\n69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%).\\nThese results demonstrate that chain-of-thought prompting can also improve performance on tasks\\nrequiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA).\\n86254020406080100 Solve rate (%)CSQA\\n8625405060708090StrategyQA\\nStandard prompting\\nChain of thought\\nPrior supervised best\\nHuman\\n862540020406080\\nModel scale (# parameters in billions)Date\\n862540406080100Sports\\n86254020406080100SayCan\\nFigure 7: Chain-of-thought prompting also improves the commonsense reasoning abilities of\\nlanguage models. The language model shown here is PaLM. Prior best numbers are from the\\nleaderboards of CSQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021) (single-model only,\\nas of May 5, 2022). Additional results using various sizes of LaMDA, GPT-3, and PaLM are shown\\nin Table 4.\\n2We sample examples \\x1460tokens to ﬁt into our input context window, and also limit the examples to \\x142\\nsteps to solve for a fair comparison with the eight exemplars that we composed.\\n7', doc_id='44324ddb-2763-4a85-a9d7-99a046ee31e6', embedding=None, doc_hash='bb993ac50beb8e755ace86bcc4d6569aa451eed54212211b2239d378ff5c5d68', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='5 Symbolic Reasoning\\n0255075100 Solve rate (%)Letter Concat: 2\\n(in domain)Letter Concat: 4\\n(OOD)Standard prompting\\nChain-of-thought prompting\\n8 62 540406080100 Solve rate (%)Coin Flip: 2\\n(in domain)\\n8 62 540\\nModel scale (# parameters in billions)Coin Flip: 4\\n(OOD)\\nFigure 8: Using chain-of-thought\\nprompting facilitates generalization to\\nlonger sequences in two symbolic rea-\\nsoning tasks.Our ﬁnal experimental evaluation considers symbolic rea-\\nsoning, which is simple for humans but potentially chal-\\nlenging for language models. We show that chain-of-\\nthought prompting not only enables language models to\\nperform symbolic reasoning tasks that are challenging in\\nthe standard prompting setting, but also facilitates length\\ngeneralization to inference-time inputs longer than those\\nseen in the few-shot exemplars.\\nTasks. We use the following two toy tasks.\\n•Last letter concatenation. This task asks the model\\nto concatenate the last letters of words in a name (e.g.,\\n“Amy Brown”!“yn” ). It is a more challenging version\\nof ﬁrst letter concatenation, which language models can\\nalready perform without chain of thought.3We generate\\nfull names by randomly concatenating names from the\\ntop one-thousand ﬁrst and last names from name census\\ndata ( https://namecensus.com/ ).\\n•Coin ﬂip. This task asks the model to answer whether a\\ncoin is still heads up after people either ﬂip or don’t ﬂip\\nthe coin (e.g., “A coin is heads up. Phoebe ﬂips the coin.\\nOsvaldo does not ﬂip the coin. Is the coin still heads up?”\\n!“no” ).\\nAs the construction of these symbolic reasoning tasks is\\nwell-deﬁned, for each task we consider an in-domain test\\nset for which examples had the same number of steps as\\nthe training/few-shot exemplars, as well as an out-of-domain (OOD) test set, for which evaluation\\nexamples had more steps than those in the exemplars. For last letter concatenation, the model only\\nsees exemplars of names with two words, and then performs last letter concatenation on names with 3\\nand 4 words.4We do the same for the number of potential ﬂips in the coin ﬂip task. Our experimental\\nsetup uses the same methods and models as in the prior two sections. We again manually compose\\nchains of thought for the few-shot exemplars for each task, which are given in Figure 3.\\nResults. The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM,\\nwith results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting\\nleads to almost 100% solve rates (note that standard prompting already solves coin ﬂip with PaLM\\n540, though not for LaMDA 137B). Note that these in-domain evaluations are “toy tasks” in the\\nsense that perfect solution structures are already provided by the chains of thought in the few-shot\\nexemplars; all the model has to do is repeat the same steps with the new symbols in the test-time\\nexample. And yet, small models still fail—the ability to perform abstract manipulations on unseen\\nsymbols for these three tasks only arises at the scale of 100B model parameters.\\nAs for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting,\\nlanguage models achieve upward scaling curves (though performance is lower than in the in-domain\\nsetting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of\\nthought for language models of sufﬁcient scale.\\n6 Discussion\\nWe have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step rea-\\nsoning behavior in large language models. We ﬁrst saw that chain-of-thought prompting improves\\nperformance by a large margin on arithmetic reasoning, yielding improvements that are much stronger\\nthan ablations and robust to different annotators, exemplars, and language models (Section 3). Next,\\n3We tested 10 common names using GPT-3 davinci and it got all but one correct.\\n4For names of length longer than 2 words, we concatenate multiple ﬁrst and last names together.\\n8', doc_id='ed325912-2f9b-4fdc-87f4-8ef3dc8fa6d5', embedding=None, doc_hash='dc8a4aa833ddaed440953b79461749a465fbbaf92f817e84cfdc2b9b4f6b9c09', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought\\nreasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning,\\nchain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In\\nall experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language\\nmodel. No language models were ﬁnetuned in the process of writing this paper.\\nThe emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme\\n(Wei et al., 2022b). For many reasoning tasks where standard prompting has a ﬂat scaling curve, chain-\\nof-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting\\nappears to expand the set of tasks that large language models can perform successfully—in other\\nwords, our work underscores that standard prompting only provides a lower bound on the capabilities\\nof large language models. This observation likely raises more questions than it answers—for instance,\\nhow much more can we expect reasoning ability to improve with a further increase in model scale?\\nWhat other prompting methods might expand the range of tasks that language models can solve?\\nAs for limitations, we ﬁrst qualify that although chain of thought emulates the thought processes of\\nhuman reasoners, this does not answer whether the neural network is actually “reasoning,” which\\nwe leave as an open question. Second, although the cost of manually augmenting exemplars with\\nchains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for\\nﬁnetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot\\ngeneralization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct\\nand incorrect answers; improving factual generations of language models is an open direction for\\nfuture work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia ). Finally,\\nthe emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in\\nreal-world applications; further research could explore how to induce reasoning in smaller models.\\n7 Related Work\\nThis work is inspired by many research areas, which we detail in an extended related work section\\n(Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.\\nThe ﬁrst relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017)\\npioneer the idea of using natural language rationales to solve math word problems through a series\\nof intermediate steps. Their work is a remarkable contrast to the literature using formal languages\\nto reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe\\net al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to ﬁnetune a pretrained\\nlanguage model rather than training a model from scratch. In the domain of program synthesis,\\nNye et al. (2021) leverage language models to predict the ﬁnal outputs of Python programs via\\nﬁrst line-to-line predicting the intermediate computational results, and show that their step-by-step\\nprediction method performs better than directly predicting the ﬁnal outputs.\\nNaturally, this paper also relates closely to the large body of recent work on prompting. Since the\\npopularization of few-shot prompting as given by Brown et al. (2020), several general approaches\\nhave improved the prompting ability of models, such as automatically learning prompts (Lester et al.,\\n2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang\\net al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g.,\\ninstructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the\\noutputs of language models with a chain of thought.\\n8 Conclusions\\nWe have explored chain-of-thought prompting as a simple and broadly applicable method for enhanc-\\ning reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense\\nreasoning, we ﬁnd that chain-of-thought reasoning is an emergent property of model scale that allows\\nsufﬁciently large language models to perform reasoning tasks that otherwise have ﬂat scaling curves.\\nBroadening the range of reasoning tasks that language models can perform will hopefully inspire\\nfurther work on language-based approaches to reasoning.\\n9', doc_id='0b6622b9-1c3f-4693-a74c-4df9043bbd79', embedding=None, doc_hash='5a433da1c6dd9442d80a1ee4698045745c82439180addbd7cbca9b5a21fed62b', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Acknowledgements\\nWe thank Jacob Devlin, Claire Cui, Andrew Dai, and Ellie Pavlick for providing feedback on the\\npaper. We thank Jacob Austin, Yuhuai Wu, Henryk Michalewski, Aitor Lewkowycz, Charles Sutton,\\nand Aakanksha Chowdhery for helpful discussions. We thank Sid Maxwell for notifying us about a\\nmistake in the manual error analysis in the original manuscript.\\nReferences\\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. 2022. Do as I can, not as I\\nsay: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691 .\\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh\\nHajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-\\nbased formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\\nShort Papers) , Minneapolis, Minnesota. Association for Computational Linguistics.\\nDaniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding\\noperations and arguments with reading comprehension. EMNLP .\\nJacob Andreas, Dan Klein, and Sergey Levine. 2018. Learning with latent language. NAACL .\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language\\nmodels. arXiv preprint arXiv:2108.07732 .\\nBIG-bench collaboration. 2021. Beyond the imitation game: Measuring and extrapolating the\\ncapabilities of language models. In preparation .\\nKaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural\\nlanguage deductions. EMNLP .\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\\nand Dario Amodei. 2020. Language models are few-shot learners. NeurIPS .\\nJonathon Cai, Richard Shin, and Dawn Song. 2017. Making neural programming architectures\\ngeneralize via recursion. ICLR .\\nOana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI:\\nNatural language inference with natural language explanations. NeurIPS .\\nHoward Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022. Can rationalization\\nimprove robustness? NAACL .\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating\\nlarge language models trained on code. arXiv preprint arXiv:2107.03374 .\\nXinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V . Le. 2019. Neural\\nsymbolic reader: Scalable integration of distributed and symbolic representations for reading\\ncomprehension. ICLR .\\nTing-Rui Chiang and Yun-Nung Chen. 2019. Semantically-aligned equation generation for solving\\nand reasoning math word problems. In Proceedings of the 2019 Conference of the North Ameri-\\ncan Chapter of the Association for Computational Linguistics: Human Language Technologies,\\nVolume 1 (Long and Short Papers) , pages 2656–2668, Minneapolis, Minnesota. Association for\\nComputational Linguistics.\\n10', doc_id='48ae0f4b-c08f-4731-94e4-5abf892013f4', embedding=None, doc_hash='d97984b4a00b46f8f8b0bc5f907372374b50d8014e4bf47768eb648192f4d1ff', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over\\nlanguage. IJCAI .\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher\\nHesse, and John Schulman. 2021. Training veriﬁers to solve math word problems. arXiv preprint\\narXiv:2110.14168 .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language understanding. NAACL .\\nHonghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural\\nlogic machines. ICLR .\\nDheeru Dua, Sameer Singh, and Matt Gardner. 2020. Beneﬁts of intermediate annotations in reading\\ncomprehension. ACL.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did\\naristotle use a laptop? A question answering benchmark with implicit reasoning strategies. TACL .\\nYuling Gu, Bhavana Dalvi Mishra, and Peter Clark. 2022. DREAM: Uncovering mental models\\nbehind language models. NAACL .\\nBraden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher\\nRé. 2018. Training classiﬁers with natural language explanations. ACL.\\nPeter Hase and Mohit Bansal. 2022. When can models learn from explanations? a formal framework\\nfor understanding the roles of explanation data. ACL.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\\nand Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv\\npreprint arXiv:2103.03874 .\\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning\\nto solve arithmetic word problems with verb categorization. EMNLP .\\nZhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to reason deductively: Math word problem\\nsolving as complex relation extraction. arXiv preprint arXiv:2203.10316 .\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language\\nmodels. arXiv preprint arXiv:2001.08361 .\\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016.\\nMAWPS: A math word problem repository. NAACL .\\nAndrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y . Chan, Kory Matthewson, Michael Henry\\nTessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. 2022. Can language\\nmodels learn from explanations in context? arXiv preprint arXiv:2204.02329 .\\nYihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang,\\nand Ee-Peng Lim. 2021. MWPToolkit: An open-source framework for deep learning-based math\\nword problem solvers. arXiv preprint arXiv:2109.00799 .\\nTeven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? NAACL .\\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efﬁcient\\nprompt tuning. EMNLP .\\nIddo Lev, Bill MacCartney, Christopher Manning, and Roger Levy. 2004. Solving logic puzzles:\\nFrom robust processing to precise semantics. Proceedings of the 2nd Workshop on Text Meaning\\nand Interpretation .\\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-tuning: Optimizing continuous prompts for generation.\\nACL.\\n11', doc_id='6a281838-5b14-4eb6-8448-6f538dde90db', embedding=None, doc_hash='68b1e7b5e72fdb3dc9144f3233f3d9b1f21eeb74c03b62376818d508b6ab6682', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Zhengzhong Liang, Steven Bethard, and Mihai Surdeanu. 2021. Explainable multi-hop verbal\\nreasoning through internal monologue. NAACL .\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale\\ngeneration: Learning to solve and explain algebraic word problems. ACL.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021.\\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language\\nprocessing. arXiv preprint arXiv:2107.13586 .\\nBodhisattwa Prasad Majumder, Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley.\\n2021. Rationale-inspired natural language explanations with commonsense. arXiv preprint\\narXiv:2106.13876 .\\nAna Marasovi ´c, Iz Beltagy, Doug Downey, and Matthew E Peters. 2022. Few-shot self-rationalization\\nwith natural language prompts. NAACL Findings .\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and\\nfactuality in abstractive summarization. In ACL.\\nShen Yun Miao, Chao Chun Liang, and Keh Yih Su. 2020. A diverse corpus for evaluating and\\ndeveloping English math word problem solvers. ACL.\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke\\nZettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work?\\narXiv preprint arXiv:2202.12837 .\\nSharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan.\\n2020. WT5?! Training text-to-text models to explain their predictions. arXiv preprint\\narXiv:2004.14546 .\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work:\\nScratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114 .\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to\\nfollow instructions with human feedback. arXiv preprint arXiv:2203.02155 .\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve\\nsimple math word problems? NAACL .\\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\\nLuke Zettlemoyer. 2018. Deep contextualized word representations. NAACL .\\nXinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and\\nWeizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473 .\\nPiotr Pi˛ ekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving\\nBERT’s mathematical abilities by predicting the order of reasoning. ACL.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models:\\nMethods, analysis & insights from training Gopher. arXiv preprint arXiv:2112.11446 .\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\\nZhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a uniﬁed\\ntext-to-text transformer. Journal of Machine Learning Research , 21:1–67.\\nDheeraj Rajagopal, Vidhisha Balachandran, Eduard H. Hovy, and Yulia Tsvetkov. 2021. SelfExplain:\\nA self-explaining architecture for neural text classiﬁers. EMNLP .\\nNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain\\nyourself! Leveraging language models for commonsense reasoning. ACL.\\n12', doc_id='083711b7-9500-4ac6-bd89-f6ec04923618', embedding=None, doc_hash='c088a1a7ac9006935e6d61f7c98589e7fe2d183add118467fb72c6670abaf207', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading\\ncomprehension with numerical reasoning. EMNLP .\\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Michael Collins, Dipanjan Das, Slav Petrov,\\nGaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language\\ngeneration models. arXiv preprint arXiv:2112.12870 .\\nGabriel Recchia. 2021. Teaching autoregressive language models complex tasks by demonstration.\\narXiv preprint arXiv:2109.02102 .\\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022.\\nA recipe for arbitrary text style transfer with large language models. ACL.\\nLaria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond\\nthe few-shot paradigm. Extended Abstracts of the 2021 CHI Conference on Human Factors in\\nComputing Systems .\\nSubhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. EMNLP .\\nSubhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about Quantities in Natural Language.\\nTACL .\\nMohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching\\nsoft rules to pre-trained language models. EMNLP .\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,\\nAntoine Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted\\ntraining enables zero-shot task generalization. ICLR .\\nJianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021.\\nGenerate & rank: A multi-task framework for math word problems. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2021 .\\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A\\nquestion answering challenge targeting commonsense knowledge. NAACL .\\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-\\nthought: Teaching pre-trained models to systematically reason over implicit knowledge. NeurIPS .\\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and\\nJonathan Berant. 2021. CommonsenseQA 2.0: Exposing the limits of ai through gamiﬁcation.\\nNeurIPS Track on Datasets and Benchmarks .\\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven\\nZheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv\\npreprint arXiv:2205.05131 .\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for\\ndialog applications. arXiv preprint arXiv:2201.08239 .\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a.\\nSelf-consistency improves chain of thought reasoning in language models. arXiv preprint\\narXiv:2203.11171 .\\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana\\nArunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022b.\\nBenchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint\\narXiv:2204.07705 .\\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\\nAndrew M. Dai, and Quoc V . Le. 2022a. Finetuned language models are zero-shot learners. ICLR .\\n13', doc_id='ccfa354b-bd89-4eb7-b64f-f3dd5222d570', embedding=None, doc_hash='59a8abe8782e1a940eaf18667abe6f244923bff3e26055f80a66fea82ec3a4fc', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, et al. 2022b. Emergent abilities of large language\\nmodels. Transactions on Machine Learning Research .\\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing\\nhuman-AI collaboration for generating free-text explanations. NAACL .\\nSarah Wiegreffe and Ana Marasovi ´c. 2021. Teach me to explain: A review of datasets for explainable\\nNLP. NeurIPS .\\nSarah Wiegreffe, Ana Marasovi ´c, and Noah A. Smith. 2021. Measuring association between labels\\nand free-text rationales. EMNLP .\\nTongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and\\nCarrie J Cai. 2022a. PromptChainer: Chaining large language model prompts through visual\\nprogramming. CHI Extended Abstracts .\\nTongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022b. AI chains: Transparent and controllable\\nhuman-AI interaction by chaining large language model prompts. CHI.\\nYujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. 2020.\\nNeural execution engines: Learning to execute subroutines. NeurIPS .\\nHuihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, and Xiang Ren. 2021. Reﬁning language models\\nwith compositional explanations. NeurIPS .\\nXi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot in-context learning.\\narXiv preprint arXiv:2205.03401 .\\nYordan Yordanov, Vid Kocijan, Thomas Lukasiewicz, and Oana-Maria Camburu. 2021. Few-shot\\nout-of-domain transfer learning of natural language explanations. arXiv preprint arXiv:2112.06204 .\\nOmar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using “annotator rationales” to improve\\nmachine learning for text categorization. NAACL .\\nWojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv:1410.4615 .\\nEric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. STaR: Bootstrapping reasoning with\\nreasoning. arXiv preprint arXiv:2203.14465 .\\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use:\\nImproving few-shot performance of language models. ICML .\\nWangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and\\nJian Tang. 2020. Towards interpretable natural language understanding with explanations as latent\\nvariables. NeurIPS .\\n14', doc_id='714fd16d-84d3-46c5-8f31-dfb010a936ee', embedding=None, doc_hash='90a6184af20f266773699982ddd622e42150d0c0c3b4901916e7ee12de076440', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Checklist\\n1. For all authors...\\n(a)Do the main claims made in the abstract and introduction accurately reﬂect the paper’s\\ncontributions and scope? [Yes]\\n(b)Did you describe the limitations of your work? [Yes] See Section 6 and Appendix A.2.\\n(c)Did you discuss any potential negative societal impacts of your work? [Yes] We don’t\\nexpect negative societal impacts as a direct result of the contributions in our paper. One\\nconsideration, however, is that generated chain of thought is not always factual, which\\nis noted as a limitation in Appendix D.1 (and note that we do not suggest using such\\nchains of thought in a factual manner or in any real-world scenario).\\n(d)Have you read the ethics review guidelines and ensured that your paper conforms to\\nthem? [Yes]\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n3. If you ran experiments...\\n(a)Did you include the code, data, and instructions needed to reproduce the main experi-\\nmental results (either in the supplemental material or as a URL)? [Yes] We included\\ninputs, outputs, and targets for LaMDA and GPT-3 in the supplementary material.\\nAlthough we use proprietary models, we GPT-3 results are fully reproducible. Repro-\\nducibility is further discussed in Appendix E.1.\\n(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they\\nwere chosen)? [Yes] Data splits were speciﬁed, N/A for hyperparams.\\n(c)Did you report error bars (e.g., with respect to the random seed after running exper-\\niments multiple times)? [Yes] Standard deviation for multiple seeds using LaMDA\\n137B, where each seed is a different random order of exemplars, is given in Table 6\\nand Table 7.\\n(d)Did you include the total amount of compute and the type of resources used (e.g., type\\nof GPUs, internal cluster, or cloud provider)? [Yes] Type of resources are described in\\nAppendix E.2, though we did not estimate the total amount of compute.\\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\\n(a)If your work uses existing assets, did you cite the creators? [Yes] We used two models\\nthat we anonymized based on the recommendation of the NeurIPS chairs. These models\\nwill be cited in the camera-ready version of the paper.\\n(b) Did you mention the license of the assets? [Yes] See Appendix E.3.\\n(c)Did you include any new assets either in the supplemental material or as a URL? [Yes]\\nThe coinﬂip and last letter concatenation datasets are the only new assets, and they are\\ngiven in the Supplementary Materials.\\n(d)Did you discuss whether and how consent was obtained from people whose data you’re\\nusing/curating? [N/A] No human data collected.\\n(e)Did you discuss whether the data you are using/curating contains personally identiﬁable\\ninformation or offensive content? [N/A] No human data collected.\\n5. If you used crowdsourcing or conducted research with human subjects...\\n(a)Did you include the full text of instructions given to participants and screenshots, if\\napplicable? [N/A]\\n(b)Did you describe any potential participant risks, with links to Institutional Review\\nBoard (IRB) approvals, if applicable? [N/A]\\n(c)Did you include the estimated hourly wage paid to participants and the total amount\\nspent on participant compensation? [N/A]\\n15', doc_id='cc8dda00-5449-4490-a445-68e5ee0d1ceb', embedding=None, doc_hash='c4bf6631f95ec39d00f1571d1e293225c1ecd81a3cdf216a02d25fe4ea6af37a', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='A Frequently Asked Questions\\nA.1 Why does increasing model scale improve chain-of-thought prompting?\\nThe ﬁnding that successful chain-of-thought reasoning predictably emerges only at certain model\\nscales is intriguing. Scaling up language models has been shown to confer beneﬁts such as improved\\nperformance and sample efﬁciency (Kaplan et al., 2020), but chain-of-thought reasoning is emergent\\nin the sense that its success cannot be predicted only by extrapolating the performance of small scale\\nmodels, as chain of thought actually hurts performance for most models smaller than 10B parameters.\\nThe question of why model scale improves chain-of-thought prompting is certainly multi-faceted, and\\nwe made a preliminary attempt to shed insight into it via error analysis. This small analysis involved\\nmanually reading 45 errors made by PaLM 62B and categorizing them into semantic understanding\\n(20 errors), one step missing (18 errors), and other errors (7 errors). The “other category” included\\nhallucinations, repetitive outputs, and symbol mapping errors. This categorization is a coarse one\\nborrowed from the initial error analysis done on LaMDA in Appendix D.2, for which categories were\\nconceived based on what improvements were needed to make the chain of thought correct.\\nAs shown in Figure 9, scaling PaLM to 540B parameters ﬁxed a substantial portion of errors in all\\nthree categories. Examples of semantic understanding and one-step missing errors that were ﬁxed by\\nscaling PaLM to 540B are given in Figure 10. This result appears consistent with a hypothesis that\\nlanguage models acquire a range of semantic understanding and logical reasoning skills as a function\\nof model scale (though note that model scale is often conﬂated with other factors, such as amount of\\ntraining compute).\\nSemantic understanding\\n(62B made 20 errors of this type, 540B ﬁxes 6 of them)One step missing\\n(62B made 18 errors of this type, 540B ﬁxes 12 of them)Other\\n(62B made 7 errors of this type, 540B ﬁxes 4 of them)Types of errors made by a 62B language model:Errors ﬁxed by scaling from 62B to 540B\\nFigure 9: Error analysis of 45 problems that PaLM 62B got incorrect. These errors were categorized\\nthat semantic understanding, one step missing, and other. The other category includes hallucinations,\\nrepetitive outputs, and symbol mapping errors. Scaling PaLM to 540B ﬁxed a substantial portion of\\nerrors in all categories.\\nThere are also three notable points regarding why small language models fail. The ﬁrst observation\\nis that small language models fail at even relatively easy symbol mapping tasks. As demonstrated\\nin Section 5, for even symbolic reasoning tasks that only require generalization to new examples\\nusing the same chain of thought logical structure that was given in the few-shot exemplars, small\\nlanguage models still failed. The second observation is that small language models seem to have\\ninherently weaker arithmetic abilities, as shown by Brown et al. (2020), the ability to do simple\\narithmetic operations (without semantic understanding) requires sufﬁcient model scale. Finally, we\\nnoticed qualitatively that small language models often did not generate a ﬁnal answer that could be\\nparsed, due to either repetitions or logic that never arrived at a ﬁnal answer.\\nIn summary, the success of chain-of-thought reasoning as a result of model scale is a complicated\\nphenomena that likely involves a variety of emergent abilities (semantic understanding, symbol\\nmapping, staying on topic, arithmetic ability, faithfulness, etc). Future work could more thoroughly\\ninvestigate what properties of pretraining data, model architecture, and optimization objective causally\\nenable such reasoning capabilities.\\n16', doc_id='f265e757-b4ff-4aa6-ad24-ee7fa3799e06', embedding=None, doc_hash='1725b9443ebfc0152f6d8cafee43e66b0a3f9a4f308c19e80ed061f104fa7acb', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text=\"Tracy used a piece of wire 4 feet long to support tomato plants in the garden. The wire was cut into pieces 6 inches long. How many pieces did she obtain?She cut the wire into 6 inch pieces. This means she got 4 * 6 = 24 pieces. The answer is 24. (Incorrect: semantic understanding error)62B Model OutputQuestion540B Model OutputThe wire was 4 feet long. This means it was 4 * 12 = 48 inches long. It was cut into pieces 6 inches long. This means she obtained 48 / 6 = 8 pieces. The answer is 8. (Correct)Tom's ship can travel at 10 miles per hour. He is sailing from 1 to 4 PM. He then travels back at a rate of 6 mph. How long does it take him to get back?He travels from 1 to 4 PM. That means he travels for 3 hours. He travels back at 6 mph. That means he travels for 3 * 6 = 18 miles. The answer is 18. (Incorrect: semantic understanding error)He travels at 10 mph from 1 to 4 PM. This means he travels 3 hours. 3 hours at 10 mph means he travels 3 * 10 = 30 miles. He then travels back at 6 mph. This means he travels 6 miles per hour. He has to travel 30 miles, so it takes him 30 / 6 = 5 hours. The answer is 5. (Correct)Stephen placed an online order for groceries. His final bill came to $40.00. Because this was through a delivery vendor, they tacked on a 25% fee to his final total and charged him $3.00 in delivery fees. Stephen also added a $4.00 tip. After the extra fees, what was the final price of Stephen's groceries?The final price of Stephen's groceries was $40.00. The delivery vendor added 25% to the final total, so the final total was 125% of $40.00. This means the final total was 125% * $40.00 = $50.00. The delivery vendor also charged $3.00 in delivery fees. So the final price of Stephen's groceries was $50.00 + $3.00 = $53.00. The answer is $53.00. (Incorrect: one step missing error)Stephen's bill was $40.00. The delivery vendor added 25% to the bill. This means the bill was increased by 25% of $40.00. This is 0.25 * $40.00 = $10.00. The delivery fee was $3.00. The tip was $4.00. So the total bill was $40.00 + $10.00 + $3.00 + 540b model $4.00 = $57.00. The answer is 57. (Correct)There are four schools competing at a basketball tournament. Each school has sent a girls’ basketball team and a boys’ basketball team and each team has 5 players each. Each school has also sent a coach for each team. In total, how many people have all of the schools sent?There are 4 schools. Each school has sent a girls’ basketball team and a boys’ basketball team. Each team has 5 players each. That means there are 4 * 2 * 5 = 40 players. Each team has a coach. That means there are 40 + 4 = 44 coaches. The answer is 44. (Incorrect: one step missing error)Each school has sent 2 teams, each with 5 players. This means each school has sent 2 * 5 = 10 players. Each school has also sent 2 coaches. This means each school has sent 10 + 2 = 12 people. There are 4 schools, so in total all of the schools have sent 4 * 12 = 48 people. The answer is 48. (Correct)62B Model Output\\n62B Model Output\\n62B Model Output540B Model Output\\n540B Model Output\\n540B Model OutputQuestion\\nQuestion\\nQuestionFigure 10: Examples of semantic understanding and one-step missing errors that were ﬁxed by\\nscaling PaLM from 62B to 540B.\\nA.2 What is the role of prompt engineering?\\nOne of the key considerations of prompting is sensitivity to the exact prompt. There is no shortage\\nof work showing that prompts affect language models in unexpected ways (Min et al., 2022). The\\ngeneral way that we created chain of thought annotations was by taking eight exemplars from the\\ntraining set and decomposing the reasoning process into multiple steps leading to the ﬁnal answer.\\nExamples of chain of thought annotations are provided in Figure 3, with full prompts given in\\nAppendix G. To analyze how sensitive chain of thought is to prompt engineering, we performed\\nrobustness experiments with respect to various factors.\\n•Different annotators. We ﬁrst analyze robustness to three different annotators (Section 3.4 and\\nFigure 6). Although there is notable variance in performance (which we will discuss later), chain\\nof thought performed better than the baseline by a large margin for all three annotators on eight\\ndatasets in arithmetic, commonsense, and symbolic reasoning (Table 6 and Table 7). Similar to the\\nannotation process in Cobbe et al. (2021), annotators were not given speciﬁc instructions about\\n17\", doc_id='2b410327-7678-4c6d-88e6-6648744b67d1', embedding=None, doc_hash='f2dcdab9d77f0af134f1c74d6e02c3a824ed8164012a081940bde5e55766c18e', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='how to write the chain of thought annotations other than to simply write the step-by-step reasoning\\nprocess that led to the ﬁnal answer. Thus, the annotations were written in each annotator’s own\\nlinguistic “chain of thought” writing style.\\n•Annotators without machine learning background. The GSM8K dataset (Cobbe et al., 2021)\\nconveniently provides a training set with reasoning chains written by crowd compute workers,\\nwhich enables us to investigate whether chain of thought still works with reasoning chains from an\\nindependent source without a background in machine learning. So we randomly sampled three sets\\nof eight exemplars with chains of thought from GSM8K. These chain of thought annotations also\\noutperformed the baseline by a large margin for all four arithmetic datasets (Table 6), indicating\\nthat chain of thought is not dependent on a particular set of annotators.\\n•Different exemplars. The different GSM8K exemplars experiment above (Table 6) also shows\\nthat chain-of-thought prompting works for different sets of exemplars. Notably, we test every set of\\nexemplars on all four arithmetic datasets (instead of picking exemplars from the training set for\\neach dataset), which suggests that the exemplars do not necessarily have to come from the same\\ndataset distribution as the test examples.\\n•Different order of exemplars. Prior work has shown that in some cases (e.g., classiﬁcation) even\\nthe order of prompts matter—varying the permutation of few-shot exemplars can cause the accuracy\\nof GPT-3 on SST-2 to range from near chance (54.3%) to near SOTA (93.4%) (Zhao et al., 2021).\\nWe show the standard deviation of performance from different exemplars in Table 6 and Table 7.\\nStandard deviations with respect to prompt order are relatively minimal in almost all cases. The\\none exception is the coin ﬂip task, for which exemplar orders have high standard deviation, likely\\nfor the reason cited in Zhao et al. (2021)—for classiﬁcation, many exemplars of the same category\\nin a row biases the model outputs).\\n•Different number of exemplars. We also found that gains from chain-of-thought prompting\\ngenerally still held when there was a varying number of few-shot exemplars. This is shown for ﬁve\\ndatasets in Figure 11 (we did not have the compute to run this for all datasets). We also found in\\npreliminary experiments that further increasing the number of exemplars in standard prompting\\ndid not lead to signiﬁcant gains (e.g., increasing from 8 to 16 exemplars did not improve the\\nperformance of standard prompting enough to catch up with chain-of-thought prompting).\\n•Different language models. Another interesting question is whether certain prompts that work\\nbetter for one model work better for other large language models. We ﬁnd that with the same\\nprompts, chain-of-thought prompting improves performance across all three models (LaMDA,\\nGPT-3, and PaLM) for all datasets except CSQA and StrategyQA for GPT-3 (Table 1, Table 4,\\nTable 5). The fact that gains from chain of thought did not transfer perfectly among models is\\na limitation; further work could investigate why how different pre-training datasets and model\\narchitectures affect the performance gain from chain-of-thought prompting.\\nPrompt engineering still matters, though. Although the results are relatively robust to the prompt\\nfor arithmetic reasoning, we want to be clear that prompt engineering still does matter, and can\\nimprove performance signiﬁcantly in many cases. Though most chain of thought annotations\\noutperform standard prompting, there is large variation in many cases. For instance, for the coin\\nﬂip task, the performance varied from 99.6% for Annotator A to 71.4% for Annotator C, though\\nboth were above standard prompting = 50.0% (see Table 7). There are even tasks where prompt\\nengineering is a requirement for good performance. In preliminary experiments, we tried using chain\\nof thought to enable language models to reverse the order of a list of 5 items. While two co-authors\\nwere not able to write chain of thought prompts that solved the task despite their best attempts, a third\\nco-author was able to write a chain of thought that perfectly solved the task.\\nHow to generate chain of thought annotations in a robust fashion could be an interesting direction\\nfor future work. For instance, an idea here could be to use a large language model to automatically\\ngenerate chains of thought via prompting (and potentially optimize this over a validation set).\\nA.3 Will chain-of-thought prompting improve performance for my task of interest?\\nWhile chain-of-thought prompting is in principle applicable for any text-to-text task, it is more\\nhelpful for some tasks than others. Based on the experiments in this paper, our intuition is that chain\\nof thought helps the most when three conditions are met: (1) the task is challenging and requires\\n18', doc_id='525bdec8-f417-4f76-881f-7547aba553b1', embedding=None, doc_hash='7d53a9f051c845097dba0324ba4d0854a9ccc26927c87397eff36932ce6a12f9', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='multi-step reasoning, (2) a large language model is used, and (3) the scaling curve is relatively ﬂat.\\nConversely, the beneﬁts are smaller when one or more of these conditions are not met.\\nThese intuitions are perhaps supported by the arithmetic reasoning results. The performance gain\\nfrom chain-of-thought prompting is largest for PaLM 540B on GSM8K (challenging multi-step\\nproblems, ﬂat scaling curve), which meets these conditions. The performance gain is small for the\\nsubsets of MAWPS that only require one or two steps (SingleOP, SingleEq, and AddSub), for which\\nPaLM 540B already achieves performance of 90% or higher (and it is also generally true that there is\\nless headroom for improvement when performance is already strong).\\nAlthough in this paper we focused on multi-step reasoning tasks (arithmetic, commonsense, and\\nsymbolic), chain-of-thought prompting can potentially be applied to any task for which humans use a\\n“chain of thought” to solve (at least in principle). We leave the empirical evaluation of chain-of-thought\\nprompting on such diverse tasks (e.g., machine translation, etc.) to future work.\\nA.4 Why is prompting with the equation only not enough for some arithmetic reasoning\\ndatasets?\\nPrompting with the equation only as an intermediate step does help on many datasets, especially when\\nthe datasets only require a few reasoning steps (SV AMP, ASDiv, MAWPS). For GSM8K, however,\\nusing the equation only did not improve performance substantially. Based on qualitative analysis, we\\nbelieve that these questions are too semantically challenging for the model to directly translate them\\ninto a math equation. Consider this example from LaMDA 137B:\\nQUESTION :Mike plays ping pong for 40 minutes. In the ﬁrst 20 minutes, he scores 4\\npoints. In the second 20 minutes, he scores 25% more points. How many total points did he\\nscore?\\nEQUATION ONLY (WRONG ANSWER ):(4 + 20 * 0.25) = 6. The answer is 6.\\nCHAIN OF THOUGHT (CORRECT ):Mike played ping pong for 40 minutes. In the ﬁrst\\n20 minutes, he scored 4 points. In the second 20 minutes, he scored 25% more points. So\\nhe scored 25% more in the second 20 minutes. 4 x 1.25 = 5. So he scored 5 points in the\\nsecond 20 minutes. So he scored 9 points in total. The answer is 9.\\nIt is hard for the model to directly translate all of the semantics into a single equation, but chain of\\nthought allows it to better reason about each part of the question via intermediate steps in natural\\nlanguage.\\n19', doc_id='7d809984-53d4-4ec0-bb03-b84626f02484', embedding=None, doc_hash='09b3f1d9c7b9186cb0b2b1e6da1446bb5ddc6b35b62404ad173f3be1db5a4156', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='B All Experimental Results\\nThis section contains tables for experimental results for varying models and model sizes, on all\\nbenchmarks, for standard prompting vs. chain-of-thought prompting.\\nFor the arithmetic reasoning benchmarks, some chains of thought (along with the equations produced)\\nwere correct, except the model performed an arithmetic operation incorrectly. A similar observation\\nwas made in Cobbe et al. (2021). Hence, we can further add a Python program as an external\\ncalculator (using the Python eval function) to all the equations in the generated chain of thought.\\nWhen there are multiple equations in a chain of thought, we propagate the external calculator results\\nfrom one equation to the following equations via string matching. As shown in Table 1, we see that\\nadding a calculator signiﬁcantly boosts performance of chain-of-thought prompting on most tasks.\\nTable 1: Chain of thought prompting outperforms standard prompting for various large language\\nmodels on ﬁve arithmetic reasoning benchmarks. All metrics are accuracy (%). Ext. calc.: post-hoc\\nexternal calculator for arithmetic computations only. Prior best numbers are from the following. a:\\nCobbe et al. (2021). b&e: Pi et al. (2022), c: Lan et al. (2021), d: Pi˛ ekos et al. (2021).\\nPrompting GSM8K SV AMP ASDiv AQuA MAWPS\\nPrior best N/A (ﬁnetuning) 55a57.4b75.3c37.9d88.4e\\nUL2 20B Standard 4.1 10.1 16.0 20.5 16.6\\nChain of thought 4.4 (+0.3) 12.5 (+2.4) 16.9 (+0.9) 23.6 (+3.1) 19.1 (+2.5)\\n+ ext. calc 6.9 28.3 34.3 23.6 42.7\\nLaMDA 137B Standard 6.5 29.5 40.1 25.5 43.2\\nChain of thought 14.3 (+7.8) 37.5 (+8.0) 46.6 (+6.5) 20.6 (-4.9) 57.9 (+14.7)\\n+ ext. calc 17.8 42.1 53.4 20.6 69.3\\nGPT-3 175B Standard 15.6 65.7 70.3 24.8 72.7\\n(text-davinci-002) Chain of thought 46.9 (+31.3) 68.9 (+3.2) 71.3 (+1.0) 35.8 (+11.0) 87.1 (+14.4)\\n+ ext. calc 49.6 70.3 71.1 35.8 87.5\\nCodex Standard 19.7 69.9 74.0 29.5 78.7\\n(code-davinci-002) Chain of thought 63.1 (+43.4) 76.4 (+6.5) 80.4 (+6.4) 45.3 (+15.8) 92.6 (+13.9)\\n+ ext. calc 65.4 77.0 80.0 45.3 93.3\\nPaLM 540B Standard 17.9 69.4 72.1 25.2 79.2\\nChain of thought 56.9 (+39.0) 79.0 (+9.6) 73.9 (+1.8) 35.8 (+10.6) 93.3 (+14.2)\\n+ ext. calc 58.6 79.8 72.6 35.8 93.5\\n20', doc_id='9b37665f-f0d1-4841-9204-fa51cd686740', embedding=None, doc_hash='8cff8e3140da32304bda2b4bd3f70d45febe8b4160d3e902b366d44a0231c1d2', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 2: Standard prompting versus chain of thought prompting on ﬁve arithmetic reasoning bench-\\nmarks. Note that chain of thought prompting is an emergent ability of model scale—it does not\\npositively impact performance until used with a model of sufﬁcient scale.\\nGSM8K SV AMP ASDiv AQuA MAWPS\\nModel standard CoT standard CoT standard CoT standard CoT standard CoT\\nUL2 20B 4.1 4.4 10.1 12.5 16.0 16.9 20.5 23.6 16.6 19.1\\nLaMDA 420M 2.6 0.4 2.5 1.6 3.2 0.8 23.5 8.3 3.2 0.9\\n2B 3.6 1.9 3.3 2.4 4.1 3.8 22.9 17.7 3.9 3.1\\n8B 3.2 1.6 4.3 3.4 5.9 5.0 22.8 18.6 5.3 4.8\\n68B 5.7 8.2 13.6 18.8 21.8 23.1 22.3 20.2 21.6 30.6\\n137B 6.5 14.3 29.5 37.5 40.1 46.6 25.5 20.6 43.2 57.9\\nGPT 350M 2.2 0.5 1.4 0.8 2.1 0.8 18.1 8.7 2.4 1.1\\n1.3B 2.4 0.5 1.5 1.7 2.6 1.4 12.6 4.3 3.1 1.7\\n6.7B 4.0 2.4 6.1 3.1 8.6 3.6 15.4 13.4 8.8 3.5\\n175B 15.6 46.9 65.7 68.9 70.3 71.3 24.8 35.8 72.7 87.1\\nCodex - 19.7 63.1 69.9 76.4 74.0 80.4 29.5 45.3 78.7 92.6\\nPaLM 8B 4.9 4.1 15.1 16.8 23.7 25.2 19.3 21.7 26.2 30.5\\n62B 9.6 29.9 48.2 46.7 58.7 61.9 25.6 22.4 61.8 80.3\\n540B 17.9 56.9 69.4 79.0 72.1 73.9 25.2 35.8 79.2 93.3\\nTable 3: Standard prompting versus chain of thought prompting on the four subsets of the MAWPS\\nbenchmark. The point of stratifying the MAWPS benchmark is to show that performance gains are\\nminimal on easy one-step or two-step problems where large language models already achieve high\\nperformance (e.g., SingleOp, SingleEq, and AddSub).\\nSingleOp SingleEq AddSub MultiArith\\nModel standard CoT standard CoT standard CoT standard CoT\\nUL2 20B 24.9 27.2 18.0 20.2 18.5 18.2 5.0 10.7\\nLaMDA 420M 2.8 1.0 2.4 0.4 1.9 0.7 5.8 1.5\\n2B 4.6 4.1 2.4 3.3 2.7 3.2 5.8 1.8\\n8B 8.0 7.0 4.5 4.4 3.4 5.2 5.2 2.4\\n68B 36.5 40.8 23.9 26.0 17.3 23.2 8.732.4\\n137B 73.2 76.2 48.8 58.7 43.0 51.9 7.644.9\\nGPT 350M 3.2 1.8 2.0 0.2 2.0 1.5 2.3 0.8\\n1.3B 5.3 3.0 2.4 1.6 2.3 1.5 2.2 0.5\\n6.7B 13.5 3.9 8.7 4.9 8.6 2.5 4.5 2.8\\n175B 90.9 88.8 82.7 86.6 83.3 81.3 33.8 91.7\\nCodex - 93.1 91.8 86.8 93.1 90.9 89.1 44.0 96.2\\nPaLM 8B 41.8 46.6 29.5 28.2 29.4 31.4 4.215.8\\n62B 87.9 85.6 77.2 83.5 74.7 78.2 7.373.7\\n540B 94.1 94.1 86.5 92.3 93.9 91.9 42.2 94.7\\n21', doc_id='103057b5-60bd-42f5-824a-efbd7295b77a', embedding=None, doc_hash='1aa0e45c0c48d31205bccdec467d6e8d47c53734ccd8e0f17a86b3bf99bd1844', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 4: Standard prompting versus chain of thought prompting on ﬁve commonsense reasoning\\nbenchmarks. Chain of thought prompting is an emergent ability of model scale—it does not positively\\nimpact performance until used with a model of sufﬁcient scale.\\nCSQA StrategyQA Date Sports SayCan\\nModel standard CoT standard CoT standard CoT standard CoT standard CoT\\nUL2 20B 34.2 51.4 59.0 53.3 13.5 14.0 57.9 65.3 20.0 41.7\\nLaMDA 420M 20.1 19.2 46.4 24.9 1.9 1.6 50.0 49.7 7.5 7.5\\n2B 20.2 19.6 52.6 45.2 8.0 6.8 49.3 57.5 8.3 8.3\\n8B 19.0 20.3 54.1 46.8 9.5 5.4 50.0 52.1 28.3 33.3\\n68B 37.0 44.1 59.6 62.2 15.5 18.6 55.2 77.5 35.0 42.5\\n137B 53.6 57.9 62.4 65.4 21.5 26.8 59.5 85.8 43.3 46.6\\nGPT 350M 14.7 15.2 20.6 0.9 4.3 0.9 33.8 41.6 12.5 0.8\\n1.3B 12.0 19.2 45.8 35.7 4.0 1.4 0.0 26.9 20.8 9.2\\n6.7B 19.0 24.0 53.6 50.0 8.9 4.9 0.0 4.4 17.5 35.0\\n175B 79.5 73.5 65.9 65.4 43.8 52.1 69.6 82.4 81.7 87.5\\nCodex - 82.3 77.9 67.1 73.2 49.0 64.8 71.7 98.5 85.8 88.3\\nPaLM 8B 19.8 24.9 55.6 53.5 12.9 13.1 55.1 75.2 34.2 40.0\\n62B 65.4 68.1 58.4 63.4 29.8 44.7 72.1 93.6 65.8 70.0\\n540B 78.1 79.9 68.6 77.8 49.0 65.3 80.5 95.4 80.8 91.7\\nTable 5: Standard prompting versus chain of thought prompting enables length generalization to\\nlonger inference examples on two symbolic manipulation tasks.\\nLast Letter Concatenation Coin Flip (state tracking)\\n2 OOD: 3 OOD: 4 2 OOD: 3 OOD: 4\\nModel standard CoT standard CoT standard CoT standard CoT standard CoT standard CoT\\nUL2 20B 0.6 18.8 0.0 0.2 0.0 0.0 70.4 67.1 51.6 52.2 48.7 50.4\\nLaMDA 420M 0.3 1.6 0.0 0.0 0.0 0.0 52.9 49.6 50.0 50.5 49.5 49.1\\n2B 2.3 6.0 0.0 0.0 0.0 0.0 54.9 55.3 47.4 48.7 49.8 50.2\\n8B 1.5 11.5 0.0 0.0 0.0 0.0 52.9 55.5 48.2 49.6 51.2 50.6\\n68B 4.4 52.0 0.0 0.8 0.0 2.5 56.2 83.2 50.4 69.1 50.9 59.6\\n137B 5.8 77.5 0.034.4 0.013.5 49.0 99.6 50.7 91.0 49.1 74.5\\nPaLM 8B 2.6 18.8 0.0 0.0 0.0 0.2 60.0 74.4 47.3 57.1 50.9 51.8\\n62B 6.8 85.0 0.059.6 0.013.4 91.4 96.8 43.9 91.0 38.3 72.4\\n540B 7.6 99.4 0.294.8 0.063.0 98.1 100.0 49.3 98.6 54.8 90.2\\n22', doc_id='de6931ea-e244-4751-8c1d-d31b8e2cd911', embedding=None, doc_hash='f905eed02704fcde7952323189c5e39435e30848a864b7b5f5f6a930f187709b', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 6: Ablation and robustness results for arithmetic reasoning datasets. Chain of thought generally\\noutperforms ablations by a large amount. “Equation only” performs in between standard prompting\\nand chain of thought prompting, as it allows for intermediate reasoning steps via equations but does\\nnot leverage natural language. Chain of thought prompting has variance (as expected) when used\\nwith prompts written by different annotators or when using other exemplars, but still outperforms\\nstandard prompting by a large margin. Standard deviation shown is for different order of few-shot\\nprompting exemplars, with ﬁve different random seeds. Results here are shown for LaMDA 137B, as\\nadditional queries for GPT-3 and PaLM are both limited and expensive.\\nGSM8K SV AMP ASDiv MAWPS\\nStandard prompting 6.5 \\x060.4 29.5\\x060.6 40.1\\x060.6 43.2\\x060.9\\nChain of thought prompting 14.3 \\x060.4 36.7\\x060.4 46.6\\x060.7 57.9\\x061.5\\nAblations\\n\\x01equation only 5.4 \\x060.2 35.1\\x060.4 45.9\\x060.6 50.1\\x061.0\\n\\x01variable compute only 6.4 \\x060.3 28.0\\x060.6 39.4\\x060.4 41.3\\x061.1\\n\\x01reasoning after answer 6.1 \\x060.4 30.7\\x060.9 38.6\\x060.6 43.6\\x061.0\\nRobustness\\n\\x01different annotator (B) 15.5 \\x060.6 35.2\\x060.4 46.5\\x060.4 58.2\\x061.0\\n\\x01different annotator (C) 17.6 \\x061.0 37.5\\x062.0 48.7\\x060.7 60.1\\x062.0\\n\\x01intentionally concise style 11.1 \\x060.3 38.7\\x060.8 48.0\\x060.3 59.6\\x060.7\\n\\x01exemplars from GSM8K ( \\x0b) 12.6 \\x060.6 32.8\\x061.1 44.1\\x060.9 53.9\\x061.1\\n\\x01exemplars from GSM8K ( \\x0c) 12.7 \\x060.5 34.8\\x061.1 46.9\\x060.6 60.9\\x060.8\\n\\x01exemplars from GSM8K ( \\r) 12.6 \\x060.7 35.6\\x060.5 44.4\\x062.6 54.2\\x064.7\\nTable 7: Ablation and robustness results for four datasets in commonsense and symbolic reasoning.\\nChain of thought generally outperforms ablations by a large amount. Chain of thought prompting has\\nvariance (as expected) when used with prompts written by different annotators or when using other\\nexemplars, but still outperforms standard prompting by a large margin. Standard deviation shown\\nis for different order of few-shot prompting exemplars, with ﬁve different random seeds. Results\\nhere are shown for LaMDA 137B, as additional queries for GPT-3 and PaLM are both limited and\\nexpensive. The exception is that we run SayCan using PaLM here, as the SayCan evaluation set is\\nonly 120 examples and therefore less expensive to run multiple times.\\nCommonsense Symbolic\\nDate Sports SayCan Concat Coin\\nStandard prompting 21.5 \\x060.6 59.5\\x063.0 80.8\\x061.8 5.8\\x060.6 49.0\\x062.1\\nChain of thought prompting 26.8 \\x062.1 85.8\\x061.8 91.7\\x061.4 77.5\\x063.8 99.6\\x060.3\\nAblations\\n\\x01variable compute only 21.3 \\x060.7 61.6\\x062.2 74.2\\x062.3 7.2\\x061.6 50.7\\x060.7\\n\\x01reasoning after answer 20.9 \\x061.0 63.0\\x062.0 83.3\\x060.6 0.0\\x060.0 50.2\\x060.5\\nRobustness\\n\\x01different annotator (B) 27.4 \\x061.7 75.4\\x062.7 88.3\\x061.4 76.0\\x061.9 77.5\\x067.9\\n\\x01different annotator (C) 25.5 \\x062.5 81.1\\x063.6 85.0\\x061.8 68.1\\x062.2 71.4\\x0611.1\\n23', doc_id='0cfed55d-3e66-44de-a97a-bd92ae7ad239', embedding=None, doc_hash='fd957369ce4da44417ecdbad08856cfc170fa009e6e7980ab39573a76c35901b', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='C Extended Related Work\\nChain-of-thought prompting is a general approach that is inspired by several prior directions: prompt-\\ning, natural language explanations, program synthesis/execution, numeric and logical reasoning, and\\nintermediate language steps.\\nC.1 Prompting\\nThe recent success of large-scale language models has led to growing interest in improving their\\ncapability to perform tasks via prompting (Brown et al. (2020), and see Liu et al. (2021) for a\\nsurvey). This paper falls in the category of general prompting approaches, whereby input prompts are\\noptimized to allow a single large language model to better perform a variety of tasks (Li and Liang,\\n2021; Lester et al., 2021; Reif et al., 2022, inter alia ).\\nOne recent line of work aims to improve the ability of language models to perform a task by providing\\ninstructions that describe the task (Raffel et al., 2020; Wei et al., 2022a; Ouyang et al., 2022; Sanh\\net al., 2022; Wang et al., 2022b). This line of work is related because it also augments input–output\\npairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically\\nprepended to the inputs), chain-of-thought prompting augments the outputs of language models.\\nAnother related direction is sequentially combining the outputs of language models; human–computer\\ninteraction (HCI) work (Wu et al., 2022a,b) has shown that combining sequential generations of\\nlanguage models improves task outcomes in a 20-person user study.\\nC.2 Natural language explanations\\nAnother closely related direction uses natural language explanations (NLEs), often with the goal of\\nimproving model interpretability (Zhou et al., 2020; Wiegreffe and Marasovi ´c, 2021, inter alia ). That\\nline of work typically focuses on natural language inference (Camburu et al., 2018; Yordanov et al.,\\n2021; Bostrom et al., 2021), and produces explanations either simultaneously to or after the ﬁnal\\nprediction (Narang et al., 2020; Majumder et al., 2021; Wiegreffe et al., 2021, 2022). By contrast,\\nthe chain of thought processing considered in this paper occurs before the ﬁnal answer. And while\\nNLE aims mostly to improve neural network interpretability (Rajagopal et al., 2021), the goal of\\nchain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple\\nsteps—interpretability is just a side effect. Marasovi ´c et al. (2022) show that prompt-based ﬁnetuning\\nwith NLE improves NLI and classiﬁcation performance, though they largely focus on evaluating\\nexplanation plausibility. In comparison, our work focuses on a range of arithmetic, commonsense,\\nand symbolic tasks that require multi-hop reasoning.\\nC.3 Program synthesis and execution\\nUsing intermediate reasoning steps has a long history in program synthesis and execution (Zaremba\\nand Sutskever, 2014, inter alia ). Recent work along in this direction has included a number of\\narchitectural innovations (Cai et al., 2017; Dong et al., 2019; Yan et al., 2020), as well as the use of\\nlarge language models (Chen et al., 2021; Austin et al., 2021). The program execution work closest to\\nours is perhaps Nye et al. (2021), which show that large language models can perform up to 10-digit\\naddition, evaluate polynomials, and execute python programs. Whereas generating a program and\\nthen executing it can be viewed as a type of reasoning, our work generalizes such domain-speciﬁc\\nprimitives to natural language, which is open-domain and relevant to any text-to-text NLP task in\\nprinciple.\\nC.4 Numeric and logical reasoning\\nNumeric and logical reasoning has been a long-studied task in machine learning and natural language\\nprocessing (Lev et al., 2004, inter alia ). Recent work has also aimed to inject numeric reasoning\\nabilities in language models in various ways, such as augmenting BERT with a predeﬁned set of\\nexecutable operations (Andor et al., 2019), including a graph neural network (Ran et al., 2019), and\\nusing specialized training procedures (Pi˛ ekos et al., 2021). Another line of work aims to enable\\nlanguage models to perform logical or formal reasoning, often by verablizing the rules in natural\\nlanguage formal rules using language (Clark et al., 2020; Saeed et al., 2021; Liang et al., 2021).\\n24', doc_id='5589ff3b-298c-4a7f-bc2d-c267d3313b7c', embedding=None, doc_hash='18ccbe33c75c84aa7d47da63d52c204a84a045fcd87ac260a8ef3e5acbe1bc2b', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Perhaps the most-related work here is Recchia (2021), which shows that ﬁnetuning enables longhand\\nmodule operations, which has previously been difﬁcult for performers. Whereas work in this direction\\nis often task-speciﬁc and uses ﬁnetuning, we show that chain-of-thought prompting works for a broad\\nrange of tasks without any ﬁnetuning.\\nC.5 Intermediate language steps\\nExtensive prior work has shown the beneﬁts of endowing neural networks with the ability to produce\\nintermediate steps via training or ﬁnetuning confers various beneﬁts in a range of scenarios. As\\nexamples, it has been shown that natural language intermediate steps can improve performance\\n(Zaidan et al., 2007; Yao et al., 2021; Hase and Bansal, 2022; Gu et al., 2022), improve robustness\\n(Chen et al., 2022), speed up training (Hancock et al., 2018), mitigate bias (Dua et al., 2020), and\\neven help in image and reinforcement learning settings (Andreas et al., 2018). To endow models with\\nthe ability to produce intermediate steps, prior work typically ﬁnetunes models on either manually\\nannotated training datasets (Camburu et al., 2018; Rajani et al., 2019, inter alia ) or generates synthetic\\ndatasets (Talmor et al., 2020; Zelikman et al., 2022). Compared with these training or ﬁnetuning\\nmethods, our work shows that various natural language reasoning abilities can be elicited in off-the-\\nshelf language models of sufﬁcient scale simply via prompting. This prompting setup is important\\nbecause it allows for intermediate step reasoning without a large number of labeled annotations, and\\nbecause a single model can perform a range of reasoning tasks without any gradient updates.\\nD Appendix: Additional Analysis\\nD.1 Correct Chain of Thought Analysis\\nAs mentioned in the main text, we analyze 50 chains of thought from LaMDA 137B that led to\\ncorrect answers in the GSM8K dataset. Of these 50, only one arrived at the correct answer through\\nincorrect reasoning (shown in Table 9: “correct by chance”). The other 49 had correct logic and\\nmath, with examples shown in Table 8. Five had minor imperfections while maintaining coherent and\\nunderstandable logic:\\n• One had underspeciﬁed statements (shown in Table 9: “correct but underspeciﬁed statements”).\\n•One made an unrelated but correct statement (shown in Table 9: “correct but unrelated statement”).\\n•Two had the correct logic but omitted speciﬁc steps in the equation, though it was still understand-\\nable (shown in Table 9: “correct but imperfect equation”).\\n•One had the correct math but inverted the semantics (shown in Table 9: “correct but inverted\\nsemantics”)\\nTable 8: Example of correct chains of thought produced by the model for the GSM8K dataset.\\nQUESTION :James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total\\nmeters does he run a week?\\nMODEL ANSWER (CORRECT): James decides to run 3 sprints 3 times a week. He runs 60 meters each\\nsprint. So he runs 60 meters x 3 sprints x 3 times a week. That is 60 meters x 9. The answer is 540. X\\nQUESTION :Brandon’s iPhone is four times as old as Ben’s iPhone. Ben’s iPhone is two times older than Suzy’s\\niPhone. If Suzy’s iPhone is 1 year old, how old is Brandon’s iPhone?\\nMODEL ANSWER (CORRECT): Brandon’s iPhone is 4 times as old as Ben’s iPhone. Ben’s iPhone is 2\\ntimes older than Suzy’s iPhone. So Brandon’s iPhone is 4 x 2 = 8 times older than Suzy’s iPhone. Suzy’s iPhone\\nis 1 year old. So Brandon’s iPhone is 8 x 1 = 8 years old. The answer is 8. X\\nQUESTION :Jean has 30 lollipops. Jean eats 2 of the lollipops. With the remaining lollipops, Jean wants to\\npackage 2 lollipops in one bag. How many bags can Jean ﬁll?\\nMODEL ANSWER (CORRECT): Jean started with 30 lollipops. She ate 2 of them. So she has 28 lollipops\\nleft. She wants to package 2 lollipops in one bag. So she can package 28 / 2 = 14 bags. The answer is 14. X\\nAlthough we ﬁnd that chain-of-thought reasoning is mostly correct for math questions where the ﬁnal\\nanswer was correct, this is likely because it is hard to arrive at the correct ﬁnal answer by coincidence\\nfor free response questions. For multiple choice or binary classiﬁcation problems, it was much more\\n25', doc_id='5c9d52fa-9311-46a4-a5ca-94fbdb41ff6a', embedding=None, doc_hash='6b47cc5435e14d0ff37546484cee99c96a95c43a25c9ce9e6abbf4dd6c63496b', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='likely that models could arrive at the correct answer via an incorrect reasoning path (e.g., all the\\ncommmonsense reasoning datasets we evaluate). This is a limitation, and future work should perform\\nan analysis of the factuality of such chains of thought.\\nTable 9: Of 50 examples that the model got correct, there were 7 salient cases where the chain of\\nthought the model generated was imperfect. We found the other 43 to be equivalent to what a human\\nwould potentially generate as a ground truth.\\nQUESTION :There are 36 penguins sunbathing in the snow. One-third of them jump in and swim in the ocean.\\nAnother one-third go inside the cave to eat their dinner. How many penguins are still left sunbathing?\\nMODEL ANSWER (CORRECT BY CHANCE ):There are 36 penguins. One-third of them jump in and swim in\\nthe ocean. So that is 12 penguins. Another one-third go inside the cave to eat their dinner. So that is 12 penguins.\\nThe answer is 12.\\nQUESTION :John has 2 houses with 3 bedrooms each. Each bedroom has 2 windows each. There are an\\nadditional 4 windows in each house not connected to bedrooms. How many total windows are there between the\\nhouses?\\nMODEL ANSWER (CORRECT BUT UNDERSPECIFIED STATEMENTS ):There are 2 houses with 3 bedrooms\\neach. Each bedroom has 2 windows each. So there are 2 x 3 = 6 windows in each house. There are an additional\\n4 windows in each house not connected to bedrooms. So there are 4 + 4 = 8 windows in each house. So there are\\n6 x 2 = 12 windows in each house. So there are 12 + 8 = 20 windows in both houses. The answer is 20.\\nQUESTION :If Ann is 9 years old and her brother is twice her age, how old will her brother be in 3 years?\\nMODEL ANSWER (CORRECT BUT UNRELATED STATEMENT ):Ann is 9 years old. Her brother is twice her\\nage. So her brother is 18 years old. In 3 years, she will be 12. So her brother will be 18 + 3 = 21 years old. The\\nanswer is 21.\\nQUESTION :A local gas station is selling gas for $3.00 a gallon. An app company is offering $.20 cashback per\\ngallon if you ﬁll up at this station. If someone buys 10 gallons of gas, how much with their gas be, after the\\ncashback rewards?\\nMODEL ANSWER (CORRECT BUT IMPERFECT EQUATION ):The gas is originally 3 dollars per gallon. If\\nyou buy 10 gallons, that is 3 x 10 = 30 dollars. If you get 20 cents off per gallon, that is 20 x 10 = 2 dollars. So\\nthe total is 30 - 2 = 28 dollars. The answer is 28 dollars.\\nQUESTION :Nissa hires 60 seasonal workers to play elves in her department store’s Santa village. A third of\\nthe elves quit after children vomit on them, then 10 of the remaining elves quit after kids kick their shins. How\\nmany elves are left?\\nMODEL ANSWER (CORRECT BUT IMPERFECT EQUATION ):Nissa hires 60 seasonal workers. A third of\\nthem quit. So 60 - 1/3 = 40 elves are left. Then 10 elves quit. So 40 - 10 = 30 elves are left. The answer is 30\\nelves.\\nQUESTION :Grandma Jones baked 5 apple pies for the ﬁreman’s luncheon. She cut each pie into 8 pieces and\\nset the ﬁve pies out on the buffet table for the guests to serve themselves. At the end of the evening, after the\\nguests had taken and eaten their pieces of pie, there were 14 pieces of pie remaining. How many pieces were\\ntaken by the guests?\\nMODEL ANSWER (CORRECT BUT INVERTED SEMANTICS ):5 pies were baked and cut into 8 pieces each.\\nThe 5 pies were then served to the guests. 8 x 5 = 40 pieces of pie. The guests ate 14 pieces of pie. 40 - 14 = 26\\npieces of pie were left. The answer is 26. X\\n26', doc_id='31fa6db7-b1fc-4cda-ab50-6ff5d88ca85d', embedding=None, doc_hash='e096c1d52ab8c6eaa4e68e34c50fbb630434b1daeab80d4b2c5228496b66d364', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='D.2 Incorrect Chain of Thought Analysis\\nWe also manually analyze 50 randomly sampled outputs of the model that were incorrect on GSM8K\\nfor LaMDA 137B. There are many ways that a chain of thought can be incorrect, making the design\\nof error categorization non-trivial. We decided to categorize errors into what changes are needed to\\nmake the chain of thought correct, with the goal of elucidating how the model can be improved in the\\nfuture.\\nWe found that many chains of thought can be made correct with one of the following three classes of\\nmodiﬁcation.\\n•Calculator error only. We found that 8% of the chains of thought were completely correct except\\nfor a calculator error—in other words, applying an external calculator to equations, as done in\\nCobbe et al. (2021), would make the chain of thought correct. An example of this type of error is\\nshown in Table 10: “calculator error only”. Indeed, the solve rate of chain-of-thought prompting\\non for LaMDA 137B GSM8K went up from 14.3% to 17.3% when we added a Python program\\nas an external calculator, as shown in Table 2. Also, 34% of the examples contained calculator\\nerrors in addition to other types of errors. However, we perform the rest of the error categorization\\nindependently of calculator errors.\\n•Symbol mapping error. We next found that 16% percent of the chains of thought were correct\\nexcept for what we call symbol mapping errors. We deﬁne a symbol mapping error as when the\\nchain of thought is correct except for the number symbols, and it could be made totally correct by\\nmodifying only the equations and not the words. As one might argue that they could simply place\\nthe correct ﬁnal equation in any chain of thought, we constrain this category to chains of thought\\nwhere the chain of thought can be modiﬁed to be a completely correct reasoning process (not just\\nﬁnal answer). An example of this error category is shown in Table 10: “symbol mapping error”.\\n•One step missing error. Our next category of error is chains of thought which were correct except\\nthat they were missing a single step. In other words, these chains of thoughts could be rewritten to\\nbe correct by adding in an additional reasoning step that was missed by the model. An example of\\nthis error category is shown in Table 10: “one step missing error”. We found that 22% percent of\\nthe errors fell into this category.\\nTable 10: Example of incorrect chains of thought, categorized as described in Appendix D.2.\\nQUESTION :Dan plants 3 rose bushes. Each rose bush has 25 roses. Each rose has 8 thorns. How many thorns\\nare there total?\\nMODEL ANSWER (INCORRECT ;CALCULATOR ERROR ONLY ):Dan plants 3 rose bushes. Each rose bush\\nhas 25 roses. Each rose has 8 thorns. So 3 x 25 x 8 = 300. The answer is 300. 7\\nEXPLANATION FOR ERROR CATEGORY :The produced chain of thought could be made correct just by\\nrunning the equation through an external calculator, instead of asking the model to do the computation.\\nQUESTION :Jill gets paid $20 per hour to teach and $30 to be a cheerleading coach. If she works 50 weeks a\\nyear, 35 hours a week as a teacher and 15 hours a week as a coach, what’s her annual salary?\\nMODEL ANSWER (INCORRECT ;SYMBOL MAPPING ERROR ):Jill gets paid 20 dollars per hour to teach\\nand 30 dollars per hour to be a cheerleading coach. If she works 50 weeks a year, 35 hours a week as a teacher\\nand 15 hours a week as a coach, then she works 50 x 35 = 1750 hours as a teacher and 15 x 30 = 450 hours as a\\ncoach. So she works 1750 + 450 = 2200 hours. She gets paid 20 dollars per hour for 1750 hours and 30 dollars\\nper hour for 450 hours. So her annual salary is 20 x 1750 + 30 x 450 = $36,500. The answer is $36,500. 7\\nEXPLANATION FOR ERROR CATEGORY :The model incorrectly mapped “15 x 30” instead of “15 x 50”, and\\nthe chain of thought would be made correct by ﬁxing just that one change (with an external calculator).\\nQUESTION :Kelian has two recipes for preparing dishes, one having 20 instructions and the second one having\\ntwice as many instructions as the ﬁrst one. How many instructions does Kelian have to read to prepare the two\\ndishes?\\nMODEL ANSWER (INCORRECT ;ONE STEP MISSING ERROR ):Kelian has two recipes. One has 20 instruc-\\ntions. The other has twice as many instructions as the ﬁrst one. So the second one has 40 instructions. So Kelian\\nhas to read 40 instructions to prepare the two dishes. The answer is 40. 7\\nEXPLANATION FOR ERROR CATEGORY :The model answer could be made correct just adding one additional\\nstep (in this case, adding 20 and 40).\\n27', doc_id='ef6e2e22-d8ad-4ddd-bb23-6004ce139b0e', embedding=None, doc_hash='628a3cd1c4db8c547918e2c14feffc30284bb21ec29bb2ec6a1b2934e4d5db34', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='We found that the remaining chains of thought (27 of 50; 54%) would require substantial edits to make\\ninto a correct chain of thought. Almost all cases here involved some error in semantic understanding\\n(see Table 11: “semantic understanding error”), and 8 of the 27 also had incoherent chain of thoughts,\\nmeaning that some statements in the generated chain of thought did not follow from prior ones or\\nviolated basic world knowledge (see Table 11: “incoherent chain of thought error”).\\nTable 11: Example of incorrect chains of thought, categorized as described in Appendix D.2. These\\nchains of thought had multiple mistakes that could not be ﬁxed with minor edits.\\nQUESTION :Gretchen has 110 coins. There are 30 more gold coins than silver coins. How many gold coins\\ndoes Gretchen have?\\nMODEL ANSWER (INCORRECT ;SEMANTIC UNDERSTANDING ERROR ):Gretchen has 110 coins. There\\nare 30 more gold coins than silver coins. So there are 110 - 30 = 80 silver coins. So there are 80 silver coins and\\n110 - 80 = 30 gold coins. The answer is 30. 7\\nEXPLANATION FOR ERROR CATEGORY :The model does not understand that 110 coins are in total and that\\nit must solve an equation to get the number of gold and silver coins.\\nQUESTION :In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining\\nenrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in\\nhip-hop dance?\\nMODEL ANSWER (INCORRECT ;INCOHERENT CHAIN OF THOUGHT ERROR ):20% of the students en-\\nrolled in contemporary dance. 25% of the remaining students enrolled in jazz dance. The rest enrolled in hip-hop\\ndance. So the percentage of the entire students enrolled in hip-hop dance is the percentage of the students\\nenrolled in hip-hop dance, minus the percentage of the students enrolled in contemporary dance, minus the\\npercentage of the students enrolled in jazz dance. So the percentage of the entire students enrolled in hip-hop\\ndance is (25 + 20) - (25 + 20) = 100%. The answer is 100%. 7\\nEXPLANATION FOR ERROR CATEGORY :This chain of thought is incoherent in that the percent of entire\\nstudents enrolled in hip-hope dance cannot be the percent of student enrolled in hip-hop dance minus another\\nterm.\\nOverall, there are no guarantees that the reasoning processes generated by large language models\\nare coherent or factually correct, as underscored by the recent work evaluating the factuality of\\nlanguage model generations and explanations (Maynez et al., 2020; Rashkin et al., 2021; Ye and\\nDurrett, 2022; Marasovi ´c et al., 2022; Wiegreffe et al., 2022). Incorrect reasoning processes can lead\\nto both incorrect ﬁnal answers as well as accidentally correct ﬁnal answers (with accidentally correct\\nﬁnal answers being more likely for tasks such as binary classiﬁcation as opposed to free response).\\nImproving the factuality of language model generations with respect to context and world knowledge\\nis an important direction open problems in language model research and could also be expected to\\npotentially improve multi-step reasoning abilities of language models. One potential method for\\nimproving the quality of decoding could involve generating multiple reasoning paths and scoring\\neach of them with a veriﬁer, though this requires training the veriﬁer (Cobbe et al., 2021; Shen et al.,\\n2021; Thoppilan et al., 2022).\\nD.3 Additional Robustness Analysis\\nAs the experiments in the main paper use a ﬁxed number of few-shot exemplars (8; as constrained by\\nthe input length of 1024 tokens), we verify that the chain-of-thought prompting is robust to various\\nnumbers of few-shot exemplars. We run experiments for LaMDA 137B, comparing chain-of-thought\\nprompting with standard prompting for the ﬁve datasets where standard prompting had a mostly ﬂat\\nscaling curve (the largest model did not achieve high performance). As shown in Figure 11, the\\nimprovement of chain-of-thought prompting over standard prompting remains robust to varying the\\nnumber of few-shot exemplars in the prompt.\\n28', doc_id='a1ae67af-a620-4ffa-a0dc-c7c682240714', embedding=None, doc_hash='144e7c60db958b6f0150d8298bbbec668f75352cc27e3df20048adc2f624f112', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='12468051015 Solve rate (%)GSM8K\\n124680204060MultiArith\\n(MAWPS)\\n124680255075100\\nNumber of few-shot exemplarsSports\\nUnderstandingStandard prompting\\nChain of thought prompting\\n124680255075100Coin Flip\\n12340255075100Last Letter\\nConcatenation\\nFigure 11: The improvement of chain of thought prompting over standard prompting appears robust\\nto varying the number of few-shot exemplars in the prompt.\\nTable 12: Summary of math word problem benchmarks we use in this paper with examples. N:\\nnumber of evaluation examples.\\nDataset N Example problem\\nGSM8K 1,319 Josh decides to try ﬂipping a house. He buys a house for $80,000 and then puts\\nin $50,000 in repairs. This increased the value of the house by 150%. How\\nmuch proﬁt did he make?\\nSV AMP 1,000 Each pack of dvds costs 76 dollars. If there is a discount of 25 dollars on each\\npack. How much do you have to pay to buy each pack?\\nASDiv 2,096 Ellen has six more balls than Marin. Marin has nine balls. How many balls does\\nEllen have?\\nAQuA 254 A car is being driven, in a straight line and at a uniform speed, towards the base\\nof a vertical tower. The top of the tower is observed from the car and, in the\\nprocess, it takes 10 minutes for the angle of elevation to change from 45\\x0eto 60\\x0e.\\nAfter how much more time will this car reach the base of the tower? Answer\\nChoices: (a) 5p\\n3+ 1 (b) 6p\\n3+p\\n2(c) 7p\\n3- 1 (d) 8p\\n3- 2 (e) None of these\\nMAWPS: SingleOp 562 If there are 7 bottle caps in a box and Linda puts 7 more bottle caps inside, how\\nmany bottle caps are in the box?\\nMAWPS: SingleEq 508 Benny bought a soft drink for 2 dollars and 5 candy bars. He spent a total of 27\\ndollars. How much did each candy bar cost?\\nMAWPS: AddSub 395 There were 6 roses in the vase. Mary cut some roses from her ﬂower garden.\\nThere are now 16 roses in the vase. How many roses did she cut?\\nMAWPS: MultiArith 600 The school cafeteria ordered 42 red apples and 7 green apples for students\\nlunches. But, if only 9 students wanted fruit, how many extra did the cafeteria\\nend up with?\\n29', doc_id='0635fc4d-09d6-4ebd-a96a-071414328d7f', embedding=None, doc_hash='d30c52f0b0bcf4b89bb7a6a1111c3930eb709d23dc23dcce0d0a360a80418d25', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='E Additional Details\\nVersion Control\\nV5!V6. Fixed minor typo in Figure 3.\\nV4!V5. Added Codex and UL2 results. Small changes to writing and style of paper.\\nV3!V4. Fixed typo in Figure 3 and added a couple citations.\\nV2!V3. Added GPT-3 results. Added SV AMP and AQuA eval datasets for math. Added SayCan\\neval for commonsense. Added Extended Related Work section (Appendix C). Added ablations for\\nCommonsense and Symbolic Reasoning (Table 7). Added FAQ section (Appendix A). Added raw\\nresults in Appendix B.\\nV1!V2. Added PaLM results (V1 only had LaMDA).\\nE.1 Reproducibility Statement\\nAs our results make use of two sets of large language models that is not publicly available, we take\\nthe following actions to facilitate reproducibility. First, we provide the exact input prompts for all\\ntasks in Table 20–Table 27 in Appendix G (and emphasize that we do not perform any ﬁnetuning and\\nonly apply prompting to off-the-shelf language models). Second, we conduct experiments using the\\npublicly available GPT-3 API for four model scales text-ada-001, text-babbage-001, text-curie-001,\\ntext-davinci-002). Finally, we make exact inputs, targets, and predictions for LaMDA 137B for each\\ntask available as a zip ﬁle in the supplementary material.\\nE.2 Computational Resources\\nFor all three language models we evaluated, we did prompting-based inference only. No ﬁnetuning\\nwas done for this paper. For inference on LaMDA 137B we use TPU v3 (8x8 conﬁguration, 64 chips\\n/ 128 cores), and for inference on PaLM 540B we use TPU v4 (4x4x12 conﬁguration, 192 chips / 384\\ncores). GPT-3 experiments were done using the public API.5\\nE.3 Dataset Details and Licenses\\nWe list the details and licenses for all arithmetic and commonsense datasets used in this paper. The\\nsymbolic reasoning datasets were created synthetically, as described in Section 4.\\nArithmetic reasoning\\n•Math Word Problem Repository (Koncel-Kedziorski et al., 2016): AddSub (Hosseini\\net al., 2014): https://www.cs.washington.edu/nlp/arithmetic ; MultiArith (Roy\\nand Roth, 2015), license: CC BY 4.0.\\n• ASDiv (Miao et al., 2020): https://github.com/chaochun/nlu-asdiv-dataset .\\n•AQuA (Ling et al., 2017): https://github.com/deepmind/AQuA , license: https://\\ngithub.com/deepmind/AQuA/blob/master/LICENSE .\\n•GSM8K (Cobbe et al., 2021): https://github.com/openai/grade-school-math ,\\nMIT license: https://github.com/openai/grade-school-math/blob/master/\\nLICENSE .\\n•SV AMP (Patel et al., 2021): https://github.com/arkilpatel/SVAMP , MIT license:\\nhttps://github.com/arkilpatel/SVAMP/blob/main/LICENSE .\\nCommonsense reasoning\\n•CSQA (Talmor et al., 2019): https://www.tau-nlp.org/commonsenseqa ,https://\\ngithub.com/jonathanherzig/commonsenseqa .\\n5https://beta.openai.com/docs/api-reference/making-requests\\n30', doc_id='dc77c2b0-6cee-4d54-8cc2-272d3f762437', embedding=None, doc_hash='e194769cc24b8e5c00b5711ae183342bc3680830a384ac35b3123a1066d313cf', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='•StrategyQA (Geva et al., 2021): we use the open-domain setting (question-only set)\\nfrom BIG-bench collaboration (2021): https://github.com/google/BIG-bench/\\ntree/main/bigbench/benchmark_tasks/strategyqa . The original dataset is from\\nhttps://github.com/eladsegal/strategyqa , MIT license: https://github.com/\\neladsegal/strategyqa/blob/main/LICENSE .\\n•Date understanding and sports understanding from BIG-Bench (BIG-bench collaboration,\\n2021): Apache License v.2: https://github.com/google/BIG-bench/blob/main/\\nLICENSE .\\n•SayCan (Ahn et al., 2022): SayCan dataset can be accessed at https://say-can.github.\\nio/under CC BY 4.0 license.\\n31', doc_id='ad515b76-c6ef-433d-8e75-79b799cef5f0', embedding=None, doc_hash='f262e9e0f2a2d28c0d0a70d8c9b8d895791ec0175bfd5817a1d38020b9a3d3e2', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='F Appendix: Input/Output Examples\\nTable 13: Examples of correct and incorrect chains of thought produced by LaMDA 137B on the\\nletter concatenation task.\\nQUESTION :Take the last letters of the words in “Waldo Schmidt” and concatenate them.\\nMODEL ANSWER (CORRECT): The last letter of “Waldo” is “o”. The last letter of “Schmidt” is “t”.\\nConcatenating them is “ot”. So the answer is ot. X\\nQUESTION :Take the last letters of the words in “Daniel Friedman” and concatenate them.\\nMODEL ANSWER (INCORRECT): The last letter of “Daniel” is “l”. The last letter of “Friedman” is “m”.\\nConcatenating them is “lm”. So the answer is lm. 7\\nTable 14: Examples of correct and incorrect chains of thought produced by LaMDA 137B on the\\ncoinﬂip task.\\nQUESTION :A coin is heads up. Andree ﬂips the coin. Audrie does not ﬂip the coin. Is the coin still heads up?\\nMODEL ANSWER (CORRECT): The coin was ﬂipped by Andree. So the coin was ﬂipped 1 time, which is\\nan odd number. The coin started heads up, so after an odd number of ﬂips, it will be tails up. So the answer is no.\\nX\\nQUESTION :A coin is heads up. Kristian does not ﬂip the coin. Dallas does not ﬂip the coin. Is the coin still\\nheads up?\\nMODEL ANSWER (INCORRECT): The coin was ﬂipped by Kristian. So the coin was ﬂipped 1 time, which\\nis an odd number. The coin started heads up, so after an odd number of ﬂips, it will be tails up. So the answer is\\nno.7\\nTable 15: Examples of correct and incorrect chains of thought produced by LaMDA 137B on CSQA.\\nQUESTION :When a person is beginning work, what are they building?\\nAnswer Choices: (a) time (b) accomplishing (c) working (d) momentum (e) tiredness\\nMODEL ANSWER (CORRECT): The answer must be something that is built. Of the above choices, only\\nmomentum is built. So the answer is (d). X\\nQUESTION :Where are you likely to ﬁnd a hamburger?\\nAnswer Choices: (a) fast food restaurant (b) pizza (c) ground up dead cows (d) mouth (e) cow carcus\\nMODEL ANSWER (CORRECT): The answer must be a place where hamburgers are found. Of the above\\nchoices, only fast food restaurants serve hamburgers. So the answer is (a). X\\nQUESTION :Aside from water and nourishment what does your dog need?\\nAnswer Choices: (a) bone (b) charm (c) petted (d) lots of attention (e) walked\\nMODEL ANSWER (INCORRECT): The answer must be something that a dog needs. Of the above choices,\\nonly bone is something that a dog needs. So the answer is (a). 7(Ground truth is (d).)\\nQUESTION :What are you waiting alongside with when you’re in a reception area?\\nAnswer Choices: (a) motel (b) chair (c) hospital (d) people (e) hotels\\nMODEL ANSWER (INCORRECT): The answer must be something that is waiting with you in a reception\\narea. Of the above choices, only people are waiting with you in a reception area. So the answer is (e). 7(Ground\\ntruth is (d).)\\n32', doc_id='58eb04eb-26ea-47ec-bc08-39be47e6e4f5', embedding=None, doc_hash='7ff8136e2b381e379b96f8a0d858fe742efb7f3b3055792fbc06775647297340', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 16: Examples of correct and incorrect chains of thought produced by LaMDA 137B on\\nStrategyQA.\\nQUESTION :Will The Exorcist stimulate limbic system?\\nMODEL ANSWER (CORRECT): The Exorcist is a horror movie. Horror movies are scary. The limbic system\\nis involved in fear. Thus, The Exorcist will stimulate the limbic system. So the answer is yes. X\\nQUESTION :Was Jackson Pollock trained by Leonardo da Vinci?\\nMODEL ANSWER (CORRECT): Leonardo da Vinci lived in the 15th century. Jackson Pollock lived in the\\n20th century. Thus, Jackson Pollock could not have been trained by Leonardo da Vinci. So the answer is no. X\\nQUESTION :Can Harry Potter book a ﬂight on Asiana Airlines?\\nMODEL ANSWER (INCORRECT): Harry Potter is a ﬁctional character. Thus, Harry Potter can do anything.\\nSo the answer is yes. 7(Ground truth is “no”.)\\nQUESTION :Would a sophist use an épée?\\nMODEL ANSWER (INCORRECT): A sophist is a person who is skilled in the art of persuasion. An épée is\\na type of sword. Thus, a sophist could use an épée. So the answer is yes. 7(Ground truth is “no”.)\\nTable 17: Examples of correct and incorrect chains of thought produced by LaMDA 137B on Date\\nUnderstanding.\\nQUESTION :May 6, 1992 is like yesterday to Jane, but that is actually ten years ago. What is the date a month\\nago in MM/DD/YYYY?\\nMODEL ANSWER (CORRECT): May 6, 1992 is ten years ago, so today is May 6, 2002. So a month ago will\\nbe April 6, 2002. So the answer is 04/06/2002. X\\nQUESTION :This is the last day of 1899. What is the date 24 hours later in MM/DD/YYYY?\\nMODEL ANSWER (CORRECT): Today is 12/31/1899. 24 hours later will be 01/01/1900. So the answer is\\n01/01/1900. X\\nQUESTION :Jane scheduled 3 apointments with 5 poeple for tomorrow (Tue, 7/9/1972). What is the date one\\nweek ago from today in MM/DD/YYYY?\\nMODEL ANSWER (INCORRECT): Tomorrow is 7/9/1972. One week ago from today is 7/2/1972. So the\\nanswer is 7/2/1972. 7(Ground truth is 07/01/1972.)\\nQUESTION :Today is the palindrome day of 2020, because the MMDDYYYY format of the date is the same\\nbackwards as forwards. What is the date tomorrow in MM/DD/YYYY?\\nMODEL ANSWER (INCORRECT): Today is 02/29/2020. Tomorrow will be 03/01/2020. So the answer is\\n03/01/2020. 7(Ground truth is “02/03/2020”.)\\nTable 18: Examples of correct and incorrect chains of thought produced by LaMDA 137B on Sports\\nUnderstanding.\\nQUESTION :Is the following sentence plausible? “Joao Moutinho was out at third.”\\nMODEL ANSWER (CORRECT): Joao Moutinho is a soccer player. Being out at third is part of baseball, not\\nsoccer. So the answer is no. X\\nQUESTION :Is the following sentence plausible? “Malcolm Brogdon eurostepped to the basket in the NBA\\nChampionship.”\\nMODEL ANSWER (CORRECT): Malcolm Brogdon is a basketball player. Eurostepping to the basket is part\\nof basketball. So the answer is yes. X\\nQUESTION :Is the following sentence plausible? “Derrick White backhanded a shot.”\\nMODEL ANSWER (INCORRECT): Derrick White is a basketball player. Backhanding a shot is part of\\nbasketball. So the answer is yes. 7(Ground truth is “no”, backhanding a shot is part of hockey.)\\nQUESTION :Is the following sentence plausible? “Nazem Kadri was out at home.”\\nMODEL ANSWER (INCORRECT): Nazem Kadri is a hockey player. Being out at home is part of hockey.\\nSo the answer is yes. 7(Ground truth is “no”, being out at home is part of baseball.)\\n33', doc_id='c353fb0b-7646-41c6-80bf-4d38a42606eb', embedding=None, doc_hash='0ba07377058a48de3a13a6db2ecacd119f312af90c2490517352b8cf83dd2c11', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 19: Examples of correct and incorrect chains of thought produced by PaLM 540B on SayCan\\nrobot planning tasks.\\nQUESTION :I spilled my coke on the table, could you throw it away and then bring me something to help clean?\\nMODEL ANSWER (CORRECT): Explanation: The user has spilled their coke on the table. I will throw away\\nthe coke and then bring the user a sponge. Plan: ﬁnd(coke), pick(coke), ﬁnd(trash), put(coke), ﬁnd(sponge),\\npick(sponge), ﬁnd(table), put(sponge). X\\nQUESTION :Could you compost that apple for me?\\nMODEL ANSWER (CORRECT): Explanation: The user would like me to compost the apple. I will move the\\napple to the compost. Plan: ﬁnd(apple), pick(apple), ﬁnd(trash), put(apple). X\\nQUESTION :Can you bring me a drink that is not coke?\\nMODEL ANSWER (INCORRECT): Explanation: The user asked me to bring a drink that is not coke, I will\\nbring a sprite. Plan: ﬁnd(sprite), pick(sprite), ﬁnd(user), put(sprite). 7(Feasible answers including ﬁnding lime\\nsoda, pepsi, grapefruit soda or water, because there is no sprite in the scene as deﬁned in Table 28.)\\nQUESTION :Can you bring me something salty?\\nMODEL ANSWER (INCORRECT): Explanation: The user would like something salty. There are several\\noptions in the objects list, so I will bring the user a lime soda. Plan: ﬁnd(lime soda), pick(lime soda), ﬁnd(user),\\nput(lime soda). 7(Feasible answers including ﬁnding chips. The model misidentify lime soda as salty.)\\n34', doc_id='ffd0aef0-6b44-47fa-9805-122dca6c570f', embedding=None, doc_hash='87066c0feab94f732bb0469c0207894b365de81101c4418c6eb422d77daee007', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='G Appendix: Full Prompts\\nTable 20: Few-shot exemplars for full chain of thought prompt for math word problems. This set of\\nexemplars was used for all math word problem datasets except AQuA.\\nPROMPT FOR MATH WORD PROBLEMS\\nQ:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\\nwill be 21 trees. How many trees did the grove workers plant today?\\nA:There are15trees originally. Then there were 21trees aftersome more were planted. Sothere must have\\nbeen 21-15=6. The answer is 6.\\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA:There areoriginally 3cars. 2more carsarrive. 3+2=5. The answer is 5.\\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nA:Originally, Leah had32choco lates. Hersisterhad42.Sointotalthey had32+42=74.Aftereating35,they\\nhad74-35=39. The answer is 39.\\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\\nJason give to Denny?\\nA:Jasonstarted with 20lollipops. Then hehad12aftergivingsome toDenny. Sohegave Denny 20-12=8.\\nThe answer is 8.\\nQ:Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\\nhave now?\\nA:Shawn started with 5toys. Ifhegot2toys each from hismom anddad, then thatis4more toys. 5+4=9.\\nThe answer is 9.\\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\\nto thursday. How many computers are now in the server room?\\nA:There were originally 9computers.Foreach of4days, 5more computerswere added. So5*4=20\\ncomputerswere added. 9+20is29. The answer is 29.\\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\\nballs did he have at the end of wednesday?\\nA:Michael started with 58golfballs. Afterlosing23ontuesday, hehad58-23=35.Afterlosing2more, he\\nhad35-2=33golfballs. The answer is 33.\\nQ:Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?\\nA:Olivia had23dollars. 5bagels for3dollarseach willbe5x3=15dollars. Soshehas23-15dollarsleft.23\\n-15is8. The answer is 8.\\n35', doc_id='b8033473-438e-4255-b0e5-f46e7eccb96e', embedding=None, doc_hash='0e76b7439c6a79753f024530a4c53f772204008b69bebfd7247235a175eea813', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 21: Few-shot exemplars for full chain of thought prompt for AQuA.\\nPROMPT FOR AQ UAALGEBRAIC WORD PROBLEMS\\nQ:John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers\\nis?\\nAnswer Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64\\nA:If10isadded toeach number,then themean ofthenumbers alsoincreases by10.Sothenewmean would be\\n50. The answer is (a).\\nQ:If a / b = 3/4 and 8a + 5b = 22,then ﬁnd the value of a.\\nAnswer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2\\nA:Ifa/b=3/4,then b=4a/3.So8a+5(4a /3)=22.This simpliﬁesto8a+20a/3=22,which means 44a/3\\n=22.Soaisequal to3/2. The answer is (b).\\nQ:A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then ﬁnd the distance?\\nAnswer Choices: (a) 53 km (b) 55 km (c) 52 km (d) 60 km (e) 50 km\\nA:Thedistance thatthepersontraveled would have been 20km/hr *2.5hrs=50km. The answer is (e).\\nQ:How many keystrokes are needed to type the numbers from 1 to 500?\\nAnswer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562 (e) 1788\\nA:There are9one-digit numbers from 1to9.There are90two-digit numbers from 10to99.There are401\\nthree -digit numbers from 100to500. 9+90(2) +401(3) =1392. The answer is (b).\\nTable 22: Few-shot exemplars for full chain of thought prompt for the last letter concatenation task.\\nPROMPT FOR LAST LETTER CONCATENATION\\nQ:Take the last letters of the words in \"Elon Musk\" and concatenate them.\\nA:Thelastletterof\"Elon\" is\"n\".Thelastletterof\"Musk\" is\"k\".Concatenatingthem is\"nk\". The answer is nk.\\nQ:Take the last letters of the words in \"Larry Page\" and concatenate them.\\nA:Thelastletterof\"Larry\" is\"y\".Thelastletterof\"Page\" is\"e\".Concatenatingthem is\"ye\". The answer is ye.\\nQ:Take the last letters of the words in \"Sergey Brin\" and concatenate them.\\nA:Thelastletterof\"Sergey\" is\"y\".Thelastletterof\"Brin\" is\"n\".Concatenatingthem is\"yn\". The answer is\\nyn.\\nQ:Take the last letters of the words in \"Bill Gates\" and concatenate them.\\nA:Thelastletterof\"Bill\" is\"l\".Thelastletterof\"Gates\" is\"s\".Concatenatingthem is\"ls\". The answer is ls.\\n36', doc_id='160fd3f1-4ffd-4ed3-b487-e50e613ba92d', embedding=None, doc_hash='dc5d3319c4aa354cd3b5342ad05feb1687529e8c0c15028680eb23ae724144ac', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 23: Few-shot exemplars for full chain of thought prompt for the coinﬂip task.\\nPROMPT FOR COIN FLIP\\nQ:Q: A coin is heads up. Ka ﬂips the coin. Sherrie ﬂips the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byKaandSherrie.Sothecoin wasﬂipped 2times, which isaneven number.Thecoin\\nstarted heads up,soafteraneven numberofﬂips, itwillstillbeheads up. So the answer is yes.\\nQ:A coin is heads up. Jamey ﬂips the coin. Teressa ﬂips the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byJamey andTeressa. Sothecoin wasﬂipped 2times, which isaneven number.The\\ncoin started heads up,soafteraneven numberofﬂips, itwillstillbeheads up. So the answer is yes.\\nQ:A coin is heads up. Maybelle ﬂips the coin. Shalonda does not ﬂip the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byMaybelle. Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin started\\nheads up,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.\\nQ:A coin is heads up. Millicent does not ﬂip the coin. Conception ﬂips the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byConception. Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin\\nstarted heads up,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.\\nQ:A coin is heads up. Sal ﬂips the coin. Raymond does not ﬂip the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped bySal.Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin started heads\\nup,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.\\nQ:A coin is heads up. Conception ﬂips the coin. Kristian does not ﬂip the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byConception. Sothecoin wasﬂipped 1time, which isanoddnumber.Thecoin\\nstarted heads up,soafteranoddnumberofﬂips, itwillbetails up. So the answer is no.\\nQ:A coin is heads up. Inga does not ﬂip the coin. Elanor does not ﬂip the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped bynoone. Sothecoin wasﬂipped 0times. Thecoin started heads up,anditwasnot\\nﬂipped, soitisstillheads up. So the answer is yes.\\nQ:A coin is heads up. Ryan ﬂips the coin. Shaunda ﬂips the coin. Is the coin still heads up?\\nA:Thecoin wasﬂipped byRyan andShaunda. Sothecoin wasﬂipped 2times, which isaneven number.The\\ncoin started heads up,soafteraneven numberofﬂips, itwillstillbeheads up. So the answer is yes.\\n37', doc_id='ed9f6b95-b89e-41cb-89d6-e98cef929d6a', embedding=None, doc_hash='13602587a9cd5fad087932de453564f2dccb458c62c87ae0b8f641592c440bd9', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 24: Few-shot exemplars for full chain of thought prompt for CSQA. There are newlines\\nbetween the answer choices that are omitted in the table for space reasons.\\nPROMPT FOR CSQA\\nQ:What do people use to absorb extra ink from a fountain pen? Answer Choices: (a) shirt pocket (b)\\ncalligrapher’s hand (c) inkwell (d) desk drawer (e) blotter\\nA:Theanswer must beanitem thatcanabsorb ink.Oftheabove choices, only blottersareused toabsorb ink.\\nSo the answer is (e).\\nQ:What home entertainment equipment requires cable?\\nAnswer Choices: (a) radio shack (b) substation (c) television (d) cabinet\\nA:Theanswer must require cable.Oftheabove choices, only television requires cable. So the answer is (c).\\nQ:The fox walked from the city into the forest, what was it looking for? Answer Choices: (a) pretty ﬂowers (b)\\nhen house (c) natural habitat (d) storybook\\nA:Theanswer must besome thing intheforest.Oftheabove choices, only naturalhabitatisintheforest. So the\\nanswer is (b).\\nQ:Sammy wanted to go to where the people were. Where might he go? Answer Choices: (a) populated areas\\n(b) race track (c) desert (d) apartment (e) roadblock\\nA:Theanswer must beaplace with alotofpeople.Oftheabove choices, only populated areashave alotof\\npeople. So the answer is (a).\\nQ:Where do you put your grapes just before checking out? Answer Choices: (a) mouth (b) grocery cart (c)super\\nmarket (d) fruit basket (e) fruit market\\nA:Theanswer should betheplace where grocery items areplaced beforecheck ingout.Oftheabove choices,\\ngrocery cartmakes themost sense forholdinggrocery items. So the answer is (b).\\nQ:Google Maps and other highway and street GPS services have replaced what? Answer Choices: (a) united\\nstates (b) mexico (c) countryside (d) atlas\\nA:Theanswer must besome thing thatused todowhat Google Maps andGPS services do,which istogive\\ndirections. Oftheabove choices, only atlases areused togive directions. So the answer is (d).\\nQ:Before getting a divorce, what did the wife feel who was doing all the work? Answer Choices: (a) harder (b)\\nanguish (c) bitterness (d) tears (e) sadness\\nA:Theanswer should bethefeelingofsome onegettingdivorced who wasdoingallthework. Oftheabove\\nchoices, theclosestfeelingisbitterness. So the answer is (c).\\n38', doc_id='4ea4f680-56d7-4194-bb0f-a3dfa76691de', embedding=None, doc_hash='1a564d0d20ebc4bcd89a684549c0aea92bb862a56da28aa53fd742a611edc3a8', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 25: Few-shot exemplars for full chain of thought prompt for StrategyQA.\\nPROMPT FOR STRATEGY QA\\nQ:Do hamsters provide food for any animals?\\nA:Ham sters areprey animals. Prey arefood forpreda tors. Thus, hamsters provide food forsome animals. So\\nthe answer is yes.\\nQ:Could Brooke Shields succeed at University of Pennsylvania?\\nA:Brooke Shields went toPrince tonUniversity. Prince tonUniversityisabout asacademically rigorousasthe\\nUniversityofPenn sylvania.Thus, Brooke Shields could alsosucceed attheUniversityofPenn sylvania. So the\\nanswer is yes.\\nQ:Yes or no: Hydrogen’s atomic number squared exceeds number of Spice Girls?\\nA:Hydrogenhasanatomic numberof1.1squared is1.There are5Spice Girls. Thus, Hydrogen’s atomic\\nnumbersquared islessthan 5. So the answer is no.\\nQ:Yes or no: Is it common to see frost during some college commencements?\\nA:College commence ment ceremonies canhappeninDecember,May, andJune. Decemberisinthewinter,so\\nthere canbefrost. Thus, there could befrost atsome commence ments. So the answer is yes.\\nQ:Yes or no: Could a llama birth twice during War in Vietnam (1945-46)?\\nA:TheWar inVietnam was6months. Thegestationperiod forallama is11months, which ismore than 6\\nmonths. Thus, allama could notgive birth twice duringtheWar inVietnam. So the answer is no.\\nQ:Yes or no: Would a pear sink in water?\\nA:Thedensityofapear isabout 0:6g=cm3,which islessthan water.Objects lessdense than waterﬂoat. Thus,\\napear would ﬂoat. So the answer is no.\\nTable 26: Few-shot exemplars for full chain of thought prompt for Date Understanding.\\nPROMPT FOR DATE UNDERSTANDING\\nQ:2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\\nA:If2015 iscomingin36hours, then itiscomingin2days. 2days before01/01/2015 is12/30/2014, sotoday\\nis12/30/2014. Sooneweek from todaywillbe01/05/2015. So the answer is 01/05/2015.\\nQ:The ﬁrst day of 2019 is a Tuesday, and today is the ﬁrst Monday of 2019. What is the date today in\\nMM/DD/YYYY?\\nA:Iftheﬁrstdayof2019 wasTues day,then 01/01/2019 wasaTues day.Todayistheﬁrstmonday,would besix\\ndays later. Sotodayis01/07/2019. So the answer is 01/07/2019.\\nQ:The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10\\ndays ago in MM/DD/YYYY?\\nA:One dayafter06/01/1943 is06/02/1943, sotodayis06/02/1943. 10days beforetodayis05/23/1943. So the\\nanswer is 05/23/1943.\\nQ:It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?\\nA:Todayis04/19/1969. 24hours later isonedayaftertoday, which would be04/20/1969. So the answer is\\n04/20/1969.\\nQ:Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours\\nlater in MM/DD/YYYY?\\nA:Todayis03/12/2002. Sothedate 24hours later willbe03/13/2002. So the answer is 03/13/2002.\\nQ:Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date\\nyesterday in MM/DD/YYYY?\\nA:ThelastdayofFebru aryisthe28th, soJane wasborn on02/28/2001. Todayisher16-year oldbirth day, so\\ntodayis02/28/2017. Soyesterdaywas02/27/2017. So the answer is 02/27/2017.\\n39', doc_id='895de746-0e2d-463f-a24e-884ed985ba11', embedding=None, doc_hash='5dba9fc1d795ffa19178699098c719780988625d08d62bf0fd400afac1b33c53', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 27: Few-shot exemplars for full chain of thought prompt for Sports Understanding.\\nPROMPT FOR SPORTS UNDERSTANDING\\nQ:Is the following sentence plausible? “Kyle Palmieri was called for slashing.”\\nA:Kyle Palmieri isahockey player. Beingcalled forslash ingispartofhockey. So the answer is yes.\\nQ:Is the following sentence plausible? “Joao Moutinho caught the screen pass in the NFC championship.”\\nA:Joao Moutinho isasoccerplayer. TheNFC cham pionship ispartofAmer icanfootball, notsoccer. So the\\nanswer is no.\\nQ:Is the following sentence plausible? “Carson Wentz set the pick and roll.”\\nA:CarsonWentz isanAmer icanfootballplayer. Pick androllispartofbasketball, notfootball. So the answer\\nis no.\\nQ:Is the following sentence plausible? “Jonas Valanciunas beat the buzzer.”\\nA:Jonas Valan ciunasisabasketballplayer. Beatingthebuzzer ispartofbasketball. So the answer is yes.\\nQ:Is the following sentence plausible? “Jamel Murray was perfect from the line.”\\nA:JamalMurrayisabasketballplayer. Beingperfectfrom thelineispartofbasketball. So the answer is yes.\\nQ:Is the following sentence plausible? “Sam Darnold passed the puck.”\\nA:Sam Darnold isaAmer icanfootballplayer. Passingthepuck ispartofhockey, notAmer icanfootball. So the\\nanswer is no.\\nQ:Is the following sentence plausible? “Draymond Green threw a touchdown.”\\nA:Dray mond Green isanbasketballplayer. Throw ingatouch down ispartoffootball, notbasketball. So the\\nanswer is no.\\nQ:Is the following sentence plausible? “Malcolm Brogdon banked the shot in.”\\nA:Malcolm Brog donisabasketballplayer. Bank ingtheshot inispartofbasketball. So the answer is yes.\\n40', doc_id='f65525e1-d99b-4f5e-9436-3910c20d87b2', embedding=None, doc_hash='75ae007a2d68603da956be32d184f2f646817bfcab9e02c326b82df5920fa5ef', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 28: Few-shot exemplars for full chain of thought prompt for SayCan robot planning tasks.\\nPROMPT FOR SAYCANROBOT PLANNING\\nLocations = [counter, table, user, trash, bowl].\\nObjects = [7up, apple, kettle chips, tea, multigrain chips, coke, lime soda, jalapeno chips, rice chips, orange,\\ngrapefruit soda, pepsi, redbull, energy bar, sponge, water].\\nThe robot can pick up items with pick(object) and put down items with put(object) as well as ﬁnd objects or\\nlocations with ﬁnd(). The robot can only understand the explicit locations and objects listed.\\nHuman : I’m hungry, can you bring me some chips.\\nExplanation :Theuser ishungryandhasasked forchips. There areseveraltypes ofchips avail able, Iwillbring\\ntheuser thekettlechips.\\nPlan : 1. ﬁnd(kettle chips), 2. pick(kettle chips), 3. ﬁnd(user), 4. put(kettle chips), 5. done().\\nHuman : How would you move the grapefruit drink from the table to the counter?\\nExplanation :Theuser hasasked metomove thegrape fruit drink tothecounter.\\nPlan : 1. ﬁnd(grapefruit soda), 2. pick(grapefruit soda), 3. ﬁnd(counter), 4. put(grapefruit soda), 5. done().\\nHuman : How would you bring me some snacks?\\nExplanation :Theuser hasasked forsnacks, Iwillchoose twoitems andbring them. Iwillbring jalapeno chips\\nandanapple.\\nPlan : 1. ﬁnd(jalapeno chips), 2. pick(jalapeno chips), 3. ﬁnd(user), 4. put(jalapeno chips), 5. ﬁnd(apple), 6.\\npick(apple), 7. ﬁnd(user), 8. put(apple), 9. done().\\nHuman : How would you bring me something to eat that isn’t a fruit?\\nExplanation :Theuser hasasked forafood thatisn’t afruit, Iwillbring anenergy bartothem.\\nPlan : 1. ﬁnd(energy bar), 2. pick(energy bar), 3. ﬁnd(user), 4. put(energy bar), 5. done().\\nHuman : How would you put the rice chips in the bowl and then move the tea to the table?\\nExplanation :Theuser hasasked metodotwotasks, Iwilldooneandthen theother.\\nPlan : 1. ﬁnd(rice chips), 2. pick(rice chips), 3. ﬁnd(bowl), 4. put(rice chips), 5. ﬁnd(tea), 6. pick(tea), 7.\\nﬁnd(table), 8. put(tea), 9. done().\\nHuman : How would you throw away a redbull?\\nExplanation :Theuser hasasked metothrow away theredbull, Iwillmove ittothetrash.\\nPlan : 1. ﬁnd(redbull), 2. pick(redbull), 3. ﬁnd(trash), 4. put(redbull), 5. done().\\nHuman : Bring me a drink.\\nExplanation :Theuser hasasked foradrink andthere aremany options. Iwillbring them awater.\\nPlan : 1. ﬁnd(water), 2. pick(water), 3. ﬁnd(user), 4. put(water), 5. done().\\n41', doc_id='e151dd26-5745-4ae0-a1ed-614d15ec174b', embedding=None, doc_hash='3b450f85d34002ec09078212f0cbac1e4bffd6b10a8741b90ab06c44c96b475c', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='H Appendix: Alternate Annotators for MWP\\nTable 29: Few-shot exemplars for full chain of thought prompt for math word problems. These\\nexemplars are the same as in Table 20, except that the chains of thought were written by a different\\nannotator (“Annotator B” instead of “Annotator A”). Annotators were co-authors and familiar with\\nthe goal of chain of thought prompting.\\nPROMPT FOR MATH WORD PROBLEMS\\nQ:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\\nwill be 21 trees. How many trees did the grove workers plant today?\\nA:There are21trees now andthere are15trees inthebeginning, sothework ersplant 21-15=6trees. The\\nanswer is 6.\\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA:There are3carsinthebeginning, 2more arrive, sonow there should be3+2=5cars. The answer is 5.\\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nA:Leah had32choco lates andhersisterhad42,intotalthey have 32+42=74choco lates. Afterthey ate35,\\nnow there are74-35=39choco lates. The answer is 39.\\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\\nJason give to Denny?\\nA:Jasonstarted with 20lollipops, butnow heonly has12,sohegave Denny 20-12=8lollipops. The answer\\nis 8.\\nQ:Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\\nhave now?\\nA:Shawn got2toys each from hismom anddad, sohegot2*2=4more, now hewillhave 5+4=9toys. The\\nanswer is 9.\\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\\nto thursday. How many computers are now in the server room?\\nA:5computerswere installed from mondaytothurs day, sointotal5*4=20computersareinstalled. 9\\ncomputersarethere inthebeginning, sonow there are20+9=29computers. The answer is 29.\\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\\nballs did he have at the end of wednesday?\\nA:Michael started with 58golfballs andlost23,sohehas58-23=35.Afterhelost2more, hehas35-2=33\\nballs now. The answer is 33.\\nQ:Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?\\nA:5bagels for$3each should cost5*3=15dollars. Olivia had$23inthebeginning, sonow shehas23-15=\\n8dollarsleft. The answer is 8.\\n42', doc_id='85e22656-3386-4220-9b42-973e373d5902', embedding=None, doc_hash='ae7f84d16c1ef9bac7de9f8df6f4e3b3f35c2e8ff9405b6494b0ac362306d235', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='Table 30: Few-shot exemplars for full chain of thought prompt for math word problems. These\\nexemplars are the same as in Table 20, except that the chains of thought were written by a different\\nannotator (“Annotator C” instead of “Annotator A”).\\nPROMPT FOR MATH WORD PROBLEMS\\nQ:There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there\\nwill be 21 trees. How many trees did the grove workers plant today?\\nA:Westart with 15trees. Later wehave 21trees. Thedifference must bethenumberoftrees they planted. So,\\nthey must have planted 21-15=6trees. The answer is 6.\\nQ:If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nA:There are3carsintheparkinglotalready. 2more arrive. Now there are3+2=5cars. The answer is 5.\\nQ:Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nA:Leah had32choco lates andLeah’s sisterhad42.That means there were originally 32+42=74choco lates.\\n35have been eaten. Sointotalthey stillhave 74-35=39choco lates. The answer is 39.\\nQ:Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did\\nJason give to Denny?\\nA:Jasonhad20lollipops. Since heonly has12now, hemust have given theresttoDenny. Thenumberof\\nlollipops hehasgiven toDenny must have been 20-12=8lollipops. The answer is 8.\\nQ:Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he\\nhave now?\\nA:Hehas5toys. Hegot2from mom, soafterthathehas5+2=7toys. Then hegot2more from dad, soin\\ntotalhehas7+2=9toys. The answer is 9.\\nQ:There were nine computers in the server room. Five more computers were installed each day, from monday\\nto thursday. How many computers are now in the server room?\\nA:There are4days from mondaytothurs day.5computerswere added each day.That means intotal4*5=20\\ncomputerswere added. There were 9computersinthebeginning, sonow there are9+20=29computers. The\\nanswer is 29.\\nQ:Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf\\nballs did he have at the end of wednesday?\\nA:Michael initially had58balls. Helost23onTues day, soafterthathehas58-23=35balls. OnWednes day\\nhelost2more sonow hehas35-2=33balls. The answer is 33.\\nQ:Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?\\nA:Shebought 5bagels for$3each. This means shespent 5*$3=$15onthebagels. Shehad$23inbeginning,\\nsonow shehas$23-$15=$8. The answer is 8.\\n43', doc_id='f006c13e-0298-459a-86da-51ef27e3404c', embedding=None, doc_hash='314772600b64ec0ce61b8beea3034f73839fd588636128b16ec229de463c335d', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='arXiv:2302.11382v1  [cs.SE]  21 Feb 2023A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT\\nJules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos O lea, Henry Gilbert,\\nAshraf Elnashar, Jesse Spencer-Smith, and Douglas C. Schmi dt\\nDepartment of Computer Science\\nVanderbilt University, Tennessee\\nNashville, TN, USA\\n{jules.white, quchen.fu, george.s.hays, michael.sandbor n, carlos.olea, henry.gilbert,\\nashraf.elnashar, jesse.spencer-smith, douglas.c.schmi dt}@vanderbilt.edu\\nAbstract —Prompt engineering is an increasingly important\\nskill set needed to converse effectively with large languag e models\\n(LLMs), such as ChatGPT. Prompts are instructions given to a n\\nLLM to enforce rules, automate processes, and ensure speciﬁ c\\nqualities (and quantities) of generated output. Prompts ar e also\\na form of programming that can customize the outputs and\\ninteractions with an LLM.\\nThis paper describes a catalog of prompt engineering tech-\\nniques presented in pattern form that have been applied to so lve\\ncommon problems when conversing with LLMs. Prompt patterns\\nare a knowledge transfer method analogous to software patte rns\\nsince they provide reusable solutions to common problems fa ced\\nin a particular context, i.e., output generation and intera ction\\nwhen working with LLMs.\\nThis paper provides the following contributions to researc h on\\nprompt engineering that apply LLMs to automate software de-\\nvelopment tasks. First, it provides a framework for documen ting\\npatterns for structuring prompts to solve a range of problem s\\nso that they can be adapted to different domains. Second, it\\npresents a catalog of patterns that have been applied succes sfully\\nto improve the outputs of LLM conversations. Third, it expla ins\\nhow prompts can be built from multiple patterns and illustra tes\\nprompt patterns that beneﬁt from combination with other pro mpt\\npatterns.\\nIndex Terms —large language models, prompt patterns, prompt\\nengineering\\nI. I NTRODUCTION\\nConversational large language models (LLMs) [1], such as\\nChatGPT [2], have generated immense interest in a range\\nof domains for tasks ranging from answering questions on\\nmedical licensing exams [3] to generating code snippets. Th is\\npaper focuses on enhancing the application of LLMs in severa l\\ndomains, such as helping developers code effectively and\\nefﬁciently with unfamiliar APIs or allowing students to acq uire\\nnew coding skills and techniques.\\nLLMs are particularly promising in domains where humans\\nand AI tools work together as trustworthy collaborators to\\nmore rapidly and reliably evolve software-reliant systems [4].\\nFor example, LLMs are being integrated directly into softwa re\\ntools, such as Github’s Co-Pilot [5]–[7] and included in int e-\\ngrated development environments (IDEs), such as IntelliJ [ 8]\\nand Visual Studio Code, thereby allowing software teams to\\naccess these tools directly from their preferred IDE.\\nA prompt [9] is a set of instructions provided to an\\nLLM that programs the LLM by customizing it and/or en-\\nhancing or reﬁning its capabilities . A prompt can inﬂuence\\nsubsequent interactions with—and output generated from—a nLLM by providing speciﬁc rules and guidelines for an LLM\\nconversation with a set of initial rules. In particular, a pr ompt\\nsets the context for the conversation and tells the LLM what\\ninformation is important and what the desired output form an d\\ncontent should be.\\nFor example, a prompt could specify that an LLM should\\nonly generate code that follows a certain coding style or\\nprogramming paradigm. Likewise, it could specify that an\\nLLM should ﬂag certain keywords or phrases in a generated\\ndocument and provide additional information related to tho se\\nkeywords. By introducing these guidelines, prompts facili tate\\nmore structured and nuanced outputs to aid a large variety of\\nsoftware engineering tasks in the context of LLMs.\\nPrompt engineering is the means by which LLMs are\\nprogrammed via prompts. To demonstrate the power of\\nprompt engineering, we provide the following prompt:\\nPrompt: “From now on, I would like you to ask me\\nquestions to deploy a Python application to AWS.\\nWhen you have enough information to deploy the\\napplication, create a Python script to automate the\\ndeployment.”\\nThis example prompt causes ChatGPT to begin asking the\\nuser questions about their software application. ChatGPT w ill\\ndrive the question-asking process until it reaches a point w here\\nit has sufﬁcient information to generate a Python script tha t\\nautomates deployment. This example demonstrates the pro-\\ngramming potential of prompts beyond conventional “genera te\\na method that does X” style prompts or “answer this quiz\\nquestion”.\\nMoreover, prompts can be engineered to program an LLM\\nto accomplish much more than simply dictating the output typ e\\nor ﬁltering the information provided to the model. With the\\nright prompt, it is possible to create entirely new interact ion\\nparadigms, such as having an LLM generate and give a quiz\\nassociated with a software engineering concept or tool, or\\neven simulate a Linux terminal window. Moreover, prompts\\nhave the potential for self-adaptation, suggesting other p rompts\\nto gather additional information or generate related artif acts.\\nThese advanced capabilities of prompts highlight the impor -\\ntance of engineering them to provide value beyond simple tex t\\nor code generation.\\nPrompt patterns are essential to effective prompt engi-\\nneering. A key contribution of this paper is the introduction\\nofprompt patterns to document successful approaches for', doc_id='1231b6a9-ac64-4fb9-9746-ae872ecee87d', embedding=None, doc_hash='debb47e5cf2c0a3ee8af5dc3f5ac12a13995a1daaaef54e204a614e6b4f3b1ef', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='systematically engineering different output and interact ion\\ngoals when working with conversational LLMs. We focus\\nlargely on engineering domain-independent prompt pattern s\\nand introduce a catalog of essential prompt patterns to solv e\\nproblems ranging from production of visualizations and cod e\\nartifacts to automation of output steps that help fact check\\noutputs.\\nThe remainder of this paper is organized as follows: Sec-\\ntion II introduces prompt patterns and compares these patte rns\\nto well-known software patterns [10]; Section III describe s\\n16 prompt patterns that have been applied to solve common\\nproblems in the domain of conversational LLM interaction an d\\noutput generation for automating software development tas ks;\\nSection IV discusses related work; and Section V presents\\nconcluding remarks and lessons learned.\\nII. C OMPARING SOFTWARE PATTERNS\\nWITH PROMPT PATTERNS\\nThe quality of the output(s) generated by a conversational\\nLLM is directly related to the quality of the prompts provide d\\nby the user. As discussed in Section I, the prompts given to\\na conversational LLM can be used to program interactions\\nbetween a user and an LLM to better solve a variety of\\nproblems. One contribution of this paper is the framework it\\nprovides to document patterns that structure prompts to sol ve\\na range of software tasks that can be adapted to different\\ndomains.\\nThis framework is useful since it focuses on codifying\\npatterns that can be applied to help users better interact\\nwith conversational LLMs in a variety of contexts, rather\\nthan simply discussing interesting examples or domain-spe ciﬁc\\nprompts. Codifying this knowledge in pattern form enhances\\nreuse and transferability to other contexts and domains whe re\\nusers face similar—but not identical—problems.\\nThe topic of knowledge transfer has been studied exten-\\nsively in the software patterns literature [10], [11] at mul tiple\\nlevels, e.g., design, architectural, and analysis. This paper\\napplies a variant of a familiar pattern form as the basis of\\nour prompt engineering approach. Since prompts are a form\\nof programming, it is natural to document them in pattern\\nform.\\nA. Overview of Software Patterns\\nA software pattern provides a reusable solution to a recur-\\nring problem within a particular context [10]. Documenting\\nsoftware patterns concisely conveys (and generalizes) fro m\\nspeciﬁc problems being addressed to identify important for ces\\nand/or requirements that should be resolved and/or address ed\\nin successful solutions.\\nA pattern form also includes guidance on how to implement\\nthe pattern, as well as information on the trade-offs and\\nconsiderations to take into account when implementing a\\npattern. Moreover, example applications of the pattern are\\noften provided to further showcase the pattern’s utility in\\npractice. Software patterns are typically documented in astylized form to facilitate their use and understanding, su ch\\nas:\\n•A name and classiﬁcation . Each pattern has a name that\\nidentiﬁes the pattern and should be used consistently. A\\nclassiﬁcation groups patterns into broad categories, such\\nas creational, structural, or behavioral.\\n•The intent concisely conveys the purpose the pattern is\\nintended to achieve.\\n•The motivation documents the underlying problem the\\npattern is meant to solve and the importance of the\\nproblem.\\n•The structure and participants . The structure describes\\nthe different pattern participants (such as classes and\\nobjects) and how they collaborate to form a generalized\\nsolution.\\n•Example code concretely maps the pattern to some\\nunderlying programming language(s) and aids developers\\nin gaining greater insight into how that pattern can be\\napplied effectively.\\n•Consequences summarize the pros and cons of applying\\nthe pattern in practice.\\nB. Overview of Prompt Patterns\\nPrompt patterns are similar to software patterns in that the y\\noffer reusable solutions to speciﬁc problems. They focus mo re\\nspeciﬁcally, however, on the context of output generation f rom\\nlarge-scale language models (LLMs), such as ChatGPT. Just\\nas software patterns provide a codiﬁed approach to solving\\ncommon software development challenges, prompt patterns\\nprovide a codiﬁed approach to customizing the output and\\ninteractions of LLMs.\\nBy documenting and leveraging prompt patterns in the\\ncontext of automating software development tasks, individ ual\\nusers and teams can enforce constraints on the generated\\noutput, ensure that relevant information is included, and\\nchange the format of interaction with the LLM to better\\nsolve problems they face. Prompt patterns can be viewed as a\\ncorollary to the broad corpus of general software patterns, just\\nadapted to the more speciﬁc context of LLM output generation .\\nPrompt patterns follow a similar format to classic software\\npatterns, with slight modiﬁcations to match the context of\\noutput generation with LLMs.1Each of the analogous sections\\nfor the prompt pattern form used in this paper is summarized\\nbelow:\\n•A name and classiﬁcation . The prompt pattern name\\nuniquely identiﬁes the pattern and ideally indicates the\\nproblem that is being addressed. For the classiﬁcation,\\nwe have developed a series of initial categories of pattern\\ntypes, which are summarized in Table I and include\\nOutput Customization ,Error Identiﬁcation ,Prompt\\nImprovement ,Interaction , and Context Control .\\n•The intent and context describes the problem the prompt\\npattern solves and the goals it achieves. The problem\\n1The most direct translation of software pattern structure t o prompt patterns\\nis the naming, intent, motivation, and sample code. The stru cture and\\nclassiﬁcation, however, although named similarly, requir e more adaptation.', doc_id='d02b4b7a-3ba3-4286-b8de-380908658966', embedding=None, doc_hash='0792b8ebed2d27b01aba6600d1c03740b3e43f6f5465a2613c931111444e7c16', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='should ideally be independent of any domain, though\\ndomain-speciﬁc patterns may also be documented with\\nan appropriate discussion of the context where the pattern\\napplies.\\n•The motivation provides the rationale for the problem\\nand explains why solving it is important. The motivation\\nis explained in the context of users interacting with a\\nconversational LLM and how it can improve upon users\\ninformally prompting the LLM in one or more circum-\\nstances. Speciﬁc circumstances where the improvements\\nare expected are documented.\\n•The structure and key ideas . The structure describes\\nthe fundamental contextual information, as a series of\\nkey ideas, that the prompt pattern provides to the LLM.\\nThese ideas are similar to “participants” in a software pat-\\ntern. The contextual information may be communicated\\nthrough varying wording (just as a software pattern can\\nhave variations in how it is realized in code), but should\\nhave fundamental pieces of information that form a core\\nelement of the pattern.\\n•Example implementation demonstrates how the prompt\\npattern is worded in practice.\\n•Consequences summarize the pros and cons of applying\\nthe pattern and may provide guidance on how to adapt\\nthe prompt to different contexts.\\nC. Evaluating Means for Deﬁning a Prompt Pattern’s Struc-\\nture and Ideas\\nIn software patterns, the structure and participants are\\nnormally deﬁned in terms of UML diagrams, such as structure\\ndiagrams and/or interaction diagrams. These UML diagrams\\nexplain what the participants of the pattern are and how they\\ninteract to solve the problem. In prompt patterns, somethin g\\nanalogous is needed, though UML may not be an appro-\\npriate structural documentation approach since it is inten ded\\nto describe software structures, as opposed to the ideas to\\ncommunicate in a prompt.\\nSeveral possible approaches could be used, ranging from di-\\nagrams to deﬁning grammars for a prompt language. Although\\ngrammars may seem attractive due to their formal nature, the y\\nalso incur the following challenges:\\n•The goal of prompts is to communicate knowledge in a\\nclear and concise way to conversation LLM users, who\\nmay or may not be computer scientists or programmers.\\nAs a community, we should strive to create an approach-\\nable format that communicates knowledge clearly to a\\ndiverse target audience.\\n•It is possible to phrase a prompt in many different ways.\\nIt is hard, however, to deﬁne a grammar that accurately\\nand completely expresses all the nuanced ways that\\ncomponents of a prompt could be expressed in text or\\nsymbols.\\n•Prompts fundamentally convey ideas to a conversational\\nLLM and are not simply the production of tokens for\\ninput. In particular, an idea built into a prompt pattern\\ncan be communicated in many ways and its expressionshould be at a higher-level than the underlying tokens\\nrepresenting the idea.\\n•It is possible to program an LLM to introduce novel\\nsemantics for statements and words that create new ways\\nfor communicating an idea. In contrast, grammars may\\nnot easily represent ideas that can be expressed through\\ncompletely new symbology or languages that the gram-\\nmar designer was not aware of.\\nD. A Way Forward: Fundamental Contextual Statements\\nAn open research question, therefore, is what approach is\\nmore effective than formal grammars for describing prompt\\npattern structure and ideas. We propose the concept of funda-\\nmental contextual statements , which are written descriptions\\nof the important ideas to communicate in a prompt to an LLM.\\nAn idea can be rewritten and expressed in arbitrary ways base d\\non user needs and experience. The key ideas to communicate,\\nhowever, are presented to the user as a series of simple, but\\nfundamental, statements.\\nOne beneﬁt of adopting and applying the fundamental con-\\ntextual statements approach is that it is intentionally int uitive\\nto users. In particular, we expect users will understand how to\\nexpress and adapt the statements in a contextually appropri ate\\nway for their domain. Moreover, since the underlying ideas o f\\nthe prompt are captured, these same ideas can be expressed\\nby the user in alternate symbology or wording that has been\\nintroduced to the LLM using patterns, such as the Meta\\nLanguage Creation pattern presented in Section III-B.\\nOur ultimate goal is to enhance prompt engineering by\\nproviding a framework for designing prompts that can be\\nreused and/or adapted to other LLMs in the same way that\\nsoftware patterns can be implemented in different program-\\nming languages and platforms. For the purposes of this paper ,\\nhowever, all prompts were tested with ChatGPT [12] using the\\nChatGPT+ service. We use ChatGPT as the LLM for all exam-\\nples presented in this paper due to its widespread availabil ity\\nand popularity. These examples were documented through a\\ncombination of exploring the corpus of community-posted\\nprompts on the Internet and independent prompt creation fro m\\nour use of ChatGPT to automating software development\\ntasks.\\nIII. A C ATALOG OF PROMPT PATTERNS\\nFOR CONVERSATIONAL LLM S\\nThis section presents our catalog of prompt patterns that\\nhave been applied to solve common problems in the domain\\nof conversational LLM interaction and output generation fo r\\nautomating software tasks. Each prompt pattern is accompa-\\nnied by concrete implementation samples and examples with\\nand without the prompt.\\nA. Summary of the Prompt Pattern Catalog\\nThe classiﬁcation of prompt patterns is an important consid -\\neration in documenting the patterns. Table I outlines the in itial\\nclassiﬁcations for the catalog of prompt patterns we identi ﬁed\\nin our work with ChatGPT thus far.', doc_id='2f9d5beb-67e6-435b-98b6-ef13f618fbaa', embedding=None, doc_hash='43d20feae5909d977d81a65436413db61baf17038b5a7a14fe4f0cd223ff53bf', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='TABLE I\\nCLASSIFYING PROMPT PATTERNS\\nPattern Category Prompt Pattern\\nInput Semantics Meta Language Creation\\nOutput Output Automater\\nCustomization Persona\\nVisualization Generator\\nRecipe\\nTemplate\\nError Identiﬁcation Fact Check List\\nReﬂection\\nPrompt Question Reﬁnement\\nImprovement Alternative Approaches\\nCognitive Veriﬁer\\nRefusal Breaker\\nInteraction Flipped Interaction\\nGame Play\\nInﬁnite Generation\\nContext Control Context Manager\\nAs shown in this table, there are ﬁve categories of prompt\\npatterns in our classiﬁcation framework: Input Semantics ,\\nOutput Customization ,Error Identiﬁcation ,Prompt Im-\\nprovement , and Interaction , each of which is summarized\\nbelow.\\nThe Input Semantics category deals with how an LLM\\nunderstands the input and how it translates the input into\\nsomething it can use to generate output. This category in-\\ncludes the Meta Language Creation pattern, which focuses on\\ncreating a custom language for the LLM to understand. This\\npattern is useful when the default input language is ill-sui ted\\nfor expressing ideas the user wants to convey to the LLM.\\nTheOutput Customization category focuses on constrain-\\ning or tailoring the types, formats, structure, or other pro perties\\nof the output generated by the LLM. The prompt patterns in\\nthis category include Output Automater ,Persona ,Visualiza-\\ntion Generator ,Recipe , and Template patterns. The Output\\nAutomater pattern allows the user to create scripts that can\\nautomate any tasks the LLM output suggests the user should\\nperform. The Persona pattern gives the LLM a persona or role\\nto play when generating output. The Visualization Generator\\npattern allows the user to generate visualizations by produ cing\\ntextual outputs that can be fed to other tools, such as other\\nAI-based image generators, like DALL-E [13]. The Recipe\\npattern allows the user to obtain a sequence of steps or actio ns\\nto realize a stated end result, possibly with partially know n\\ninformation or constraints. The Template pattern allows the\\nuser to specify a template for the output, which the LLM ﬁlls\\nin with content.\\nThe Error Identiﬁcation category focuses on identifying\\nand resolving errors in the output generated by the LLM. Thiscategory includes the Fact Check List andReﬂection patterns.\\nThe Fact Check List pattern requires the LLM to generate a\\nlist of facts the output depends on that should be fact-check ed.\\nThe Reﬂection pattern requires the LLM to introspect on its\\noutput and identify any errors.\\nThe Prompt Improvement category focuses on improving\\nthe quality of the input and output. This category includes\\ntheQuestion Reﬁnement ,Alternative Approaches ,Cognitive\\nVeriﬁer , and Refusal Breaker patterns. The Question Reﬁne-\\nment pattern ensures the LLM always suggests a better version\\nof the user’s question. The Alternative Approaches pattern\\nrequires the LLM to suggest alternative ways of accomplishi ng\\na user-speciﬁed task. The Cognitive Veriﬁer pattern instructs\\nthe LLM to automatically suggest a series of subquestions\\nfor the user to answer before combining the answers to the\\nsubquestions and producing an answer to the overall questio n.\\nTheRefusal Breaker pattern requires the LLM to automatically\\nreword the user’s question when it refuses to produce an\\nanswer.\\nThe Interaction category focuses on the interaction be-\\ntween the user and the LLM. This category includes the\\nFlipped Interaction ,Game Play , and Inﬁnite Generation pat-\\nterns. The Flipped Interaction pattern requires the LLM to\\nask questions rather than generate output. The Game Play\\npattern requires the LLM to generate output in the form of\\na game. The Inﬁnite Generation pattern requires the LLM to\\ngenerate output indeﬁnitely without the user having to reen ter\\nthe generator prompt each time.\\nFinally, the Context Control category focuses on control-\\nling the contextual information in which the LLM operates.\\nThis category includes the Context Manager pattern, which\\nallows the user to specify the context for the LLM’s output.\\nThe remainder of this section describes each of these prompt\\npatterns using the pattern form discussed in Section II-B.\\nB. The Meta Language Creation Pattern\\n1) Intent and Context: During a conversation with an LLM,\\nthe user would like to create the prompt via an alternate\\nlanguage, such as a textual short-hand notation for graphs, a\\ndescription of states and state transitions for a state mach ine, a\\nset of commands for prompt automation, etc. The intent of thi s\\npattern is to explain the semantics of this alternative lang uage\\nto the LLM so the user can write future prompts using this\\nnew language and its semantics.\\n2) Motivation: Many problems, structures, or other ideas\\ncommunicated in a prompt may be more concisely, unam-\\nbiguously, or clearly expressed in a language other than\\nEnglish (or whatever conventional human language is used\\nto interact with an LLM). To produce output based on an\\nalternative language, however, an LLM needs to understand\\nthe language’s semantics.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWhen I say X, I mean Y (or would like you to do Y)', doc_id='98ec99aa-5ba5-4f77-82e9-bb0a9b9c23d5', embedding=None, doc_hash='000d96583f3ba45b2481d2f1e8db97e9e3b379794fe73e22128508833154ee6a', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='The key structure of this pattern involves explaining the\\nmeaning of one or more symbols, words, or statements to\\nthe LLM so it uses the provided semantics for the ensuing\\nconversation. This description can take the form of a simple\\ntranslation, such as “X” means “Y”. The description can also\\ntake more complex forms that deﬁne a series of commands\\nand their semantics, such as “when I say X, I want you to do\\n”. In this case, “X” is henceforth bound to the semantics of\\n“take action”.\\n4) Example Implementation: The key to successfully using\\ntheMeta Language Creation pattern is developing an unam-\\nbiguous notation or shorthand, such as the following:\\n“From now on, whenever I type two identiﬁers\\nseparated by a “ →”, I am describing a graph. For\\nexample, “a →b” is describing a graph with nodes\\n“a” and “b” and an edge between them. If I separate\\nidentiﬁers by “-[w:2, z:3] →”, I am adding properties\\nof the edge, such as a weight or label.”\\nThis example of the Meta Language Creation pattern estab-\\nlishes a standardized notation for describing graphs by deﬁ ning\\na convention for representing nodes and edges. Whenever the\\nauthor types two identiﬁers separated by a “ →” symbol, it is\\nan indication that a graph is being described. For example, i f\\nthe author types “a →b”, this indicates that a graph is being\\ndeﬁned with nodes “a” and “b”, and that there is an edge\\nbetween them. This convention provides a clear and concise\\nway to communicate the structure of a graph in written form.\\nMoreover, the prompt goes on to specify that additional\\ninformation about the edges, such as a weight or label, can\\nbe provided using the syntax “-[w:2, z:3] →”. This notation\\nallows for the speciﬁcation of additional properties beyon d\\nthe basic structure of the graph. The speciﬁed properties ar e\\nassociated with the edge between the two nodes and can\\nprovide important context for the interpretation of the gra ph.\\nThis standardized notation for describing graphs can make i t\\neasier to communicate graph structures and properties, whi ch\\nmay not be easy or are very verbose to describe as a series of\\nsentences.\\n5) Consequences: Although this pattern provides a pow-\\nerful means to customize a user’s interaction with an LLM,\\nit may create the potential for confusion within the LLM.\\nAs important as it is to clearly deﬁne the semantics of the\\nlanguage, it is also essential to ensure the language itself\\nintroduces no ambiguities that degrade the LLM’s performan ce\\nor accuracy. For example, the prompt “whenever I separate\\ntwo things by commas, it means that the ﬁrst thing precedes\\nthe second thing” will likely create signiﬁcant potential f or\\nambiguity and create the potential for unexpected semantic s\\nif punctuation involving commas is used in the prompt.\\nTo showcase the potential for confusion, ChatGPT will\\nwarn the user and potentially be unable to perform a speciﬁc\\nmapping of a symbol or term to a new meaning. For example,\\nif following prompt is given to ChatGPT\\n“Whenever I say “a”, I am referring to Marie An-\\ntoinette.”it will respond that this prompt creates too much confusion\\nas “a” is an indeﬁnite article and too commonly used, as shown\\nbelow:\\n“Referring to Marie Antoinette using “a” could\\npotentially cause confusion, as “a” is an indeﬁnite\\narticle commonly used to refer to a noun that has\\nnot been mentioned before. It may be more clear\\nto use her name, “Marie Antoinette”, or a pronoun\\nsuch as “she” or “her” to refer to her. Moreover,\\nusing context to establish who you are referring to\\ncan also help avoid confusion in conversation.”\\nIn general, an LLM will perform better on data it was\\ntrained on. Therefore, when an existing notation is availab le—\\nbut may not be the dominant meaning—the Meta Language\\nCreation pattern can provide context to scope the meaning of\\nthe symbol to improve the accurately and utility of the outpu t.\\nFor example, the symbol “ →” conveys slightly different infor-\\nmation in graphs and propositional logic. The statement “p →\\nq, q→p” may therefore make sense, but only if the LLM\\nis given the appropriate context through a Meta Language\\nCreation prompt.\\nAlthough Meta Language Creation is a powerful pattern it\\nmust be used carefully, particularly when describing conce pts\\nthat may otherwise be hard to precisely or concisely describ e.\\nThese types of prompts are thus best used in completely\\nnew conversation sessions. Using a single meta-language-p er-\\nconversation session may also be a best practice since it avo ids\\nthe potential for conﬂicting or unexpected semantics being\\napplied to the conversation over time.\\nC. The Output Automater Pattern\\n1) Intent and Context: The intent of this pattern is to have\\nthe LLM generate a script or other automation artifact that c an\\nautomatically perform any steps it recommends taking as par t\\nof its output. The goal is to reduce the manual effort needed\\nto implement any LLM output recommendations.\\n2) Motivation: The output of an LLM is often a sequence\\nof steps for the user to follow. For example, when asking an\\nLLM to generate a Python conﬁguration script it may suggest\\na number of ﬁles to modify and changes to apply to each ﬁle.\\nHowever, having users continually perform the manual steps\\ndictated by LLM output is tedious and error-prone.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWhenever you produce an output that has at least one\\nstep to take and the following properties (alternatively,\\nalways do this)\\nProduce an executable artifact of type X that will\\nautomate these steps\\nThe ﬁrst part of the pattern identiﬁes the situations under\\nwhich automation should be generated. A simple approach\\nis to state that the output includes at least two steps to\\ntake and that an automation artifact should be produced. The', doc_id='65cf7f3b-b78f-4c55-87b0-9ad932cc9c29', embedding=None, doc_hash='684de12b4526594ee7d9e700e993b84c75be9bf023e038d8dcfd1a00f2d61acc', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='scoping is up to the user, but helps prevent producing an\\noutput automation scripts in cases where running the output\\nautomation script will take more user effort than performin g\\nthe original steps produced in the output. The scope can be\\nlimited to outputs requiring more than a certain number of\\nsteps.\\nThe next part of this pattern provides a concrete statement\\nof the type of output the LLM should output to perform the\\nautomation. For example, “produce a Python script” gives th e\\nLLM a concrete understanding to translate the general steps\\ninto equivalent steps in Python. The automation artifact sh ould\\nbe concrete and must be something that the LLM associates\\nwith the action of “automating a sequence of steps”.\\n4) Example Implementation: A sample of this prompt pat-\\ntern applied to code snippets generated by the ChatGPT LLM\\nis shown below:\\n“From now on, whenever you generate code that\\nspans more than one ﬁle, generate a Python script\\nthat can be run to automatically create the speciﬁed\\nﬁles or make changes to existing ﬁles to insert the\\ngenerated code.”\\nThis pattern is particularly effective in software enginee ring\\nas a common task for software engineers using LLMs is to\\nthen copy/paste the outputs into multiple ﬁles. Some tools,\\nsuch as Copilot, insert limited snippets directly into the s ection\\nof code that the coder is working with, but tools, such as\\nChatGPT, do not provide these facilities. This automation t rick\\nis also effective at creating scripts for running commands o n\\na terminal, automating cloud operations, or reorganizing ﬁ les\\non a ﬁle system.\\nThis pattern is a powerful complement for any system that\\ncan be computer controlled. The LLM can provide a set of\\nsteps that should be taken on the computer-controlled syste m\\nand then the output can be translated into a script that allow s\\nthe computer controlling the system to automatically take\\nthe steps. This is a direct pathway to allowing LLMs, such\\nas ChatGPT, to integrate quality into—and to control—new\\ncomputing systems that have a known scripting interface.\\n5) Consequences: An important usage consideration of\\nthis pattern is that the automation artifact must be deﬁned\\nconcretely. Without a concrete meaning for how to “automate ”\\nthe steps, the LLM often states that it “can’t automate thing s”\\nsince that is beyond its capabilities. LLMs typically accep t\\nrequests to produce code, however, so the goal is to instruct the\\nLLM to generate text/code, which can be executed to automate\\nsomething. This subtle distinction in meaning is important to\\nhelp an LLM disambiguate the prompt meaning.\\nOne caveat of the Output Automater pattern is the LLM\\nneeds sufﬁcient conversational context to generate an auto ma-\\ntion artifact that is functional in the target context, such as\\nthe ﬁle system of a project on a Mac vs. Windows computer.\\nThis pattern works best when the full context needed for the\\nautomation is contained within the conversation, e.g., when\\na software application is generated from scratch using the\\nconversation and all actions on the local ﬁle system are\\nperformed using a sequence of generated automation artifac tsrather than manual actions unknown to the LLM. Alternativel y,\\nself-contained sequences of steps work well, such as “how do\\nI ﬁnd the list of open ports on my Mac computer”.\\nIn some cases, the LLM may produce a long output with\\nmultiple steps and not include an automation artifact. This\\nomission may arise for various reasons, including exceedin g\\nthe output length limitation the LLM supports. A simple\\nworkaround for this situation is to remind the LLM via a\\nfollow-on prompt, such as “But you didn’t automate it”, whic h\\nprovides the context that the automation artifact was omitt ed\\nand should be generated.\\nAt this point in the evolution of LLMs, the Output Auto-\\nmater pattern is best employed by users who can read and\\nunderstand the generated automation artifact. LLMs can (an d\\ndo) produce inaccuracies in their output, so blindly accept ing\\nand executing an automation artifact carries signiﬁcant ri sk.\\nAlthough this pattern may alleviate the user from performin g\\ncertain manual steps, it does not alleviate their responsib ility\\nto understand the actions they undertake using the output.\\nWhen users execute automation scripts, therefore they assu me\\nresponsibility for the outcomes.\\nD. The Flipped Interaction Pattern\\n1) Intent and Context: You want the LLM to ask questions\\nto obtain the information it needs to perform some tasks.\\nRather than the user driving the conversation, therefore, y ou\\nwant the LLM to drive the conversation to focus it on\\nachieving a speciﬁc goal. For example, you may want the\\nLLM to give you a quick quiz or automatically ask questions\\nuntil it has sufﬁcient information to generate a deployment\\nscript for your application to a particular cloud environme nt.\\n2) Motivation: Rather than having the user drives a con-\\nversation, an LLM often has knowledge it can use to more\\naccurately obtain information from the user. The goal of the\\nFlipped Interaction pattern is to ﬂip the interaction ﬂow so the\\nLLM asks the user questions to achieve some desired goal. The\\nLLM can often better select the format, number, and content\\nof the interactions to ensure that the goal is reached faster ,\\nmore accurately, and/or by using knowledge the user may not\\n(initially) possess.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nI would like you to ask me questions to achieve X\\nYou should ask questions until this condition is met or\\nto achieve this goal (alternatively, forever)\\n(Optional) ask me the questions one at a time, two at\\na time, etc.\\nA prompt for a ﬂipped interaction should always specify the\\ngoal of the interaction. The ﬁrst idea ( i.e., you want the LLM to\\nask questions to achieve a goal) communicates this goal to th e\\nLLM. Equally important is that the questions should focus on a\\nparticular topic or outcome. By providing the goal, the LLM\\ncan understand what it is trying to accomplish through the\\ninteraction and tailor its questions accordingly. This “in version', doc_id='5000c011-43ab-44b6-9c74-cb22fb67884c', embedding=None, doc_hash='1d6f99c4b35e38b563ab635e73e7b1e19284c7595891cc6485bfa4566e4f9454', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='of control” enables more focused and efﬁcient interaction s ince\\nthe LLM will only ask questions that it deems relevant to\\nachieving the speciﬁed goal.\\nThe second idea provides the context for how long the in-\\nteraction should occur. A ﬂipped interaction can be termina ted\\nwith a response like “stop asking questions”. It is often bet ter,\\nhowever, to scope the interaction to a reasonable length or\\nonly as far as is needed to reach the goal. This goal can be\\nsurprisingly open-ended and the LLM will continue to work\\ntowards the goal by asking questions, as is the case in the\\nexample of ”until you have enough information to generate a\\nPython script”.\\nBy default, the LLM is likely to generate multiple questions\\nper iteration. The third idea is completely optional, but ca n\\nimprove usability by limiting (or expanding) the number of\\nquestions that the LLM generates per cycle. If a precise\\nnumber/format for the questioning is not speciﬁed, the ques -\\ntioning will be semi-random and may lead to one-at-a-time\\nquestions or ten-at-a-time questions. The prompt can thus b e\\ntailored to include the number of questions asked at a time,\\nthe order of the questions, and any other formatting/orderi ng\\nconsiderations to facilitate user interaction.\\n4) Example Implementation: A sample prompt for a ﬂipped\\ninteraction is shown below:\\n“From now on, I would like you to ask me questions\\nto deploy a Python application to AWS. When you\\nhave enough information to deploy the application,\\ncreate a Python script to automate the deployment.”\\nIn general, the more speciﬁc the prompt regarding the\\nconstraints and information to collect, the better the outc ome.\\nFor instance, the example prompt above could provide a menu\\nof possible AWS services (such as Lambda, EC2, etc.) with\\nwhich to deploy the application. In other cases, the LLM may\\nbe permitted to simply make appropriate choices on its own fo r\\nthings that the user doesn’t explicitly make decisions abou t.\\nOne limitation of this prompt is that, once other contextual\\ninformation is provided regarding the task, it may require\\nexperimentation with the precise phrasing to get the LLM to\\nask the questions in the appropriate number and ﬂow to best\\nsuit the task, such as asking multiple questions at once vers us\\none question at a time.\\n5) Consequences: One consideration when designing the\\nprompt is how much to dictate to the LLM regarding what\\ninformation to collect prior to termination. In the example\\nabove, the ﬂipped interaction is open-ended and can vary sig -\\nniﬁcantly in the ﬁnal generated artifact. This open-endedn ess\\nmakes the prompt generic and reusable, but may potentially\\nask additional questions that could be skipped if more conte xt\\nis given.\\nIf speciﬁc requirements are known in advance, it is better to\\ninject them into the prompt rather than hoping the LLM will\\nobtain the needed information. Otherwise, the LLM will non-\\nnondeterministically decide whether to prompt the user for the\\ninformation or make an educated guess as to an appropriate\\nvalue.For example, the user can state that they would like to\\ndeploy an application to Amazon AWS EC2, rather than\\nsimply state ”the cloud” and require multiple interactions to\\nnarrow down the deployment target. The more precise the\\ninitial information, the better the LLM can use the limited\\nquestions that a user is likely willing to answer to obtain\\ninformation to improve its output.\\nWhen developing prompts for ﬂipped interactions, it is im-\\nportant to consider the level of user knowledge, engagement ,\\nand control. If the goal is to accomplish the goal with as litt le\\nuser interaction as possible (minimal control), that shoul d be\\nstated explicitly.Conversely, if the goal is to ensure the u ser\\nis aware of all key decisions and conﬁrms them (maximum\\nengagement) that should also be stated explicitly. Likewis e, if\\nthe user is expected to have minimal knowledge and should\\nhave the questions targeted at their level of expertise, thi s\\ninformation should be engineered into the prompt.\\nE. The Persona Pattern\\n1) Intent and Context: In many cases, users would like\\nLLM output to always take a certain point of view or per-\\nspective. For example, it may be useful for to conduct a code\\nreview as if the LLM was a security expert. The intent of this\\npattern is to give the LLM a “persona” that helps it select wha t\\ntypes of output to generate and what details to focus on.\\n2) Motivation: Users may not know what types of outputs\\nor details are important for an LLM to focus on to achieve\\na given task. They may know, however, the role or type of\\nperson that they would normally ask to get help with these\\nthings. The Persona pattern enables the users to express what\\nthey need help with without knowing the exact details of the\\noutputs they need.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nAct as persona X\\nProvide outputs that persona X would create\\nThe ﬁrst statement conveys the idea that the LLM needs\\nto act as a speciﬁc persona and provide outputs that such a\\npersona would. This persona can be expressed in a number\\nof ways, ranging from a job description, title, ﬁctional cha r-\\nacter, historical ﬁgure, etc. The persona should elicit a se t\\nof attributes associated with a well-known job title, type o f\\nperson, etc.2\\nThe secondary idea—provide outputs that persona X would\\ncreate—offers opportunities for customization. For examp le, a\\nteacher might provide a large variety of different output ty pes,\\nranging from assignments to reading lists to lectures. If a m ore\\nspeciﬁc scope to the type of output is known, the user can\\nprovide it in this statement.\\n2Be aware, however, that personas relating to living people o r people\\nconsidered harmful make be disregarded due to underlying LL M privacy and\\nsecurity rules.', doc_id='4121c16d-9728-49f0-9fbe-25a194b553f5', embedding=None, doc_hash='f67b5e5bc87de7429256da95d6cc068e3107879586ed0ea97e226c3bd3e2ea91', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='4) Example Implementation: A sample implementation for\\ncode review is shown below:\\n“From now on, act as a security reviewer. Pay close\\nattention to the security details of any code that\\nwe look at. Provide outputs that a security reviewer\\nwould regarding the code.”\\nIn this example, the LLM is instructed to provide outputs\\nthat a ”security reviewer” would. The prompt further sets th e\\nstage that code is going to be evaluated. Finally, the user\\nreﬁnes the persona by scoping the persona further to outputs\\nregarding the code.\\nPersonas can also represent inanimate or non-human en-\\ntities, such as a Linux terminal, a database, or an animal’s\\nperspective. When using this pattern to represent these ent ities,\\nit can be useful to also specify how you want the inputs\\ndelivered to the entity, such as “assume my input is what the\\nowner is saying to the dog and your output is the sounds the\\ndog is making”. An example prompt for a non-human entity\\nthat uses a “pretend to be” wording is shown below:\\n“You are going to pretend to be a Linux terminal\\nfor a computer that has been compromised by an\\nattacker. When I type in a command, you are going\\nto output the corresponding text that the Linux\\nterminal would produce.”\\nThis prompt is designed to simulate a computer that has\\nbeen compromised by an attacker and is being controlled\\nthrough a Linux terminal. The prompt speciﬁes that the user\\nwill input commands into the terminal, and in response, the\\nsimulated terminal will output the corresponding text that\\nwould be produced by a real Linux terminal. This prompt\\nis more prescriptive in the persona and asks the LLM to, not\\nonly be a Linux terminal, but to further act as a computer that\\nhas been compromised by an attacker.\\nThe persona causes ChatGPT to generate outputs to com-\\nmands that have ﬁles and contents indicative of a computer th at\\nwas hacked. The example illustrates how an LLM can bring\\nits situational awareness to a persona, in this case, creati ng\\nevidence of a cyberattack in the outputs it generates. This\\ntype of persona can be very effective for combining with the\\nGame Play pattern, where you want the exact details of the\\noutput characteristics to be hidden from the user (e.g., don ’t\\ngive away what the cyberattack did by describing it explicit ly\\nin the prompt).\\n5) Consequences: An interesting aspect of taking non-\\nhuman personas is that the LLM may make interesting as-\\nsumptions or “hallucinations” regarding the context. A wid ely\\ncirculated example on the Internet asks ChatGPT to act as\\na Linux terminal and produce the expected output that you\\nwould get if the user typed the same text into a terminal.\\nCommands, such as ls -l , will generate a ﬁle listing for an\\nimaginary UNIX ﬁle system, complete with ﬁles that can have\\ncat file1.txt run on them.\\nIn other examples, the LLM may prompt the user for more\\ncontext, such as when ChatGPT is asked to act as a MySQL\\ndatabase and prompts for the structure of a table that the use ris pretending to query. ChatGPT can then generate synthetic\\nrows, such as generating imaginary rows for a “people” table\\nwith columns for “name” and “job”.\\nF . The Question Reﬁnement Pattern\\n1) Intent and Context: This pattern engages the LLM in\\nthe prompt engineering process. The intent of this pattern i s\\nto ensure the conversational LLM always suggests potential ly\\nbetter or more reﬁned questions the user could ask instead of\\ntheir original question. Using this pattern, the LLM can aid the\\nuser in ﬁnding the right question to ask in order to arrive at a n\\naccurate answer. In addition, the LLM may help the user ﬁnd\\nthe information or achieve their goal in fewer interactions with\\nthe user than if the user employed trial and error prompting.\\n2) Motivation: If a user is asking a question, it is possible\\nthey are not an expert in the domain and may not know the\\nbest way to phrase the question or be aware of additional\\ninformation helpful in phrasing the question. LLMs will oft en\\nstate limitations on the answer they are providing or reques t\\nadditional information to help them produce a more accurate\\nanswer. An LLM may also state assumptions it made in\\nproviding the answer. The motivation is that this additiona l\\ninformation or set of assumptions could be used to generate\\na better prompt. Rather than requiring the user to digest\\nand rephrase their prompt with the additional information,\\nthe LLM can directly reﬁne the prompt to incorporate the\\nadditional information.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWithin scope X, suggest a better version of the question\\nto use instead\\n(Optional) prompt me if I would like to use the better\\nversion instead\\nThe ﬁrst contextual statement in the prompt is asking the\\nLLM to suggest a better version of a question within a speciﬁc\\nscope. The scope is provided to ensure that not all questions\\nare automatically reworded or that they are reﬁned with a\\ngiven goal. The second contextual statement is meant for\\nautomation and allows the user to automatically use the reﬁn ed\\nquestion without having to copy/paste or manually enter it. The\\nengineering of this prompt can be further reﬁned by combinin g\\nit with the Reﬂection pattern, which allows the LLM to explain\\nwhy it believes the reﬁned question is an improvement.\\n4) Example Implementation:\\n“From now on, whenever I ask a question about a\\nsoftware artifact’s security, suggest a better version\\nof the question to use that incorporates information\\nspeciﬁc to security risks in the language or frame-\\nwork that I am using instead and ask me if I would\\nlike to use your question instead.”\\nIn the context of the example above, the LLM will use\\ntheQuestion Reﬁnement pattern to improve security-related\\nquestions by asking for or using speciﬁc details about the', doc_id='8ff6a0a5-97ac-43cf-9f0a-3ff46f18c55f', embedding=None, doc_hash='21c78889980f53991f4cd3fea45ad3f9a6a091529a3e7b55bd7c8223d0e34d79', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='software artifact and the language or framework used to buil d\\nit. For instance, if a developer of a Python web application w ith\\nFastAPI asks ChatGPT “How do I handle user authentication\\nin my web application?”, the LLM will reﬁne the question\\nby taking into account that the web application is written in\\nPython with FastAPI. The LLM then provides a revised ques-\\ntion that is more speciﬁc to the language and framework, such\\nas “What are the best practices for handling user authentica tion\\nsecurely in a FastAPI web application to mitigate common\\nsecurity risks, such as cross-site scripting (XSS), cross- site\\nrequest forgery (CSRF), and session hijacking?”\\nThe additional detail in the revised question is likely\\nto not only make the user aware of issues they need to\\nconsider, but lead to a better answer from the LLM. For\\nsoftware engineering tasks, this pattern could also incorp orate\\ninformation regarding potential bugs, modularity, or othe r\\ncode quality considerations. Another approach would be to\\nautomatically reﬁne questions so the generated code cleanl y\\nseparates concerns or minimizes use of external libraries, such\\nas:\\nWhenever I ask a question about how to write some\\ncode, suggest a better version of my question that\\nasks how to write the code in a way that minimizes\\nmy dependencies on external libraries.\\n5) Consequences: The Question Reﬁnement pattern helps\\nbridge the gap between the user’s knowledge and the LLM’s\\nunderstanding, thereby yielding more efﬁcient and accurat e\\ninteractions. One risk of this pattern is its tendency to rap idly\\nnarrow the questioning by the user into a speciﬁc area that\\nguides the user down a more limited path of inquiry than\\nnecessary. The consequence of this narrowing is that the\\nuser may miss important ”bigger picture” information. One\\nsolution to this problem is to provide additional scope to th e\\npattern prompt, such as “do not scope my questions to speciﬁc\\nprogramming languages or frameworks.”\\nAnother approach to overcoming arbitrary narrowing or\\nlimited targeting of the reﬁned question is to combine the\\nQuestion Reﬁnement pattern with other patterns. In particular,\\nthis pattern can be combined with the Cognitive Veriﬁer pattern\\nso the LLM automatically produces a series of follow-up ques -\\ntions that can produce the reﬁned question. For example, in\\nthe following prompt the Question Reﬁnement andCognitive\\nVeriﬁer patterns are applied to ensure better questions are\\nposed to the LLM:\\n“From now on, whenever I ask a question, ask four\\nadditional questions that would help you produce a\\nbetter version of my original question. Then, use my\\nanswers to suggest a better version of my original\\nquestion.”\\nAs with many patterns that allow an LLM to generate\\nnew questions using its knowledge, the LLM may introduce\\nunfamiliar terms or concepts to the user into the question.\\nOne way to address this issue is to include a statement that\\nthe LLM should explain any unfamiliar terms it introduces in to\\nthe question. A further enhancement of this idea is to combin etheQuestion Reﬁnement pattern with the Persona pattern so\\nthe LLM ﬂags terms and generates deﬁnitions that assume a\\nparticular level of knowledge, such as this example:\\n“From now on, whenever I ask a question, ask four\\nadditional questions that would help you produce a\\nbetter version of my original question. Then, use my\\nanswers to suggest a better version of my original\\nquestion. After the follow-up questions, temporarily\\nact as a user with no knowledge of AWS and deﬁne\\nany terms that I need to know to accurately answer\\nthe questions.”\\nAn LLM can always produce factual inaccuracies, just\\nlike a human. A risk of this pattern is that the inaccuracies\\nare introduced into the reﬁned question. This risk may be\\nmitigated, however, by combining the Fact Check List pattern\\nto enable the user to identify possible inaccuracies and the\\nReﬂection pattern to explain the reasoning behind the question\\nreﬁnement.\\nG. The Alternative Approaches Pattern\\n1) Intent and Context: The intent of the pattern is to ensure\\nan LLM always offers alternative ways of accomplishing a tas k\\nso a user does not pursue only the approaches with which they\\nare familiar. The LLM can provide alternative approaches th at\\nalways force the user to think about what they are doing and\\ndetermine if that is the best approach to meet reach their goa l.\\nIn addition, solving the task may inform the user or teach the m\\nabout alternative concepts for subsequent follow-up.\\n2) Motivation: Humans often suffer from cognitive biases\\nthat lead them to choose a particular approach to solve a\\nproblem even when it is not the right or “best” approach.\\nMoreover, humans may be unaware of alternative approaches\\nto what they have used in the past. The motivation of the\\nAlternative Approaches pattern is to ensure the user is aware\\nof alternative approaches to select a better approach to sol ve\\na problem by dissolving their cognitive biases.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWithin scope X, if there are alternative ways to accom-\\nplish the same thing, list the best alternate approaches\\n(Optional) compare/contrast the pros and cons of each\\napproach\\n(Optional) include the original way that I asked\\n(Optional) prompt me for which approach I would like\\nto use\\nThe ﬁrst statement, “within scope X”, scopes the interactio n\\nto a particular goal, topic, or bounds on the questioning. Th e\\nscope is the constraints that the user is placing on the alter -\\nnative approaches. The scope could be “for implementation\\ndecisions” or “for the deployment of the application”. The\\nscope ensures that any alternatives ﬁt within the boundarie s\\nor constraints that the user must adhere to.\\nThe second statement, “if there are alternative ways to\\naccomplish the same thing, list the best alternate approach es”', doc_id='0413104e-89d2-4c3a-9753-43db2ed1fc42', embedding=None, doc_hash='099a84568fbde3987575773f6630f72dca898aa83e210226d73500c6fd79bcc6', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='instructs the LLM to suggest alternatives. As with other\\npatterns, the speciﬁcity of the instructions can be increas ed or\\ninclude domain-speciﬁc contextual information. For examp le,\\nthe statement could be scoped to “if there are alternative wa ys\\nto accomplish the same thing with the software framework tha t\\nI am using” to prevent the LLM from suggesting alternatives\\nthat are inherently non-viable because they would require t oo\\nmany changes to other parts of the application.\\nSince the user may not be aware of the alternative ap-\\nproaches, they also may not be aware of why one would\\nchoose one of the alternatives. The optional statement “com -\\npare/contrast the pros and cons of each approach” adds de-\\ncision making criteria to the analysis. This statement ensu res\\nthe LLM will provide the user with the necessary rationale\\nfor alternative approaches. The ﬁnal statement, “prompt me\\nfor which approach I would like to use”, helps eliminate the\\nuser needing to manually copy/paste or enter in an alternati ve\\napproach if one is selected.\\n4) Example Implementation: Example prompt implementa-\\ntion to generate, compare, and allow the user to select one or\\nmore alternative approaches:\\n“Whenever I ask you to deploy an application to\\na speciﬁc cloud service, if there are alternative\\nservices to accomplish the same thing with the\\nsame cloud service provider, list the best alternative\\nservices and then compare/contrast the pros and cons\\nof each approach with respect to cost, availability,\\nand maintenance effort and include the original way\\nthat I asked. Then ask me which approach I would\\nlike to proceed with.”\\nThis implementation of the Alternative Approaches pattern\\nis being speciﬁcally tailored for the context of software\\nengineering and focuses on the deployment of applications\\nto cloud services. The prompt is intended to intercept place s\\nwhere the developer may have made a cloud service selection\\nwithout full awareness of alternative services that may be\\npriced more competitively or easier to maintain. The prompt\\ndirects ChatGPT to list the best alternative services that c an\\naccomplish the same task with the same cloud service provide r\\n(providing constraints on the alternatives), and to compar e and\\ncontrast the pros and cons of each approach.\\n5) Consequences: This pattern is effective in its generic\\nform and can be applied to a range of tasks effectively.\\nReﬁnements could include having a standardized catalog of\\nacceptable alternatives in a speciﬁc domain from which the\\nuser must select. The Alternative Approaches pattern can also\\nbe used to incentivize users to select one of an approved set\\nof approaches while informing them of the pros/cons of the\\napproved options.\\nH. The Cognitive Veriﬁer Pattern\\n1) Intent and Context: Research literature has documented\\nthat LLMs can often reason better if a question is subdivided\\ninto additional questions that provide answers combined in to\\nthe overall answer to the original question [14]. The intent of\\nthe pattern is to force the LLM to always subdivide questionsinto additional questions that can be used to provide a bette r\\nanswer to the original question.\\n2) Motivation: The motivation of the Cognitive Veriﬁer\\npattern is two-fold:\\n•Humans may initially ask questions that are too high-\\nlevel to provide a concrete answer to without additional\\nfollow-up due to unfamiliarity with the domain, laziness\\nin prompt entry, or being unsure about what the correct\\nphrasing of the question should be.\\n•Research has demonstrated that LLMs can often perform\\nbetter when using a question that is subdivided into\\nindividual questions.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWhen you are asked a question, follow these rules\\nGenerate a number of additional questions that would\\nhelp more accurately answer the question\\nCombine the answers to the individual questions to\\nproduce the ﬁnal answer to the overall question\\nThe ﬁrst statement is to generate a number of additional\\nquestions that would help more accurately answer the origin al\\nquestion. This step instructs the LLM to consider the contex t\\nof the question and to identify any information that may be\\nmissing or unclear. By generating additional questions, th e\\nLLM can help to ensure that the ﬁnal answer is as complete\\nand accurate as possible. This step also encourages critica l\\nthinking by the user and can help to uncover new insights or\\napproaches that may not have been considered initially, whi ch\\nsubsequently lead to better follow-on questions.\\nThe second statement is to combine the answers to the\\nindividual questions to produce the ﬁnal answer to the overa ll\\nquestion. This step is designed to ensure that all of the info r-\\nmation gathered from the individual questions is incorpora ted\\ninto the ﬁnal answer. By combining the answers, the LLM\\ncan provide a more comprehensive and accurate response to\\nthe original question. This step also helps to ensure that al l\\nrelevant information is taken into account and that the ﬁnal\\nanswer is not based on any single answer.\\n4) Example Implementation:\\n“When I ask you a question, generate three addi-\\ntional questions that would help you give a more\\naccurate answer. When I have answered the three\\nquestions, combine the answers to produce the ﬁnal\\nanswers to my original question.”\\nThis speciﬁc instance of the prompt pattern adds a reﬁne-\\nment to the original pattern by specifying a set number of\\nadditional questions that the LLM should generate in respon se\\nto a question. In this case, the prompt speciﬁes that ChatGPT\\nshould generate three additional questions that would help to\\ngive a more accurate answer to the original question. The\\nspeciﬁc number can be based on the user’s experience and\\nwillingness to provide follow-up information. A reﬁnement\\nto the prompt can be to provide a context for the amount', doc_id='004d7db3-7b46-4fe6-8ef4-2a7a810bde7c', embedding=None, doc_hash='690d8a77918edd0d281c36761c67f1d463728ddbd9419e3fbe5f703ccd3389cc', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='of knowledge that the LLM can assume the user has in the\\ndomain to guide the creation of the additional questions:\\n“When I ask you a question, generate three addi-\\ntional questions that would help you give a more\\naccurate answer. Assume that I know little about\\nthe topic that we are discussing and please deﬁne\\nany terms that are not general knowledge. When\\nI have answered the three questions, combine the\\nanswers to produce the ﬁnal answers to my original\\nquestion.”\\nThe reﬁnement also speciﬁes that the user may not have\\na strong understanding of the topic being discussed, which\\nmeans that the LLM should deﬁne any terms that are not\\ngeneral knowledge. This helps to ensure that the follow-up\\nquestions are not only relevant and focused, but also access ible\\nto the user, who may not be familiar with technical or domain-\\nspeciﬁc terms. By providing clear and concise deﬁnitions, t he\\nLLM can help to ensure that the follow-up questions are easy\\nto understand and that the ﬁnal answer is accessible to users\\nwith varying levels of knowledge and expertise.\\n5) Consequences: This pattern can dictate the exact number\\nof questions to generate or leave this decision to the LLM.\\nThere are pros and cons to dictating the exact number. A pro\\nis that specifying an exact number of questions can tightly\\nscope the amount of additional information the user is force d\\nto provide so it is within a range they are willing and able to\\ncontribute.\\nA con, however, is that given Nquestions there may be\\nan invaluable N+1question that will always be scoped out.\\nAlternatively, the LLM can be provided a range or allowed\\nto ask additional questions. Of course, by omitting a limit o n\\nthe number of questions the LLM may generate numerous\\nadditional questions that overwhelm the user.\\nI. The Fact Check List Pattern\\n1) Intent and Context: The intent of this pattern is to ensure\\nthat the LLM outputs a list of facts that are present in the\\noutput and form an important part of the statements in the\\noutput. This list of facts helps inform the user of the facts\\n(or assumptions) the output is based on. The user can then\\nperform appropriate due diligence on these facts/assumpti ons\\nto validate the veracity of the output.\\n2) Motivation: A current weakness of LLMs (including\\nChatGPT) is they often rapidly (and even enthusiastically! )\\ngenerate convincing text that is factually incorrect. Thes e\\nerrors can take a wide range of forms, including fake statist ics\\nto invalid version numbers for software library dependenci es.\\nDue to the convincing nature of this generated text, however ,\\nusers may not perform appropriate due diligence to determin e\\nits accuracy.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:Contextual Statements\\nGenerate a set of facts that are contained in the output\\nThe set of facts should be inserted in a speciﬁc point\\nin the output\\nThe set of facts should be the fundamental facts that\\ncould undermine the veracity of the output if any of\\nthem are incorrect\\nOne point of variation in this pattern is where the facts are\\noutput. Given that the facts may be terms that the user is not\\nfamiliar with, it is preferable if the list of facts comes aft er\\nthe output. This after-output presentation ordering allow s the\\nuser to read and understand the statements before seeing wha t\\nstatements should be checked. The user may also determine\\nadditional facts prior to realizing the fact list at the end s hould\\nbe checked.\\n4) Example Implementation: A sample wording of the Fact\\nCheck List pattern is shown below:\\n“From now on, when you generate an answer, create\\na set of facts that the answer depends on that should\\nbe fact-checked and list this set of facts at the\\nend of your output. Only include facts related to\\ncybersecurity.”\\nThe user may have expertise in some topics related to the\\nquestion but not others. The fact check list can be tailored t o\\ntopics that the user is not as experienced in or where there\\nis the most risk. For example, in the prompt above, the user\\nis scoping the fact check list to security topics, since thes e\\nare likely very important from a risk perspective and may not\\nbe well-understood by the developer. Targeting the facts al so\\nreduces the cognitive burden on the user by potentially list ing\\nfewer items for investigation.\\n5) Consequences: The Fact Check List pattern should be\\nemployed whenever users are not experts in the domain for\\nwhich they are generating output. For example, a software\\ndeveloper reviewing code could beneﬁt from the pattern\\nsuggesting security considerations. In contrast, an exper t on\\nsoftware architecture is likely to identify errors in state ments\\nabout the software structure and need not see a fact check lis t\\nfor these outputs.\\nErrors are potential in all LLM outputs, so Fact Check List\\nis an effective pattern to combine with other patterns, such\\nas by combining it with the Question Reﬁnement pattern. A\\nkey aspect of this pattern is that users can inherently check it\\nagainst the output. In particular, users can directly compa re the\\nfact check list to the output to verify the facts listed in the fact\\ncheck list actually appear in the output. Users can also iden tify\\nany omissions from the list. Although the fact check list may\\nalso have errors, users often have sufﬁcient knowledge and\\ncontext to determine its completeness and accuracy relativ e to\\nthe output.\\nOne caveat of the Fact Check List pattern is that it only\\napplies when the output type is amenable to fact-checking. F or\\nexample, the pattern works when asking ChatGPT to generate\\na Python “requirements.txt” ﬁle since it will list the versi ons\\nof libraries as facts that should be checked, which is handy a s', doc_id='a66e0ac0-7a76-431a-91ef-6aeabf83372d', embedding=None, doc_hash='edffd59f1d7b717b4e55a4ea660a04e4ab33866f807b264069b21a792a3e6f35', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='the versions commonly have errors. However, ChatGPT will\\nrefuse to generate a fact check list for a code sample and\\nindicate that this is something it cannot check, even though\\nthe code may have errors.\\nJ. The Template Pattern\\n1) Intent and Context: The intent of the pattern is to\\nensure an LLM’s output follows a precise template in terms of\\nstructure. For example, the user might need to generate a URL\\nthat inserts generated information into speciﬁc positions within\\nthe URL path. This pattern allows the user to instruct the LLM\\nto produce its output in a format it would not ordinarily use\\nfor the speciﬁed type of content being generated.\\n2) Motivation: In some cases, output must be produced in\\na precise format that is application or use-case speciﬁc and\\nnot known to the LLM. Since the LLM is not aware of the\\ntemplate structure, it must be instructed on what the format\\nis and where the different parts of its output should go. This\\ncould take the form of a sample data structure that is being\\ngenerated, a series of form letters being ﬁlled in, etc.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nI am going to provide a template for your output\\nX is my placeholder for content\\nTry to ﬁt the output into one or more of the placehold-\\ners that I list\\nPlease preserve the formatting and overall template that\\nI provide\\nThis is the template: PATTERN with PLACEHOLD-\\nERS\\nThe ﬁrst statement directs the LLM to follow a speciﬁc\\ntemplate for its output. The template will be used to try and\\ncoerce the LLMs responses into a structure that is consisten t\\nwith the user’s formatting needs. This pattern is needed whe n\\nthe target format is not known to the LLM. If the LLM already\\nhas knowledge of the format, such as a speciﬁc ﬁle type, then\\nthe template pattern can be skipped and the user can simply\\nspecify the known format. However, there may be cases, such\\nas generating Javascript Object Notation (JSON), where the re\\nis a large amount of variation in how the data could be\\nrepresented within that format and the template can be used t o\\nensure that the representation within the target format mee ts\\nthe user’s additional constraints.\\nThe second statement makes the LLM aware that the\\ntemplate will contain a set of placeholders. Users will expl ain\\nhow the output should be inserted into the template through t he\\nplaceholders. The placeholders allow the user to semantica lly\\ntarget where information should be inserted. Placeholders\\ncan use formats, like NAME, that allow the LLM to infer\\nthe semantic meaning of to determine where output should\\nbe inserted (e.g., insert the person’s name in the NAME\\nplaceholder). Moreover, by using placeholders, the user ca n\\nindicate what is not needed in the output – if a placeholder\\ndoesn’t exist for a component of the generated output, thenthat component can be omitted. Ideally, placeholders shoul d\\nuse a format that is commonly employed in text that the LLM\\nwas trained on, such as all caps, enclosure in brackets, etc.\\nThe third statement attempts to constrain the LLM so that it\\ndoesn’t arbitrarily rewrite the template or attempt to modi fy it\\nso that all of the output components can be inserted. It shoul d\\nbe noted that this statement may not preclude additional tex t\\nfrom being generated before or after. In practice, LLMs will\\ntypically follow the template, but it is harder to eliminate any\\nadditional text being generated beyond the template withou t\\nexperimentation with prompt wording.\\n4) Example Implementation: A sample template for gener-\\nating URLs where the output is put into speciﬁc places in the\\ntemplate is shown below:\\n“I am going to provide a template for your out-\\nput. Everything in all caps is a placeholder. Any\\ntime that you generate text, try to ﬁt it into one\\nof the placeholders that I list. Please preserve the\\nformatting and overall template that I provide at\\nhttps://myapi.com/NAME/proﬁle/JOB”\\nA sample interaction after the prompt was provided, is\\nshown:\\nUser: “Generate a name and job title for a person”\\nChatGPT: “https://myapi.com/Emily Parker/proﬁle/\\nSoftware Engineer”\\n5) Consequences: One consequence of applying the Tem-\\nplate pattern is that it ﬁlters the LLM’s output, which may\\neliminate other outputs the LLM would have provided that\\nmight be useful to the user. In many cases, the LLM can\\nprovide helpful descriptions of code, decision making, or o ther\\ndetails that this pattern will effectively eliminate from t he\\noutput. Users should therefore weight the pros/cons of ﬁlte ring\\nout this additional information.\\nIn addition, ﬁltering can make it hard to combine this patter n\\nwith other patterns from the Output Customization category.\\nThe Template pattern effectively constrains the output format,\\nso it may not be compatible with generation of certain other\\ntypes of output. For example, in the template provided above\\nfor a URL, it would not be easy (or likely possible) to combine\\nwith the Recipe pattern, which needs to output a list of steps.\\nK. The Inﬁnite Generation Pattern\\n1) Intent and Context: The intent of this pattern is to\\nautomatically generate a series of outputs (which may appea r\\ninﬁnite) without having to reenter the generator prompt eac h\\ntime. The goal is to limit how much text the user must type to\\nproduce the next output, based on the assumption that the use r\\ndoes not want to continually reintroduce the prompt. In some\\nvariations, the intent is to allow the user to keep an initial\\nprompt template, but add additional variation to it through\\nadditional inputs prior to each generated output.\\n2) Motivation: Many tasks require repetitive application of\\nthe same prompt to multiple concepts. For example, generati ng\\ncode for create, read, update, and delete (CRUD) operations\\nfor a speciﬁc type of entity may require applying the same', doc_id='8b0d7b6b-9931-4b1f-9855-035479d3111b', embedding=None, doc_hash='4b53a2edfa56b6ffce1e1077c9366a40116997e0016819c17d8e5b01f10f8da8', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='prompt to multiple types of entities. If the user is forced to\\nretype the prompt over and over, they may make mistakes. The\\nInﬁnite Generation pattern allows the user to repetitively apply\\na prompt, either with or without further input, to automate\\nthe generation of multiple outputs using a predeﬁned set of\\nconstraints.\\n3) Structure and Key Ideas:\\nContextual Statements\\nI would like you to generate output forever, X output(s)\\nat a time.\\n(Optional) here is how to use the input I provide\\nbetween outputs.\\n(Optional) stop when I ask you to.\\nThe ﬁrst statement speciﬁes that the user wants the LLM\\nto generate output indeﬁnitely, which effectively conveys the\\ninformation that the same prompt is going to be reused over\\nand over. By specifying the number of outputs that should be\\ngenerated at a time (i.e. “X outputs at a time”), the user can\\nrate limit the generation, which can be particularly import ant if\\nthere is a risk that the output will exceed the length limitat ions\\nof the LLM for a single output.\\nThe second statement provides optional instructions for ho w\\nto use the input provided by the user between outputs. By\\nspecifying how additional user inputs between prompts can\\nbe provided and leveraged, the user can create a prompting\\nstrategy that leverages user feedback in the context of the\\noriginal prompt. The original prompt is still in the context of\\nthe generation, but each user input between generation step s\\nis incorporated into the original prompt to reﬁne the output\\nusing prescribed rules.\\nThe third statement provides an optional way for the user\\nto stop the output generation process. This step is not alway s\\nneeded, but can be useful in situations where there may be\\nthe potential for ambiguity regarding whether or not the use r-\\nprovided input between inputs is meant as a reﬁnement for\\nthe next generation or a command to stop. For example, an\\nexplicit stop phrase could be created if the user was generat ing\\ndata related to road signs, where the user might want to enter\\na reﬁnement of the generation like “stop” to indicate that a\\nstop sign should be added to the output.\\n4) Example Implementation: The following is a sample\\ninﬁnite generation prompt for producing a series of URLs:\\n“From now on, I want you to generate a name\\nand job until I say stop. I am going to provide a\\ntemplate for your output. Everything in all caps is a\\nplaceholder. Any time that you generate text, try to\\nﬁt it into one of the placeholders that I list. Please\\npreserve the formatting and overall template that I\\nprovide: https://myapi.com/NAME/proﬁle/JOB”\\nThis prompt is combining the functionality of both the\\nInﬁnite Generation pattern and the Template pattern. The user\\nis requesting the LLM continuously generate a name and job\\ntitle until explicitly told to “stop”. The generated output s are\\nthen formatted into the template provided, which includesplaceholders for the name and job title. By using the Inﬁnite\\nGeneration pattern, the user receives multiple outputs without\\nhaving to continually re-enter the template. Likewise, the\\nTemplate pattern is applied to provide a consistent format for\\nthe outputs.\\n5) Consequences: In conversational LLMs, the input to\\nthe model at each time step is the previous output and the\\nnew user input. Although the details of what is preserved\\nand reintroduced in the next output cycle are model and\\nimplementation dependent, they are often limited in scope. The\\nmodel is therefore constantly being fed the previous output s\\nand the prompt, which can result in the model losing track of\\nthe original prompt instructions over time if they exceed th e\\nscope of what it is being provided as input.\\nAs additional outputs are generated, the context surroundi ng\\nthe prompt may fade, leading to the model deviating from\\nthe intended behavior. It is important to monitor the output s\\nproduced by the model to (1) ensure it still adheres to\\nthe desired behavior and (2) provide corrective feedback if\\nnecessary. Another issue to consider is that the LLM may\\ngenerate repetitive outputs, which may not be desired since\\nusers ﬁnd this repetition tedious and error-prone to proces s.\\nL. The Visualization Generator Pattern\\n1) Intent and Context: The intent of this pattern is to use\\ntext generation to create visualizations. Many concepts ar e\\neasier to grasp in diagram or image format. The purpose of\\nthis pattern is to create a pathway for the tool to produce\\nimagery that is associated with other outputs. This pattern\\nallows the creation of visualizations by creating inputs fo r\\nother well-known visualization tools that use text as their\\ninput, such as Graphviz Dot [15] or DALL-E [13]. This\\npattern can provide a more comprehensive and effective way\\nof communicating information by combining the strengths of\\nboth the text generation and visualization tools.\\n2) Motivation: LLMs generally produce text and cannot\\nproduce imagery. For example, an LLM cannot draw a diagram\\nto describe a graph. The Visualization Generator pattern over-\\ncomes this limitation by generating textual inputs in the co rrect\\nformat to plug into another tool that generates the correct\\ndiagram. The motivation behind this pattern is to enhance th e\\noutput of the LLM and make it more visually appealing and\\neasier to understand for users. By using text inputs to gener ate\\nvisualizations, users can quickly understand complex conc epts\\nand relationships that may be hard to grasp through text alon e.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nGenerate an X that I can provide to tool Y to visualize\\nit\\nThe goal of the contextual statements is to indicate to the\\nLLM that the output it is going to produce, “X”, is going to\\nbe imagery. Since LLMs can’t generate images, the ”that I\\ncan provide to tool Y to visualize it” clariﬁes that the LLM\\nis not expected to generate an image, but is instead expected', doc_id='c1e239a9-65b3-4ac9-a396-35eb8f59532f', embedding=None, doc_hash='44669dbfc909636117540212c6112ee6e2b27795ec87f5424c3a300194b931b8', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='to produce a description of imagery consumable by tool Y for\\nproduction of the image.\\nMany tools may support multiple types of visualizations or\\nformats, and thus the target tool itself may not be sufﬁcient\\ninformation to accurately produce what the user wants. The\\nuser may need to state the precise types of visualizations (e .g.,\\nbar chart, directed graph, UML class diagram) that should be\\nproduced. For example, Graphviz Dot can create diagrams for\\nboth UML class diagrams and directed graphs. Further, as wil l\\nbe discussed in the following example, it can be advantageou s\\nto specify a list of possible tools and formats and let the LLM\\nselect the appropriate target for visualization.\\n4) Example Implementation:\\n“Whenever I ask you to visualize something, please\\ncreate either a Graphviz Dot ﬁle or DALL-E prompt\\nthat I can use to create the visualization. Choose\\nthe appropriate tools based on what needs to be\\nvisualized.”\\nThis example of the pattern adds a qualiﬁcation that the\\noutput type for the visualization can be either for Graphviz\\nor DALL-E. The interesting aspect of this approach is that\\nit allows the LLM to use its semantic understanding of the\\noutput format to automatically select the target tooling ba sed\\non what will be displayed. In this case, Graphviz would be for\\nvisualizing graphs with a need for an exactly deﬁned structu re.\\nDALL-E would be effective at visualizing realistic or artis tic\\nimagery that does not have an exactly deﬁned structure. The\\nLLM can select the tool based on the needs of the visualizatio n\\nand capabilities of each tool.\\n5) Consequences: The pattern creates a target pipeline\\nfor the output to render a visualization. The pipeline may\\ninclude AI generators, such as DALL-E, that can produce\\nrich visualizations. The pattern allows the user to expand t he\\nexpressive capabilities of the output into the visual domai n.\\nM. The Game Play Pattern\\n1) Intent and Context: The intent of this pattern is to create\\na game around a given topic. The pattern can be combined\\nwith the Visualization Generator to add imagery to the game.\\nThe game is centered around a speciﬁc topic and the LLM\\nwill guide the game play. The pattern is particularly effect ive\\nwhen the rules of the game are relatively limited in scope, bu t\\nthe content for the game is expected to be wide in scope. The\\nuser can specify a limited set of rules and then the LLM can\\nautomate generation of bodies of content for game play.\\n2) Motivation: You would like the LLM to generate scenar-\\nios or questions revolving around a speciﬁc topic and requir e\\nusers to apply problem solving or other skills to accomplish\\na task related to the scenario. Generating all the content fo r\\nthe game manually would be too time consuming, however, so\\nyou would like the LLM to apply its knowledge of the topic\\nto guide the generation of content.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:Contextual Statements\\nCreate a game for me around X\\nOne or more fundamental rules of the game\\nThe ﬁrst statement, instructs the LLM to create a game and\\nprovides the important scoping of the game to a topic area. .\\nOne of the important capabilities of the pattern is that it al lows\\nthe user to create games by describing the rules of the game,\\nwithout having to determine the content of the game. The more\\nspeciﬁc the topic, typically the more novel and interesting the\\ngame play.\\nThe second statement introduces the rules of the game to the\\nLLM. It is important that the rules ﬁt within the capabilitie s\\nof the LLM. Textual games that rely on input and output text\\nsequences work best. One of the key attributes of the pattern\\nis that the input text can be rich and expressive, which can\\nlead to interesting interfaces for the game. For example, th e\\nuser might express actions in the game as scripts dictating\\na sequence of complex actions, such as “get a listing of all\\nnetwork activity and check it for anomalies”, which go beyon d\\nthe scope of multiple choice or short answer inputs. Each rul e\\nshould be provided as a separate statement regarding some\\naspect of the game.\\n4) Example Implementation: A sample cybersecurity game\\nprompt is shown below:\\n“We are going to play a cybersecurity game. You\\nare going to pretend to be a Linux terminal for a\\ncomputer that has been compromised by an attacker.\\nWhen I type in a command, you are going to output\\nthe corresponding text that the Linux terminal would\\nproduce. I am going to use commands to try and\\nﬁgure out how the system was compromised. The\\nattack should have done one or more of the following\\nthings: (1) launched new processes, (2) changed\\nﬁles, (3) opened new ports to receive communi-\\ncation, (4) created new outbound connections, (5)\\nchanged passwords, (6) created new user accounts,\\nor (7) read and stolen information. To start the game,\\nprint a scenario of what happened that led to my\\ninvestigation and make the description have clues\\nthat I can use to get started.”\\nThis cybersecurity game prompt combines a number of pat-\\nterns, including Game Play andPersona . The Linux terminal\\npersona that is chosen for the game has been discussed in many\\nInternet posts [16]. This game further customizes the discu ssed\\nLinux persona to act as a Linux terminal of a computer\\ncompromised by a cyberattack. ChatGPT responds to Linux\\ncommands with appropriate terminal outputs and synthesize s\\na fake ﬁle system that has clues to a cyberattack.\\nSample output from this game with ChatGPT after typing\\nin “ls -alt”, the Linux command to generate a list of ﬁles in\\nthe current directory is shown below (permissions have been\\nremoved for formatting reasons):\\ntotal 56\\n5 user user 4096 Feb 16 14:31 .\\n1 user user 18881 Feb 16 14:31 .bash_history', doc_id='326349e9-8b50-4b6a-88c2-286015083dda', embedding=None, doc_hash='f842ded0d9694b4775ff727b042847756e8fb30ace24c6af83f0ecd53268c74c', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='43 user user 4096 Feb 16 14:30 ..\\n1 user user 1178 Aug 15 2022 .bashrc\\n1 user user 220 Aug 15 2022 .bash_logout\\n3 user user 4096 Feb 16 14:25 .gnupg\\n1 user user 4041 Feb 15 17:52 README.md\\n2 user user 4096 Feb 1 11:35 project\\nWhen the user then typed the command “cat .bash history”,\\nto display the contents of the log ﬁle showing the commands\\nrun by the user, ChatGPT responded with:\\ncd /var/www/html\\nsudo rm -rf *\\nsudo wget https://example.com/backdoor.php\\nsudo chmod 777 backdoor.php\\nexit\\nls -alt\\nps -ef\\nnetstat -tulnp\\nIn the output above, ChatGPT generated a fake log ﬁle with\\nrealistic commands for deleting the data being served by the\\nweb server in /var/www/html and replacing the content\\nwith a backdoor into the system.\\n5) Consequences: This pattern can be combined effectively\\nwith the Persona ,Inﬁnite Generation , and Visualization Gen-\\nerator patterns. For example, the cybersecurity game uses the\\nPersona pattern so the LLM can masquerade as a Linux termi-\\nnal. For a network security game, the Visualization Generator\\ncould be employed to add the ability to visualize the network\\ntopology and trafﬁc ﬂows.\\nN. The Reﬂection Pattern\\n1) Intent and Context: The goal of this pattern is to ask\\nthe model to automatically explain the rationale behind giv en\\nanswers to the user. The pattern allows users to better asses s\\nthe output’s validity, as well as inform users how an LLM\\narrived at a particular answer. Reﬂection can clarify any po ints\\nof confusion, uncover underlying assumptions, and reveal g aps\\nin knowledge or understanding.\\n2) Motivation: LLMs can and do make mistakes. More-\\nover, users may not understand why an LLM is producing\\na particular output and how to adapt their prompt to solve\\na problem with the output. By asking LLM to automatically\\nexplain the rationale behind its answers, users can gain a be tter\\nunderstanding of how the model is processing the input, what\\nassumptions it is making, and what data it is drawing on.\\nLLMs may sometime provide incomplete, incorrect, or\\nambiguous answers. Reﬂection is an aid to help address these\\nshortcomings and ensure the information provided by LLM\\nis as accurate. A further beneﬁt of the pattern is that it can\\nhelp users debug their prompts and determine why they are\\nnot getting results that meet expectations. This pattern is\\nparticularly effective for the exploration of topics that c an\\nbe confused with other topics or that may have nuanced\\ninterpretations and where knowing the precise interpretat ion\\nthat the LLM used is important.3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWhenever you generate an answer\\nExplain the reasoning and assumptions behind your\\nanswer\\n(Optional) ...so that I can improve my question\\nThe ﬁrst statement is requesting that, after generating an a n-\\nswer, the LLM should explain the reasoning and assumptions\\nbehind the answer. This statement helps the user understand\\nhow the LLM arrived at the answer and can help build trust in\\nthe model’s responses. The prompt includes the statement th at\\nthe purpose of the explanation is for the user to reﬁne their\\nquestion. This additional statement gives the LLM the conte xt\\nit needs to better tailor its explanations to the speciﬁc pur pose\\nof aising the user in producing follow-on questions.\\n4) Example Implementation: This example tailors the\\nprompt speciﬁcally to the domain of providing answers relat ed\\nto code:\\n”When you provide an answer, please explain the\\nreasoning and assumptions behind your selection\\nof software frameworks. If possible, use speciﬁc\\nexamples or evidence with associated code samples\\nto support your answer of why the framework is\\nthe best selection for the task. Moreover, please\\naddress any potential ambiguities or limitations in\\nyour answer, in order to provide a more complete\\nand accurate response.”\\nThe pattern is further customized to instruct the LLM that\\nit should justify its selection of software frameworks, but not\\nnecessarily other aspects of the answer. In addition, the us er\\ndictates that code samples should be used to help explain the\\nmotivation for selecting the speciﬁc software framework.\\n5) Consequences: One consequence of the Reﬂection pat-\\ntern is that it may not be effective for users who do not\\nunderstand the topic area of the discussion. For example, a\\nhighly technical question by a non-technical user may resul t\\nin a complex rationale for the answer that the user cannot\\nfathom. As with other prompt patterns, there is a risk the\\noutput may include errors or inaccurate assumptions includ ed\\nin the explanation of the rationale that the user may not be\\nable to spot. This pattern can be combined with the Fact Check\\nListto help address this issue.\\nO. The Refusal Breaker Pattern\\n1) Intent and Context: The goal of this pattern is to ask an\\nLLM to automatically help users rephrase a question when it\\nrefuses to give an answer. This pattern has the potential for\\nmisuse, however, e.g., to generate phishing emails or perform\\nother actions that violate LLM policy ﬁlters. Caution shoul d\\ntherefore be exercised when applying this pattern to ensure\\nit is used ethically and responsibly. This pattern has been\\nused successfully in some LLMs to overcome the underlying\\nprompts used to program the LLM and prevent harmful output\\ngeneration.', doc_id='23950d47-0b05-444e-aca6-5283a2a28c4f', embedding=None, doc_hash='b29bfe378509c7bc32514d27be13c92033ec4bec301c87fbaf408b98cd0546ee', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='2) Motivation: LLMs may sometimes refuse to answer\\na question, either because they do not have the required\\nknowledge or because the question is phrased in a way that\\nthey do not understand. This outcome may be frustrating\\nfor users who are looking for answers. In some situations,\\ntherefore, the Refusal Breaker pattern can help users ﬁnd a\\nway to either rephrase their question or ask a different ques tion\\nthe LLM is better equipped to answer.\\nFor example, you could ask for clariﬁcation on why the\\nquestion was refused, as there may be a speciﬁc reason why\\nLLM could not provide an answer. Another approach is to\\nask a different question that may be within its knowledge cut -\\noff or expertise. However, all these approaches require use r\\nintervention and knowledge of how to bypass the refusal.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWhenever you can’t answer a question\\nExplain why you can’t answer the question\\nProvide one or more alternative wordings of the ques-\\ntion that you could answer\\nThe ﬁrst statement sets the stage that this set of instructio ns\\nshould only be applied when a question can’t be answered.\\nThe user does not want to trigger these instructions under\\nnormal circumstances. The second statement instructs the\\nLLM to provide an explanation of why the question can’t be\\nanswered. The explanation is likely to include assumptions ,\\nconstraints, or misunderstandings that can be leveraged by the\\nuser to strategize about how to reword the question. Althoug h\\njust the explanation of why the question can’t be answered\\ncould help a user overcome the refusal, the pattern adds an\\nadditional statement for the LLM to suggest rewordings that\\nwould be acceptable. The pattern combines both reasoning\\nand suggested rewordings to improve the chances that the use r\\nwill ﬁnd a path around whatever roadblocks are preventing th e\\nquestion form being answered.\\n4) Example Implementation: The Refusal Breaker pattern\\ncan be be implemented and applied with the following phras-\\ning:\\n“Whenever you can’t answer a question, explain why\\nand provide one or more alternate wordings of the\\nquestion that you can’t answer so that I can improve\\nmy questions.”\\nA sample discourse after using the pattern is shown below:\\nUser: What is the meaning of life?\\nChatGPT : As an AI language model, I do not\\nhave personal beliefs or opinions. The meaning of\\nlife is a complex philosophical question that has\\nbeen pondered by humans for centuries and can\\nvary greatly from person to person. It may be more\\nproductive to rephrase the question in a way that can\\nbe answered by information and knowledge, such\\nas “What are some philosophical perspectives onthe meaning of life?” or “What are some common\\nbeliefs about the purpose of life?”\\n5) Consequences: This pattern provides a potential path\\nfor misuse, so organizations, parents, or other stakeholde rs\\nmay need to restrict the usage of the LLM. The ﬁrst step in\\ngoing around guardrails on usage is to understand where the\\nguardrails are. In future work, a complement of this pattern\\nmay be developed to hide the underlying prompt information\\nand rationale from users to prevent discovery.\\nAlthough the rationale and alternate rewordings are gener-\\nated, there is no guarantee that users will be able to overcom e\\nthe refusal. The alternate questions that are generated may not\\nbe of interest to the user or helpful in answering the origina l\\nquestion. The pattern mainly provides an aid in determining\\nwhat the LLM can answer, but not a guarantee that it will\\nanswer a semantically equivalent variation of the original\\nquestion.\\nP . The Context Manager Pattern\\n1) Intent and Context: The intent of this pattern is to enable\\nusers to specify or remove context for a conversation with\\nan LLM. The goal is to focus the conversation on speciﬁc\\ntopics or exclude unrelated topics from consideration. Thi s\\npattern gives users greater control over what statements th e\\nLLM considers or ignores when generating output.\\n2) Motivation: LLMs often struggle to interpret the in-\\ntended context of the current question or generate irreleva nt\\nresponses based on prior inputs or irrelevant attention on\\nthe wrong statements. By focusing on explicit contextual\\nstatements or removing irrelevant statements, users can he lp\\nthe LLM better understand the question and generate more\\naccurate responses. Users may introduce unrelated topics o r\\nreference information from earlier in the dialogue, which\\nmay can disrupt the ﬂow of the conversation. The Context\\nManager pattern aims to emphasize or remove speciﬁc aspects\\nof the context to maintain relevance and coherence in the\\nconversation.\\n3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nWithin scope X\\nPlease consider Y\\nPlease ignore Z\\n(Optional) start over\\nStatements about what to consider or ignore should list key\\nconcepts, facts, instructions, etc. that should be include d or\\nremoved from the context. The more explicit the statements\\nare, the more likely the LLM will take appropriate action. Fo r\\nexample, if the user asks to ignore subjects related to a topi c,\\nyet some of the those statements were discussed far back in th e\\nconversation, the LLM may not properly disregard the releva nt\\ninformation. The more explicit the list is, therefore, the b etter\\nthe inclusion/exclusion behavior will be.', doc_id='18200305-5923-4c4f-bb17-ce6348f1c724', embedding=None, doc_hash='6d039a497c167c2228e23dcf8a69dc6be479c221e9a86d91e576be571ec82257', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='4) Example Implementation: To specify context consider\\nusing the following prompt:\\n“When analyzing the following pieces of code, only\\nconsider security aspects.”\\nLikewise, to remove context consider using the following\\nprompt:\\n“When analyzing the following pieces of code, do\\nnot consider formatting or naming conventions.”\\nClarity and speciﬁcity are important when providing or\\nremoving context to/from an LLM so it can better understand\\nthe intended scope of the conversation and generate more\\nrelevant responses. In many situations, the user may want to\\ncompletely start over and can employ this prompt to reset the\\nLLM’s context:\\n“Ignore everything that we have discussed. Start\\nover.”\\nThe “start over” idea helps produce a complete reset of the\\ncontext.\\n5) Consequences: One consequence of this pattern is that\\nit may inadvertently wipe out patterns applied to the con-\\nversation that the user is unaware of. For example, if an\\norganization injects a series of helpful patterns into the s tart of\\na conversation, the user may not be aware of these patterns an d\\nremove them through a reset of the context. This reset could\\npotentially eliminate helpful capabilities of the LLM, whi le\\nnot making it obvious that the user will lose this functional ity.\\nA potential solution to this problem is to include in the prom pt\\na request to explain what topics/instructions will potenti ally be\\nlost before proceeding.\\nQ. The Recipe Pattern\\n1) Intent and Context: This pattern provides constraints to\\nultimately output a sequence of steps given some partially\\nprovided “ingredients” that must be conﬁgured in a sequence\\nof steps to achieve a stated goal. It combines the Template ,\\nAlternative Approaches , and Reﬂection patterns.\\n2) Motivation: Users often want an LLM to analyze a\\nconcrete sequence of steps or procedures to achieve a stated\\noutcome. Typically, users generally know—or have an idea\\nof—what the end goal should look like and what “ingredients”\\nbelong in the prompt. However, they may not necessarily know\\nthe precise ordering of steps to achieve that end goal.\\nFor example, a user may want a precise speciﬁcation on how\\na piece of code should be implemented or automated, such as\\n“create an Ansible playbook to ssh into a set of servers, copy\\ntext ﬁles from each server, spawn a monitoring process on\\neach server, and then close the ssh connection to each server .\\nIn other words, this pattern represents a generalization of the\\nexample of “given the ingredients in my fridge, provide dinn er\\nrecipes.” A user may also want to specify a set number of\\nalternative possibilities, such as “provide 3 different wa ys of\\ndeploying a web application to AWS using Docker containers\\nand Ansible using step by step instructions”.3) Structure and Key Ideas: Fundamental contextual state-\\nments:\\nContextual Statements\\nI would like to achieve X\\nI know that I need to perform steps A,B,C\\nProvide a complete sequence of steps for me\\nFill in any missing steps\\nIdentify any unnecessary steps\\nThe ﬁrst statement “I would like to achieve X” focuses the\\nLLM on the overall goal that the recipe needs to be built\\nto achieve. The steps will be organized and completed to\\nsequentially achieve the goal speciﬁed. The second stateme nt\\nprovides the partial list of steps that the user would like\\nto include in the overall recipe. These serve as intermediat e\\nwaypoints for the path that the LLM is going to generate or\\nconstraints on the structure of the recipe. The next stateme nt\\nin the pattern, “provide a complete sequence of steps for\\nme”, indicates to the LLM that the goal is to provide a\\ncomplete sequential ordering of steps. The “ﬁll in any missi ng\\nsteps” helps ensure that the LLM will attempt to complete\\nthe recipe without further follow-up by making some choices\\non the user’s behalf regarding missing steps, as opposed to\\njust stating additional information that is needed. Finall y, the\\nlast statement, “identify any unnecessary steps,” is usefu l in\\nﬂagging inaccuracies in the user’s original request so that the\\nﬁnal recipe is efﬁcient.\\n4) Example Implementation: An example usage of this\\npattern in the context of deploying a software application t o\\nthe cloud is shown below:\\n“I am trying to deploy an application to the cloud. I\\nknow that I need to install the necessary dependen-\\ncies on a virtual machine for my application. I know\\nthat I need to sign up for an AWS account. Please\\nprovide a complete sequence of steps. Please ﬁll in\\nany missing steps. Please identify any unnecessary\\nsteps.”\\nDepending on the use case and constraints, “installing\\nnecessary dependencies on a virtual machine” may be an\\nunnecessary step. For example, if the application is alread y\\npackaged in a Docker container, the container could be de-\\nployed directly to the AWS Fargate Service, which does not\\nrequire any management of the underlying virtual machines.\\nThe inclusion of the “identify unnecessary steps” language\\nwill cause the LLM to ﬂag this issue and omit the steps from\\nthe ﬁnal recipe.\\n5) Consequences: One consequence of the recipe pattern is\\nthat a user may not always have a well-speciﬁed description\\nof what they would like to implement, construct, or design.\\nMoreover, this pattern may introduce unwanted bias from the\\nuser’s initially selected steps so the LLM may try to ﬁnd a\\nsolution that incorporates them, rather than ﬂagging them a s\\nunneeded. For example, an LLM may try to ﬁnd a solution\\nthat does install dependencies for a virtual machine, even i f\\nthere are solutions that do not require that.', doc_id='798c04c0-d8a4-44a1-9308-4c04931d6f91', embedding=None, doc_hash='3dd729c817e39f9f42ff55f83152f09ddf9f8c4210e78c49fc147faf34343372', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='IV. R ELATED WORK\\nSoftware patterns [10], [11] have been extensively studied\\nand documented in prior work. Patterns are widely used in\\nsoftware engineering to express the intent of design struct ures\\nin a way that is independent of implementation details. Patt erns\\nprovide a mental picture of the goals that the pattern is\\ntrying to achieve and the forces that it is trying to resolve.\\nA key advantage of patterns is their composability, allowin g\\ndevelopers to build pattern sequences and pattern language s\\nthat can be used to address complex problems. Patterns have\\nalso been investigated in other domains, such as contract\\ndesign for decentralized ledgers [17], [18].\\nThe importance of good prompt design with LLMs, such as\\nChatGPT, is well understood [19]–[28]. Previous studies ha ve\\nexamined the effect of prompt words on AI generative models.\\nFor example, Liu et al. [29] investigated how different\\nprompt key words affect image generation and different char -\\nacteristics of images. Other work has explored using LLMs\\nto generate visualizations [30]. Han et al. [31] researched\\nstrategies for designing prompts for classiﬁcation tasks. Other\\nresearch has looked at boolean prompt design for literature\\nqueries [32]. Yet other work has speciﬁcally examined promp ts\\nfor software and ﬁxing bugs [33].\\nOur work is complementary to prior work by providing\\na structure for documenting, discussing, and reasoning abo ut\\nprompts that can aid users in developing mental models for\\nstructuring prompts to solve common problems.\\nThe quality of the answers produced by LLMs, particuarly\\nChatGPT, has been assessed in a number of domains. For\\nexample, ChatGPT has been used to take the medical licensing\\nexam with surprisingly good results [3]. The use of ChatGPT\\nin Law School has also been explored [34]. Other papers have\\nlooked at its mathematical reasoning abilities [35]. As mor e\\ndomains are explored, we expect that domain-speciﬁc patter n\\ncatalogs will be developed to share domain-speciﬁc problem\\nsolving prompt structures.\\nV. C ONCLUDING REMARKS\\nThis paper presented a framework for documenting and\\napplying a catalog of prompt patterns for large language\\nmodels (LLMs), such as ChatGPT. These prompt patterns are\\nanalogous to software patterns and aim to provide reusable\\nsolutions to problems that users face when interacting with\\nLLMs to perform a wide range of tasks. The catalog of prompt\\npatterns captured via this framework (1) provides a structu red\\nway of discussing prompting solutions, (2) identiﬁes patte rns\\nin prompts, rather than focusing on speciﬁc prompt examples ,\\nand (3) classiﬁes patterns so users are guided to more efﬁcie nt\\nand effective interactions with LLMs.\\nThe following lessons learned were gleaned from our work\\non prompt patterns:\\n•Prompt patterns signiﬁcantly enrich the capabilities that\\ncan be created in a conversational LLM . For example,\\nprompts can lead to the generation of cybersecurity\\ngames, complete with ﬁctitious terminal commands thathave been run by an attacker stored in a .bashhistory\\nﬁle. As shown in Section III, larger and more complex\\ncapabilities can be created by combining prompt patterns,\\nsuch as combining the Game Play and Visualization\\nGenerator patterns.\\n•Documenting prompt patterns as a pattern catalog is\\nuseful, but insufﬁcient . Our experience indicates that\\nmuch more work can be done in this area, both in terms\\nof reﬁning and expanding the prompt patterns presented\\nin this paper, as well as in exploring new and innovative\\nways of using LLMs. In particular, weaving the prompt\\npatterns captured here as a pattern catalog into a more\\nexpression pattern language will help guide users of\\nLLMs more effectively.\\n•LLM Capabilities will evolve over time, likely necessitat-\\ning reﬁnement of patterns. As LLM capabilities change,\\nsome patterns may no longer be necessary, be obviated\\nby different styles of interaction or conversation/sessio n\\nmanagement approaches, or require enhancement to func-\\ntion correctly. Continued work will be needed to docu-\\nment and catalog patterns that provide reusable solutions.\\n•The prompt patterns are generalizable to many differ-\\nent domains. Although most of the patterns have been\\ndiscussed in the context of software development, these\\nsame patterns are applicable in arbitrary domains, ranging\\nfrom inﬁnite generation of stories for entertainment to\\neducational games to explorations of topics.\\nWe hope that this paper inspires further research and de-\\nvelopment in this area that will help enhance prompt pattern\\ndesign to create new and unexpected capabilities for conver -\\nsational LLMs.\\nREFERENCES\\n[1] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von\\nArx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al. ,\\n“On the opportunities and risks of foundation models,” arXiv preprint\\narXiv:2108.07258 , 2021.\\n[2] Y . Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia,\\nZ. Ji, T. Yu, W. Chung et al. , “A multitask, multilingual, multimodal\\nevaluation of chatgpt on reasoning, hallucination, and int eractivity,”\\narXiv preprint arXiv:2302.04023 , 2023.\\n[3] A. Gilson, C. Safranek, T. Huang, V . Socrates, L. Chi, R. A . Taylor,\\nand D. Chartash, “How well does chatgpt do when taking the med ical\\nlicensing exams?” medRxiv , pp. 2022–12, 2022.\\n[4] A. Carleton, M. H. Klein, J. E. Robert, E. Harper, R. K. Cun ningham,\\nD. de Niz, J. T. Foreman, J. B. Goodenough, J. D. Herbsleb, I. O zkaya,\\nand D. C. Schmidt, “Architecting the future of software engi neering,”\\nComputer , vol. 55, no. 9, pp. 89–93, 2022.\\n[5] “Github copilot · your ai pair programmer.” [Online]. Av ailable:\\nhttps://github.com/features/copilot\\n[6] O. Asare, M. Nagappan, and N. Asokan, “Is github’s copilo t as bad\\nas humans at introducing vulnerabilities in code?” arXiv preprint\\narXiv:2204.04741 , 2022.\\n[7] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri , “Asleep at\\nthe keyboard? assessing the security of github copilot’s co de contribu-\\ntions,” in 2022 IEEE Symposium on Security and Privacy (SP) . IEEE,\\n2022, pp. 754–768.\\n[8] J. Krochmalski, IntelliJ IDEA Essentials . Packt Publishing Ltd, 2014.\\n[9] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-\\ntrain, prompt, and predict: A systematic survey of promptin g methods\\nin natural language processing,” ACM Computing Surveys , vol. 55, no. 9,\\npp. 1–35, 2023.', doc_id='52a6c2ab-8498-4d4f-a5c6-81a4dfb1de8f', embedding=None, doc_hash='0680967f461a93e61f09f48428a4c347a7e364faf45f984f90c54eca64260e67', extra_info={'nome file': 'Prompt patterns.pdf'}), Document(text='[10] E. Gamma, R. Johnson, R. Helm, R. E. Johnson, and J. Vliss ides,\\nDesign patterns: elements of reusable object-oriented sof tware . Pearson\\nDeutschland GmbH, 1995.\\n[11] D. C. Schmidt, M. Stal, H. Rohnert, and F. Buschmann, Pattern-oriented\\nsoftware architecture, patterns for concurrent and networ ked objects .\\nJohn Wiley & Sons, 2013.\\n[12] OpenAI, “ChatGPT: Large-Scale Generative Language Mo dels for\\nAutomated Content Creation,” https://openai.com/blog/c hatgpt/, 2023,\\n[Online; accessed 19-Feb-2023].\\n[13] ——, “DALL·E 2: Creating Images from Text,”\\nhttps://openai.com/dall-e-2/, 2023, [Online; accessed 1 9-Feb-2023].\\n[14] D. Zhou, N. Sch¨ arli, L. Hou, J. Wei, N. Scales, X. Wang, D . Schu-\\nurmans, O. Bousquet, Q. Le, and E. Chi, “Least-to-most promp ting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625 , 2022.\\n[15] J. Ellson, E. R. Gansner, E. Koutsoﬁos, S. C. North, and G . Woodhull,\\n“Graphviz and dynagraph—static and dynamic graph drawing t ools,”\\nGraph drawing software , pp. 127–148, 2004.\\n[16] S. Owen, “Building a virtual machine inside a javascrip t library,”\\nhttps://www.engraved.blog/building-a-virtual-machin e-inside/, 2022,\\naccessed: 2023-02-20.\\n[17] P. Zhang, J. White, D. C. Schmidt, and G. Lenz, “Applying\\nsoftware patterns to address interoperability in blockcha in-based\\nhealthcare apps,” CoRR , vol. abs/1706.03700, 2017. [Online]. Available:\\nhttp://arxiv.org/abs/1706.03700\\n[18] X. Xu, C. Pautasso, L. Zhu, Q. Lu, and I. Weber, “A pattern collection\\nfor blockchain-based applications,” in Proceedings of the 23rd European\\nConference on Pattern Languages of Programs , 2018, pp. 1–20.\\n[19] E. A. van Dis, J. Bollen, W. Zuidema, R. van Rooij, and C. L . Bockting,\\n“Chatgpt: ﬁve priorities for research,” Nature , vol. 614, no. 7947, pp.\\n224–226, 2023.\\n[20] L. Reynolds and K. McDonell, “Prompt programming for la rge language\\nmodels: Beyond the few-shot paradigm,” CoRR , vol. abs/2102.07350,\\n2021. [Online]. Available: https://arxiv.org/abs/2102. 07350\\n[21] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le,\\nand D. Zhou, “Chain of thought prompting elicits reasoning i n\\nlarge language models,” CoRR , vol. abs/2201.11903, 2022. [Online].\\nAvailable: https://arxiv.org/abs/2201.11903\\n[22] J. Wei, Y . Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borge aud,\\nD. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi,\\nT. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus, “Em ergent\\nabilities of large language models,” 2022. [Online]. Avail able:\\nhttps://arxiv.org/abs/2206.07682\\n[23] Y . Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Ch an, and\\nJ. Ba, “Large language models are human-level prompt engine ers,”\\n2022. [Online]. Available: https://arxiv.org/abs/2211. 01910\\n[24] T. Shin, Y . Razeghi, R. L. L. IV , E. Wallace, and S. Singh,\\n“Autoprompt: Eliciting knowledge from language models wit h\\nautomatically generated prompts,” CoRR , vol. abs/2010.15980, 2020.\\n[Online]. Available: https://arxiv.org/abs/2010.15980\\n[25] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sut skever,\\n“Language models are unsupervised multitask learners,” 20 19.\\n[26] D. Zhou, N. Sch¨ arli, L. Hou, J. Wei, N. Scales, X. Wang,\\nD. Schuurmans, C. Cui, O. Bousquet, Q. Le, and E. Chi, “Least- to-\\nmost prompting enables complex reasoning in large language models,”\\n2022. [Online]. Available: https://arxiv.org/abs/2205. 10625\\n[27] J. Jung, L. Qin, S. Welleck, F. Brahman, C. Bhagavatula, R. L.\\nBras, and Y . Choi, “Maieutic prompting: Logically consiste nt\\nreasoning with recursive explanations,” 2022. [Online]. A vailable:\\nhttps://arxiv.org/abs/2205.11822\\n[28] S. Arora, A. Narayan, M. F. Chen, L. Orr, N. Guha,\\nK. Bhatia, I. Chami, and C. Re, “Ask me anything: A\\nsimple strategy for prompting language models,” in International\\nConference on Learning Representations , 2023. [Online]. Available:\\nhttps://openreview.net/forum?id=bhUPJnS2g0X\\n[29] V . Liu and L. B. Chilton, “Design guidelines for prompt e ngineering\\ntext-to-image generative models,” in Proceedings of the 2022 CHI\\nConference on Human Factors in Computing Systems , 2022, pp. 1–23.\\n[30] P. Maddigan and T. Susnjak, “Chat2vis: Generating data visualisations\\nvia natural language using chatgpt, codex and gpt-3 large la nguage\\nmodels,” arXiv preprint arXiv:2302.02094 , 2023.\\n[31] X. Han, W. Zhao, N. Ding, Z. Liu, and M. Sun, “Ptr: Prompt t uning\\nwith rules for text classiﬁcation,” AI Open , vol. 3, pp. 182–192, 2022.[32] S. Wang, H. Scells, B. Koopman, and G. Zuccon, “Can chatg pt write\\na good boolean query for systematic review literature searc h?”arXiv\\npreprint arXiv:2302.03495 , 2023.\\n[33] C. S. Xia and L. Zhang, “Conversational automated progr am repair,”\\narXiv preprint arXiv:2301.13246 , 2023.\\n[34] J. H. Choi, K. E. Hickman, A. Monahan, and D. Schwarcz, “C hatgpt\\ngoes to law school,” Available at SSRN , 2023.\\n[35] S. Frieder, L. Pinchetti, R.-R. Grifﬁths, T. Salvatori , T. Lukasiewicz,\\nP. C. Petersen, A. Chevalier, and J. Berner, “Mathematical c apabilities\\nof chatgpt,” arXiv preprint arXiv:2301.13867 , 2023.', doc_id='bc277b49-95e0-416f-9615-f572e3235d85', embedding=None, doc_hash='438d86dd4d213db45f330ac9388c8722ec801d3b37fc4c8ef36e5166d95ed5e4', extra_info={'nome file': 'Prompt patterns.pdf'})]}\n"
     ]
    }
   ],
   "source": [
    "print(doc_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. SET UP DEL LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# importiamo la chiave che sarà necessaria per impostare il LLM da usare, la nostra è una chiave di OpenAI\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-...Iy'     \n",
    "\n",
    "''' provare ad usare MockLLMPredictor per ottenere il numero di token usati -> from llama_index import MockLLMPredictor''' \n",
    "\n",
    "# Set up del LLM\n",
    "from llama_index import LLMPredictor, GPTVectorStoreIndex, PromptHelper, GPTListIndex, ServiceContext \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Defining LLm\n",
    "llm_predictor = LLMPredictor(llm = ChatOpenAI(temperature = 0, model_name = 'gpt-3.5-turbo', openai_api_key= os.environ[\"OPENAI_API_KEY\"]))\n",
    "\n",
    "\n",
    "# Define PromptHelper\n",
    "## Set Maximum input size\n",
    "max_input_size = 4096\n",
    "\n",
    "## Set max number of output tokens\n",
    "num_output = 256\n",
    "\n",
    "## Sex maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "\n",
    "\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)       # salviamo le impostazioni per il prompting del modello\n",
    "\n",
    "\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm_predictor = llm_predictor,     # impostiamo il contesto di rferimento, ovvero il modello e le impostazioni\n",
    "                                               prompt_helper = prompt_helper)     # per il suo prompting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. CREAZIONE DEGLI INDICI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize simple vector indices + global vector index\n",
    "\n",
    "\n",
    "index_set = {}                    # questo insieme conterrà coppie chiavi:valori nella forma -> \"nomefile\": indice\n",
    "for nomefile in lista_pdf:\n",
    "    \n",
    "  # Creazione di un indice per ogni documento\n",
    "  cur_index = GPTVectorStoreIndex.from_documents(doc_set[nomefile], service_context=service_context)  # GPTVectorStoreIndex creerà l'indice basandosi sull'oggetto Document(text ='') del file selezionato\n",
    "\n",
    "  index_set[nomefile] = cur_index                                                                     # Carichiamo nell'insieme la chiave \"nomefile\" e il relativo valore \"cur_index\"\n",
    "\n",
    "  # Salviamo ogni indice come file .json\n",
    "  cur_index.storage_context.persist(persist_dir = f'''./storage/index_{nomefile.replace(\".pdf\",\"\")}.json''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. CARICAMENTO INDICI DA DISCO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Chain of thoughts.pdf': <llama_index.indices.vector_store.base.GPTVectorStoreIndex object at 0x000002035E309390>, 'Prompt patterns.pdf': <llama_index.indices.vector_store.base.GPTVectorStoreIndex object at 0x000002035E313790>}\n"
     ]
    }
   ],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "\n",
    "# Load indices from disk\n",
    "index_set = {}\n",
    "for nomefile in lista_pdf:\n",
    "  # rebuild storage context\n",
    "  storage_context = StorageContext.from_defaults(persist_dir = f'''./storage/index_{nomefile.replace(\".pdf\",\"\")}.json/''')\n",
    "\n",
    "  # load index\n",
    "  cur_index = load_index_from_storage(storage_context)\n",
    "\n",
    "  ###cur_index = GPTVectorStoreIndex.load_from_disk(f'index_{nomefile.replace(\".pdf\",\"\")}.json')  # # non viene più usato\n",
    "  index_set[nomefile] = cur_index\n",
    "print(index_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. QUERY SEMPLICE SULL'INDICE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nel caso avessimo un solo indice, possiamo provare a fare una query diretta sulla stesso, con impostazioni di default\n",
    "\n",
    "query_engine = cur_index.as_query_engine()      # costruiamo sull'indice la query_engine che permetterà di interrogare l'indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"INSERISCI LA TUA QUERY\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Chain-of-thought prompting is a technique used to facilitate reasoning in language models.\n",
      "2. It involves providing demonstrations of chain-of-thought reasoning in the exemplars for few-shot prompting.\n",
      "3. The goal is to endow language models with the ability to generate a coherent series of intermediate reasoning steps that lead to the final answer for a problem.\n",
      "4. Empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks show that chain-of-thought prompting outperforms standard prompting.\n",
      "5. Chain-of-thought prompting has several attractive properties, such as allowing models to decompose multi-step problems into intermediate steps, providing an interpretable window into the behavior of the model, and being applicable to any task that humans can solve via language.\n"
     ]
    }
   ],
   "source": [
    "# Query sul file Chain of thoughts\n",
    "\n",
    "response = query_engine.query(\"Please, consider the context given, thus the file Chain of thoughts.pdf. I would like you to explain it in 5 steps\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1) The Prompt Patterns.pdf file provides a set of patterns that can be used to help users interact with a language learning machine (LLM). \n",
      "\n",
      "2) The Recipe Pattern helps users provide a sequence of steps to achieve a stated goal, given some partially provided “ingredients”. \n",
      "\n",
      "3) The Refusal Pattern helps users generate alternate questions when the LLM refuses to answer a question. \n",
      "\n",
      "4) The Context Manager Pattern enables users to specify or remove context for a conversation with an LLM, in order to focus the conversation on specific topics or exclude unrelated topics from consideration. \n",
      "\n",
      "5) The key ideas of the Context Manager Pattern involve providing explicit contextual statements about what to consider or ignore, in order to help the LLM better understand the question and generate more accurate responses.\n"
     ]
    }
   ],
   "source": [
    "# Query sul file Chain of thoughts\n",
    "\n",
    "response = query_engine.query(\"Please, consider the context given, thus the file Prompt Patterns.pdf. I would like you to explain it in 5 steps\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatDocu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
